{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "from pandas_profiling.utils.cache import cache_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the dataset I am using is very large for GitHub's size limit. They actually recommend not uploading things of +50MB and will block anything over +100MB [link](https://help.github.com/en/github/managing-large-files/conditions-for-large-files). For this reason the csv file is not shared in the repository but can be accessed directly from [Kaggle](https://www.kaggle.com/datasnaek/mbti-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'/Users/diego/Google Drive/2. Business Intelligence/Data/MBTI/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "As we can see, the dataset is complete since it does not have missing values but it only has 2 columns.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate a report through profile_report for a more elegant presentation of this information\n",
    "\n",
    "report = df.profile_report(sort='None', html={'style':{'full_width':True}}, progress_bar=False)\n",
    "#report # uncomment this line to see it here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saves our report as a html file in our directory\n",
    "\n",
    "report.to_file('mbti_report_raw.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    8675\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if there are any duplicates\n",
    "\n",
    "duplicate_rows = df.duplicated()\n",
    "duplicate_rows.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Since there are not missing values and no duplicate values we cannot do much in this section of data cleaning but we can do a lot to create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying and Creating Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually part of the EDA section but I proceed to do it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the individual traits and later evaluate them individually generating thus categories of 2 concepts:\n",
    "<ul><li>Introversion (I) – Extroversion (E)</li>\n",
    "<li>Intuition (N) – Sensing (S)</li>\n",
    "<li>Thinking (T) – Feeling (F)</li>\n",
    "<li>Judging (J) – Perceiving (P)</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = df['type'].str.get_dummies('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since every category is part of a spectrum (I - E) (N - S) (T - F) (J - P) we can save one of each instead of keeping both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = df_dummies.drop(['E','S','F','P'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I check the length of all the posts. This will probably provide little insights since we do not know how the data was gathered\n",
    "\n",
    "df['posts_len'] = df['posts'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'http://www.youtube.com/watch?v=qsXHcwe3krw\",\n",
       " 'http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg',\n",
       " 'enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks',\n",
       " 'What has been the most life-changing experience in your life?',\n",
       " 'http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.',\n",
       " 'May the PerC Experience immerse you.',\n",
       " 'The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206',\n",
       " \"Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...\",\n",
       " '84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...',\n",
       " 'Welcome and stuff.',\n",
       " 'http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.',\n",
       " \"Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...\",\n",
       " \"Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...\",\n",
       " 'All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...',\n",
       " 'Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:',\n",
       " 'https://www.youtube.com/watch?v=QyPqT8umzmY',\n",
       " 'It appears to be too late. :sad:',\n",
       " \"There's someone out there for everyone.\",\n",
       " 'Wait... I thought confidence was a good thing.',\n",
       " \"I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...\",\n",
       " \"Yo entp ladies... if you're into a complimentary personality,well, hey.\",\n",
       " '... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.',\n",
       " 'http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50',\n",
       " 'http://www.youtube.com/watch?v=msqXffgh7b8',\n",
       " 'Banned because this thread requires it of me.',\n",
       " 'Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.',\n",
       " 'http://www.youtube.com/watch?v=Mw7eoU3BMbE',\n",
       " 'http://www.youtube.com/watch?v=4V2uYORhQOk',\n",
       " 'http://www.youtube.com/watch?v=SlVmgFQQ0TI',\n",
       " \"Banned for too many b's in that sentence. How could you! Think of the B!\",\n",
       " 'Banned for watching movies in the corner with the dunces.',\n",
       " 'Banned because Health class clearly taught you nothing about peer pressure.',\n",
       " 'Banned for a whole host of reasons!',\n",
       " 'http://www.youtube.com/watch?v=IRcrv41hgz4',\n",
       " \"1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...\",\n",
       " 'a pokemon world  an infj society  everyone becomes an optimist',\n",
       " '49142',\n",
       " 'http://www.youtube.com/watch?v=ZRCEq_JFeFM',\n",
       " 'http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg',\n",
       " 'http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif',\n",
       " 'http://www.serebii.net/potw-dp/Scizor.jpg',\n",
       " \"Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.\",\n",
       " \"Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:\",\n",
       " 'Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.',\n",
       " 'http://www.youtube.com/watch?v=w8IgImn57aQ',\n",
       " 'Banned for being too much of a thundering, grumbling kind of storm... yep.',\n",
       " \"Ahh... old high school music I haven't heard in ages.   http://www.youtube.com/watch?v=dcCRUPCdB1w\",\n",
       " \"I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...\",\n",
       " \"I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M\",\n",
       " \"Move to the Denver area and start a new life for myself.'\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split one observation to see what is contains, when we checked the header we saw that each post is separated with \"\"|||\"\n",
    "\n",
    "df.iloc[0,1].split('|||')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have seen  convert every observation into a list of each post\n",
    "\n",
    "df['posts_separated'] = df['posts'].apply(lambda x: x.split('|||'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We count how many posts each person got recorded\n",
    "\n",
    "df['count_posts'] = df['posts_separated'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50    7587\n",
       "47      82\n",
       "48      79\n",
       "42      61\n",
       "49      60\n",
       "      ... \n",
       "5        1\n",
       "77       1\n",
       "14       1\n",
       "78       1\n",
       "75       1\n",
       "Name: count_posts, Length: 77, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the results from the previous line of code. It does not seem a very reliable category being so skewed towards 50\n",
    "\n",
    "df.count_posts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe the average number of characters used in each post wil provide us with more information\n",
    "\n",
    "from statistics import mean\n",
    "df['avg_num_char_x_post'] = [mean([len(i) for i in x]) for x in df.iloc[:,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the number of links a persons uses?\n",
    "\n",
    "df['num_of_links'] = [sum([url.count('http') for url in x]) for x in df.iloc[:,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keirsey Temperament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
       "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how many unique types of MBTI profiles exist\n",
    "df['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This is correct, there are 16 different personality types according to MBTI. In the EDA section we will check how many observations of each type we have, but for now can expect that 16 types are maybe too many to predict. Consequently, we can try to narrow it down. We have several options, to do it into each category (e.g. Introversion and Extroversion) or to do it for through Keirsey's Temperaments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be careful howeever, Keirsey's Temperaments are \"closely associated with the Myers–Briggs Type Indicator (MBTI); however, there are significant practical and theoretical differences between the two personality questionnaires and their associated different descriptions.\" [Wikipedia](https://en.wikipedia.org/wiki/Keirsey_Temperament_Sorter). If we use them we are assuming that the content of the posts assigned to each MBTI profile can be translated into KT. We will do this, but this needs to be kept in the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/Keirsey_Temperament_Sorter.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the Keirsey Temperaments into new columns\n",
    "\n",
    "NF = df['type'].str.contains('NF')\n",
    "df.insert(6, 'NF', NF)\n",
    "\n",
    "NT = df['type'].str.contains('NT')\n",
    "df.insert(7, 'NT', NT)\n",
    "\n",
    "SP = df['type'].str.contains('S.P', regex=True)\n",
    "df.insert(8, 'SP', SP)\n",
    "\n",
    "SJ = df['type'].str.contains('S.J', regex=True)\n",
    "df.insert(9, 'SJ', SJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is another method to extract the pair of letters into one column\n",
    "\n",
    "#IE = df.apply(lambda x:x[0][0], axis=1)\n",
    "#df.insert(5,'IE',IE)\n",
    "#NS = df.apply(lambda x:x[0][1], axis=1)\n",
    "#df.insert(6,'NS',NS)\n",
    "#FT = df.apply(lambda x:x[0][2], axis=1)\n",
    "#df.insert(7,'FT',FT)\n",
    "#JP = df.apply(lambda x:x[0][3], axis=1)\n",
    "#df.insert(8,'JP',JP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentions of other groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of self-referencing and mentioning other groups\n",
    "\n",
    "for type in types:\n",
    "    df[type + '_mentions'] = [sum([x.casefold().count(type.casefold()) for x in post]) for post in df['posts_separated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Mentions'] = df.iloc[:,15:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = [':D', ';D', ':)',';)',':(','xD','XD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This counts the number of emojis used by each user\n",
    "\n",
    "for emoji in emojis:\n",
    "    df[emoji + '_count'] = [sum([x.count(emoji) for x in post]) for post in df['posts_separated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Emojis'] = df.iloc[:,32:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation & URLs from posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://machinelearningmastery.com/clean-text-machine-learning-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the regular expressions package\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422845"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how many posts we have\n",
    "\n",
    "sum(df['count_posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['http://www.youtube.com/watch?v=qsXHcwe3krw, ...\n",
       "1    ['I'm finding the lack of me in these posts ve...\n",
       "2    ['Good one  _____   https://www.youtube.com/wa...\n",
       "3    ['Dear INTP,   I enjoyed our conversation the ...\n",
       "4    ['You're fired., That's another silly misconce...\n",
       "Name: posts_separated, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we review how our posts looked like\n",
    "\n",
    "df['posts_separated'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This removes all URLs \n",
    "\n",
    "posts = [[re.sub(r'http\\S+', '', word) for word in post] for post in df['posts_separated']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maketrans works in conjuction with translate will substitute the punctuation values above for no value\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = [[word.translate(table) for word in post] for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert all values to lowercase\n",
    "words = [[word.lower() for word in alist] for alist in stripped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the unnecessary spaces\n",
    "\n",
    "posts = []\n",
    "for x in words:\n",
    "    while '' in x:\n",
    "        x.remove('')\n",
    "    posts.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were some spaces left inside each post so with this we clean it\n",
    "\n",
    "posts = [[re.sub(' +', ' ', sentence).strip() for sentence in post] for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411495\n",
      "8675\n"
     ]
    }
   ],
   "source": [
    "# With the previous steps we lost around 11300 posts. This could be due to posts that were only urls\n",
    "\n",
    "count = 0\n",
    "for post in posts:\n",
    "    count += len(post)\n",
    "\n",
    "print(count)\n",
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tend to remove stopwords because they have low predicting capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we check what type of stopwords exist in the English language\n",
    "\n",
    "stopwords.words('English')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will divide each sentence into separate words\n",
    "\n",
    "word_tokens = [[word_tokenize(words) for words in post] for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will erase the stopwords from our list\n",
    "\n",
    "words = [[[word for word in post if word not in stopwords.words('english')] for post in posts] for posts in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['enfp', 'intj', 'moments', 'sportscenter', 'top', 'ten', 'plays', 'pranks'],\n",
       " ['lifechanging', 'experience', 'life'],\n",
       " ['repeat', 'today'],\n",
       " ['may', 'perc', 'experience', 'immerse'],\n",
       " ['last',\n",
       "  'thing',\n",
       "  'infj',\n",
       "  'friend',\n",
       "  'posted',\n",
       "  'facebook',\n",
       "  'committing',\n",
       "  'suicide',\n",
       "  'next',\n",
       "  'day',\n",
       "  'rest',\n",
       "  'peace']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check a bit to see if it worked well\n",
    "\n",
    "words[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411495\n",
      "8675\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in words:\n",
    "    count += len(word)\n",
    "\n",
    "print(count)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorten words back to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = ['Foot','feet','nights','waking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lemmatized = [[[lemmatizer.lemmatize(i) for i in elements] for elements in post] for post in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['enfp', 'intj', 'moment', 'sportscenter', 'top', 'ten', 'play', 'prank']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check if it worked well\n",
    "\n",
    "words_lemmatized[0][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign the new list of words to a column of our dataset\n",
    "\n",
    "df['posts_clean'] = words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts_separated</th>\n",
       "      <th>posts_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['http://www.youtube.com/watch?v=qsXHcwe3krw, ...</td>\n",
       "      <td>[[enfp, intj, moment, sportscenter, top, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['I'm finding the lack of me in these posts ve...</td>\n",
       "      <td>[[im, finding, lack, post, alarming], [sex, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Good one  _____   https://www.youtube.com/wa...</td>\n",
       "      <td>[[good, one], [course, say, know, thats, bless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Dear INTP,   I enjoyed our conversation the ...</td>\n",
       "      <td>[[dear, intp, enjoyed, conversation, day, esot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['You're fired., That's another silly misconce...</td>\n",
       "      <td>[[youre, fired], [thats, another, silly, misco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     posts_separated  \\\n",
       "0  ['http://www.youtube.com/watch?v=qsXHcwe3krw, ...   \n",
       "1  ['I'm finding the lack of me in these posts ve...   \n",
       "2  ['Good one  _____   https://www.youtube.com/wa...   \n",
       "3  ['Dear INTP,   I enjoyed our conversation the ...   \n",
       "4  ['You're fired., That's another silly misconce...   \n",
       "\n",
       "                                         posts_clean  \n",
       "0  [[enfp, intj, moment, sportscenter, top, ten, ...  \n",
       "1  [[im, finding, lack, post, alarming], [sex, bo...  \n",
       "2  [[good, one], [course, say, know, thats, bless...  \n",
       "3  [[dear, intp, enjoyed, conversation, day, esot...  \n",
       "4  [[youre, fired], [thats, another, silly, misco...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if it has worked. We compare both columns and it seem quite faithful to what we looked for. \n",
    "\n",
    "df[['posts_separated', 'posts_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Sentiments analysis is the field of study that analyzes people's opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes.\" B. Liu. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, 2012. [link](https://www.researchgate.net/post/What_is_the_best_way_to_do_a_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(lists):\n",
    "    \n",
    "    pos_counter = 0\n",
    "    neg_counter = 0\n",
    "    neu_counter = 0\n",
    "    \n",
    "    for post in lists:\n",
    "        analysis = TextBlob(post)\n",
    "        \n",
    "        if analysis.sentiment.polarity >= 0.6:\n",
    "            pos_counter += 1\n",
    "        elif analysis.sentiment.polarity <= -0.6:\n",
    "            neg_counter += 1\n",
    "        else:\n",
    "            neu_counter += 1\n",
    "   \n",
    "    return [pos_counter,neg_counter,neu_counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = [analyze_sentiment(posts) for posts in df['posts_clean'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "I take one of the results to analyze how it assigned each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 16]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing',\n",
       " 'moderation',\n",
       " 'sims',\n",
       " 'indeed',\n",
       " 'video',\n",
       " 'game',\n",
       " 'good',\n",
       " 'one',\n",
       " 'note',\n",
       " 'good',\n",
       " 'one',\n",
       " 'somewhat',\n",
       " 'subjective',\n",
       " 'completely',\n",
       " 'promoting',\n",
       " 'death',\n",
       " 'given',\n",
       " 'sim']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['posts_clean'][0][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It detected the two \"good\" as positives but left \"death\" out. I would say it did not do bad but not very realiable. I decided to test this same sample by with the NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df['posts_clean'][0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_NBA(text):\n",
    "    result = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
    "    classification, pos, neg = result.sentiment\n",
    "    print(classification)\n",
    "    print(pos)\n",
    "    print(neg)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing\n",
      "neg\n",
      "0.462648556876061\n",
      "0.5373514431239389\n",
      "-----\n",
      "moderation\n",
      "neg\n",
      "0.16666666666666646\n",
      "0.833333333333333\n",
      "-----\n",
      "sims\n",
      "pos\n",
      "0.6499999999999997\n",
      "0.35000000000000003\n",
      "-----\n",
      "indeed\n",
      "pos\n",
      "0.5576923076923077\n",
      "0.44230769230769224\n",
      "-----\n",
      "video\n",
      "neg\n",
      "0.39539748953974896\n",
      "0.6046025104602512\n",
      "-----\n",
      "game\n",
      "pos\n",
      "0.5530303030303029\n",
      "0.4469696969696968\n",
      "-----\n",
      "good\n",
      "pos\n",
      "0.5042265426880812\n",
      "0.4957734573119189\n",
      "-----\n",
      "one\n",
      "pos\n",
      "0.5061902082160945\n",
      "0.4938097917839054\n",
      "-----\n",
      "note\n",
      "pos\n",
      "0.5232558139534884\n",
      "0.4767441860465117\n",
      "-----\n",
      "good\n",
      "pos\n",
      "0.5042265426880812\n",
      "0.4957734573119189\n",
      "-----\n",
      "one\n",
      "pos\n",
      "0.5061902082160945\n",
      "0.4938097917839054\n",
      "-----\n",
      "somewhat\n",
      "pos\n",
      "0.6272727272727273\n",
      "0.3727272727272728\n",
      "-----\n",
      "subjective\n",
      "pos\n",
      "0.6499999999999997\n",
      "0.35000000000000003\n",
      "-----\n",
      "completely\n",
      "neg\n",
      "0.4661971830985916\n",
      "0.5338028169014085\n",
      "-----\n",
      "promoting\n",
      "pos\n",
      "0.5\n",
      "0.5\n",
      "-----\n",
      "death\n",
      "pos\n",
      "0.5542763157894737\n",
      "0.44572368421052644\n",
      "-----\n",
      "given\n",
      "neg\n",
      "0.4542079207920791\n",
      "0.5457920792079208\n",
      "-----\n",
      "sim\n",
      "pos\n",
      "0.5\n",
      "0.5\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for i in test:\n",
    "    print(i)\n",
    "    analyze_sentiment_NBA(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the NaiveBayesAnalyzer did even worst\n",
    "\n",
    "**I am not completely convinced about the results, so for now I will not add them as a column, maybe later we used other models or libraries to do it again**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Interim Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>N</th>\n",
       "      <th>T</th>\n",
       "      <th>NF</th>\n",
       "      <th>NT</th>\n",
       "      <th>SP</th>\n",
       "      <th>SJ</th>\n",
       "      <th>...</th>\n",
       "      <th>Total_Mentions</th>\n",
       "      <th>:D_count</th>\n",
       "      <th>;D_count</th>\n",
       "      <th>:)_count</th>\n",
       "      <th>;)_count</th>\n",
       "      <th>:(_count</th>\n",
       "      <th>xD_count</th>\n",
       "      <th>XD_count</th>\n",
       "      <th>Total_Emojis</th>\n",
       "      <th>posts_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[enfp, intj, moment, sportscenter, top, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>[[im, finding, lack, post, alarming], [sex, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[[good, one], [course, say, know, thats, bless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[dear, intp, enjoyed, conversation, day, esot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[[youre, fired], [thats, another, silly, misco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  I  J  N  T     NF  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...  1  1  1  0   True   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...  0  0  1  1  False   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...  1  0  1  1  False   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...  1  1  1  1  False   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...  0  1  1  1  False   \n",
       "\n",
       "      NT     SP     SJ  ...  Total_Mentions :D_count  ;D_count  :)_count  \\\n",
       "0  False  False  False  ...               8        0         0         0   \n",
       "1   True  False  False  ...              19        9         3         5   \n",
       "2   True  False  False  ...               4        2         0         7   \n",
       "3   True  False  False  ...              12        0         0         0   \n",
       "4   True  False  False  ...               5        0         0         0   \n",
       "\n",
       "   ;)_count  :(_count  xD_count  XD_count  Total_Emojis  \\\n",
       "0         0         0         0         0             0   \n",
       "1         0         0         0         0            17   \n",
       "2         0         0         0         0             9   \n",
       "3         0         0         0         0             0   \n",
       "4         1         0         0         1             2   \n",
       "\n",
       "                                         posts_clean  \n",
       "0  [[enfp, intj, moment, sportscenter, top, ten, ...  \n",
       "1  [[im, finding, lack, post, alarming], [sex, bo...  \n",
       "2  [[good, one], [course, say, know, thats, bless...  \n",
       "3  [[dear, intp, enjoyed, conversation, day, esot...  \n",
       "4  [[youre, fired], [thats, another, silly, misco...  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the current dataframe\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 41)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 41 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   type                 8675 non-null   object \n",
      " 1   posts                8675 non-null   object \n",
      " 2   I                    8675 non-null   int64  \n",
      " 3   J                    8675 non-null   int64  \n",
      " 4   N                    8675 non-null   int64  \n",
      " 5   T                    8675 non-null   int64  \n",
      " 6   NF                   8675 non-null   bool   \n",
      " 7   NT                   8675 non-null   bool   \n",
      " 8   SP                   8675 non-null   bool   \n",
      " 9   SJ                   8675 non-null   bool   \n",
      " 10  posts_len            8675 non-null   int64  \n",
      " 11  posts_separated      8675 non-null   object \n",
      " 12  count_posts          8675 non-null   int64  \n",
      " 13  avg_num_char_x_post  8675 non-null   float64\n",
      " 14  num_of_links         8675 non-null   int64  \n",
      " 15  INFJ_mentions        8675 non-null   int64  \n",
      " 16  ENTP_mentions        8675 non-null   int64  \n",
      " 17  INTP_mentions        8675 non-null   int64  \n",
      " 18  INTJ_mentions        8675 non-null   int64  \n",
      " 19  ENTJ_mentions        8675 non-null   int64  \n",
      " 20  ENFJ_mentions        8675 non-null   int64  \n",
      " 21  INFP_mentions        8675 non-null   int64  \n",
      " 22  ENFP_mentions        8675 non-null   int64  \n",
      " 23  ISFP_mentions        8675 non-null   int64  \n",
      " 24  ISTP_mentions        8675 non-null   int64  \n",
      " 25  ISFJ_mentions        8675 non-null   int64  \n",
      " 26  ISTJ_mentions        8675 non-null   int64  \n",
      " 27  ESTP_mentions        8675 non-null   int64  \n",
      " 28  ESFP_mentions        8675 non-null   int64  \n",
      " 29  ESTJ_mentions        8675 non-null   int64  \n",
      " 30  ESFJ_mentions        8675 non-null   int64  \n",
      " 31  Total_Mentions       8675 non-null   int64  \n",
      " 32  :D_count             8675 non-null   int64  \n",
      " 33  ;D_count             8675 non-null   int64  \n",
      " 34  :)_count             8675 non-null   int64  \n",
      " 35  ;)_count             8675 non-null   int64  \n",
      " 36  :(_count             8675 non-null   int64  \n",
      " 37  xD_count             8675 non-null   int64  \n",
      " 38  XD_count             8675 non-null   int64  \n",
      " 39  Total_Emojis         8675 non-null   int64  \n",
      " 40  posts_clean          8675 non-null   object \n",
      "dtypes: bool(4), float64(1), int64(32), object(4)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an interim report with Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = df.profile_report(sort='None', html={'style':{'full_width':True}}, progress_bar=False)\n",
    "report.to_file('mbti_report_interim.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the dataset is too large to upload to GitHub at this point... I have added **find ./* -size +100M | cat >> .gitignore\"** in the gitignore file to avoid uploading it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/mbti_interim.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
