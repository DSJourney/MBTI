{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project - Modeling (step 5)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Introduction</h3>\n",
    "    <p>This notebook contains the <b>Modeling</b> step which comes after the <b>Feature Engineering & Preprocessing</b> step. The main goal of this step involves selecting, training and deploying a model to make predictive insights.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-danger\">\n",
    "\n",
    "<h3>Disclaimer</h3>\n",
    "    <p>The purpose of this notebook is to go over certain aspects of Natural Language Processing. There might be some parts of the notebook that do not have particular use for the future of this project but they are useful for learning purposes so I left them inside. I also would like to mention that some of the code here is recycled from online articles and notebooks on GitHub, I will try to mention every source as best as possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Summarized goals](#goals)\n",
    "- [Importing Libraries](#importing)\n",
    "- [Review of our Dataset](#review)\n",
    "- [Models Introduction](#model)\n",
    "- [Parameters and Models](#parameters)\n",
    "- [Stopwords](#stopwords)\n",
    "- [Train Test Split](#train_test)\n",
    "- [CountVectorizer and tf-id](#cv)\n",
    "- [Report Function](#report)\n",
    "- [Let's Start Modeling](#modeling) Every model is created with CountVectorizer, TF-IDF words, TF-IDF n_grams, TF-IDF characters\n",
    "    - [MACHINE LEARNING](#ml)\n",
    "        - [Multinomial Naive Bayes Models](#nb)  \n",
    "        - [Logistic Regression](#lr)  \n",
    "        - [Support Vector Machines](#svm)  \n",
    "        - [K-Nearest Neightbors](#knn)  \n",
    "        - [Random Forest](#NB)      \n",
    "        - [Stocastic Gradient Descent](#sgd)\n",
    "        - [Boosting](#boost)\n",
    "            - [Gradient Boosting Classifier](#gbc)\n",
    "            - [XGBoost](#xgb)\n",
    "            - [Catboost](#cb) - Pending\n",
    "            - [Adaboost](#ab) - Pending        \n",
    "            - [LightGBM](#lgbm) - Pending       \n",
    "    - [DEEP LEARNING](#dl) - Pending all section\n",
    "        - [Shallow Neural Network](#snn)\n",
    "        - [Deep Neural Network](#dnn)\n",
    "        - [Transformers](#trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=goals></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarized Goals\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best model that classifies each post into the pair of attributes of the MBTI:\n",
    " - Introversion vs. Extraversion (I vs. E) --> This notebook focuses on this category\n",
    " - Intuition vs. Sensing (N vs. S)\n",
    " - Thinking vs. Feeling (T vs. F)\n",
    " - Judging vs. Perceiving (J vs. P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=importing></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# data wrangiling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.transforms\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set() #\n",
    "\n",
    "# natural language processing libraries\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "import textstat\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas(desc=\"Progress!\")\n",
    "import time\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/mbti_nlp.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Models Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece generates four dataframes one for each pair of attributes\n",
    "I = df[['I','text_clean_joined']]\n",
    "N = df[['N','text_clean_joined']]\n",
    "T = df[['T','text_clean_joined']]\n",
    "J = df[['J','text_clean_joined']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** I will be using `Christophe Pere's` notebook as the basis for this model. All credits go to him, [here is the original notebook](https://github.com/Christophe-pere/Model-Selection/blob/master/Text_Classification_Compare_Models.ipynb) and here is his [TowardsDataScience article](https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract the true, false positive and true false negative\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=parameters></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Models\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT           = \"text_clean_joined\"\n",
    "LABEL          = \"I\"\n",
    "NAME_SAVE_FILE = \"model_selection_results_EI\" # put just the name the .csv will be added at the end\n",
    "\n",
    "# global parameters\n",
    "num_gpu                = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "CV_splits              = 5        # Number of splits for cross-validation and k-folds\n",
    "save_results           = True     # if you want an output file containing all the results\n",
    "lang                   = False    # test if you want to use Google API detection (you will need to \"import from googletrans import Translator\")\n",
    "sample                 = True     # use just a sample of data\n",
    "nb_sample              = 6000     # default value of rows if sample selected\n",
    "save_model             = True     # concat all the data representation\n",
    "root_dir               = \"models/\"       # Place here the path where you want your models stored or use /path/to/your/folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names how the files will be saved as \n",
    "NAME_ENCODER                  = \"encoder.sav\"\n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_TOKEN_EMBEDDINGS         = \"token_embeddings.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "multinomial_naive_bayes= True\n",
    "logistic_regression    = True\n",
    "svm_model              = True\n",
    "k_nn_model             = True\n",
    "sgd                    = True\n",
    "random_forest          = True\n",
    "gradient_boosting      = True\n",
    "xgboost_classifier     = True\n",
    "adaboost_classifier    = True \n",
    "catboost_classifier    = True \n",
    "lightgbm_classifier    = False \n",
    "extratrees_classifier  = True\n",
    "shallow_network        = True\n",
    "deep_nn                = True\n",
    "rnn                    = True\n",
    "lstm                   = True\n",
    "cnn                    = True\n",
    "gru                    = True\n",
    "cnn_lstm               = True\n",
    "cnn_gru                = True\n",
    "bidirectional_rnn      = True\n",
    "bidirectional_lstm     = True\n",
    "bidirectional_gru      = True\n",
    "rcnn                   = True\n",
    "transformers           = False\n",
    "pre_trained            = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder can not be created\n"
     ]
    }
   ],
   "source": [
    "if save_model:\n",
    "    # will create the folder to save all the models\n",
    "    try:\n",
    "        dir_name =  NAME_SAVE_FILE\n",
    "        os.makedirs(os.path.join(root_dir,dir_name))\n",
    "        print(\"The folder is created\")\n",
    "    except:\n",
    "        print(\"The folder can not be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can put all the metrics you want (included in sklearn.metrics).\n",
    "score_metrics = {'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Christophe Pere` has a set of functions to clean the text but we have already done that so I will not add them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do add a remove stop words function\n",
    "def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBTI types are rarely discussed in day to day converstaions, we will take them out since they would have low prediction power\n",
    "types = [x.lower() for x in df['type'].unique()] \n",
    "types_plural = [x+'s' for x in types]\n",
    "\n",
    "# some words that appear a lot but do not add value\n",
    "additional_stop_words = ['ll','type','fe','ni','na','wa','ve','don','nt','nf', 'ti','se','op','ne'] \n",
    "\n",
    "# We put these together and include the normal stopwords from the English language\n",
    "stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS.union(additional_stop_words + types + types_plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress!: 100%|██████████| 8675/8675 [00:03<00:00, 2845.01it/s]\n"
     ]
    }
   ],
   "source": [
    "I[TEXT] = I.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the code simple (for the time being), I will start by focusing on the `Thinking / Feeling` and later implement the same process for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = I.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[TEXT], df[LABEL], test_size=0.25, random_state=42, stratify=df[LABEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Personal note on stratify:** if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(y_train),\n",
    "                                                 y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 2.1701\tclass: 0\n",
      "Class weight: 0.6497\tclass: 1\n"
     ]
    }
   ],
   "source": [
    "print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset is balanced (ratio=0.299)\n"
     ]
    }
   ],
   "source": [
    "# Determined if the dataset is balanced or imbalanced \n",
    "ratio = np.min(df[LABEL].value_counts()) / np.max(df[LABEL].value_counts())\n",
    "if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "    balanced = True\n",
    "    print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "else:\n",
    "    balanced = False\n",
    "    print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "    #from imblearn.over_sampling import ADASYN\n",
    "    # put class for debalanced data \n",
    "    # in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer & TF-IDF\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section transforms our data into something interpretable by the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.69 s, sys: 289 ms, total: 9.98 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df[TEXT])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "x_train_count =  count_vect.transform(X_train)\n",
    "x_test_count =  count_vect.transform(X_test)\n",
    "\n",
    "if save_model:\n",
    "    # save the model to disk\n",
    "    filename = NAME_COUNT_VECT_MODEL\n",
    "    pickle.dump(count_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n",
      "CPU times: user 2min 17s, sys: 2.57 s, total: 2min 19s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "tfidf_vect.fit(df[TEXT])\n",
    "x_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "x_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "print(\"word level tf-idf done\")\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect_ngram.fit(df[TEXT])\n",
    "x_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "x_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "print(\"ngram level tf-idf done\")\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) \n",
    "tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "x_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "x_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) \n",
    "print(\"characters level tf-idf done\")\n",
    "\n",
    "if save_model:\n",
    "    # save the model tf-idf to disk\n",
    "    filename = NAME_TF_IDF_MODEL\n",
    "    pickle.dump(tfidf_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "\n",
    "    # save the model ngram to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # save the model ngram char to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_CHAR_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram_chars, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Function\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followign function will generate, for each model we create, a set of metrics that evaluate how well that model did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, x, y, X_test, y_test, name='classifier', cv=5, dict_scoring=None, fit_params=None, save=save_model):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param clf: (model) classifier\n",
    "    @param x: (list or matrix or tensor) training x data\n",
    "    @param y: (list) label data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param cv: (int) number of fold for cross-validation (default 5)\n",
    "    @param dict_scoring: (dict) dictionary of metrics and names\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param save: (bool) determine if the model need to be saved\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    \n",
    "    '''{'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}'''\n",
    "    \n",
    "    \n",
    "    if dict_scoring!=None:\n",
    "        score = dict_scoring.copy() # save the original dictionary\n",
    "        for i in score.keys():\n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted') # make each function scorer\n",
    "                elif i==\"roc_auc\":\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted', multi_class=\"ovo\",needs_proba=True) # make each function scorer\n",
    "                else:\n",
    "                    score[i] = make_scorer(score[i]) # make each function scorer\n",
    "                    \n",
    "            else:\n",
    "                score[i] = make_scorer(score[i]) # make each function scorer\n",
    "            \n",
    "    try:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False, n_jobs=-1,  fit_params=fit_params)\n",
    "    except:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False,  fit_params=fit_params)\n",
    "        \n",
    "    # Train test on the overall data\n",
    "    fit_start = time.time()\n",
    "    _model = clf\n",
    "    _model.fit(x, y)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    \n",
    "    score_start = time.time()\n",
    "    y_pred = _model.predict(X_test) #>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    \n",
    "    # this saves the model for reuse\n",
    "    if save:\n",
    "        filename= name+\".sav\"\n",
    "        pickle.dump(_model, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # initialisation \n",
    "    index = []\n",
    "    value = []\n",
    "    index.append(\"Model\")\n",
    "    value.append(name)\n",
    "    for i in scores:  # loop on each metric generate text and values\n",
    "        if i == \"estimator\":\n",
    "            continue\n",
    "        for j in enumerate(scores[i]):\n",
    "            index.append(i+\"_cv\"+str(j[0]+1))\n",
    "            value.append(j[1])\n",
    "        \n",
    "        \n",
    "        index.append(i+\"_mean\")\n",
    "        value.append(np.mean(scores[i]))\n",
    "        index.append(i+\"_std\")\n",
    "        value.append(np.std(scores[i]))\n",
    "    \n",
    "     # add metrics averall dataset on the dictionary \n",
    "    \n",
    "    for i in scores:    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,fit_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,score_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(score_end)\n",
    "            continue\n",
    "        \n",
    "        scores[i] = np.append(scores[i] ,score[i.split(\"test_\")[-1]](_model, X_test, y_test))\n",
    "        index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "        value.append(scores[i][-1])\n",
    "    \n",
    "    return pd.DataFrame(data=value, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Modeling!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating the empty dataframe we will use to put the results of each model we create\n",
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Multinomial Naïve Bayes</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 178 ms, total: 1.38 s\n",
      "Wall time: 4.13 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if multinomial_naive_bayes:\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_count, y_train, x_test_count, y_test, name='NB_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf, y_train, x_test_tfidf, y_test, name='NB_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='NB_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='NB_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Logistic Regression</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 2.23 s, total: 29.5 s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if logistic_regression:\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_count, y_train, x_test_count, y_test, name='LR_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='LR_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='LR_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='LR_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Support Vector Machine</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 39min 29s, sys: 22.3 s, total: 1h 39min 51s\n",
      "Wall time: 3h 23min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if svm_model:\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_count, y_train, x_test_count, y_test, name='SVM_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SVM_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SVM_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SVM_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>K-Nearest Neighbors</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 43s, sys: 25.1 s, total: 18min 8s\n",
      "Wall time: 11min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if k_nn_model:\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_count, y_train, x_test_count, y_test, name='kNN_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf, y_train, x_test_tfidf, y_test, name='kNN_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='kNN_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='kNN_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Random Forest</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 30s, sys: 1.76 s, total: 3min 31s\n",
      "Wall time: 6min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_count, y_train, x_test_count, y_test, name='RF_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf, y_train, x_test_tfidf, y_test, name='RF_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='RF_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='RF_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Stocastis Gradient Descent</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 s, sys: 291 ms, total: 2.72 s\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if sgd:\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_count, y_train, x_test_count, y_test, name='SGD_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SGD_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SGD_N-Gram_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SGD_CharLevel_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boost'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gbc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Gradient Boosting Classifier</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 431 ms, total: 28.5 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_count, y_train, x_test_count, y_test,\n",
    "                                          name='GB_Count_Vectors', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 s, sys: 166 ms, total: 25.6 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf, y_train, x_test_tfidf, y_test,\n",
    "                                          name='GB_WordLevel_TF-IDF', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 s, sys: 54.2 ms, total: 4.01 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test,\n",
    "                                          name='GB_N-Gram_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 1.21 s, total: 2min 49s\n",
      "Wall time: 8min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,\n",
    "                                          name='GB_CharLevel_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>XGBoost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 10s, sys: 7.19 s, total: 14min 17s\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        # run on CPU\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 59s, sys: 3.22 s, total: 13min 2s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist', n_estimators=1000, subsample=0.8), x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8),x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 53s, sys: 1.47 s, total: 5min 55s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 13s, sys: 4.79 s, total: 46min 18s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.154731</td>\n",
       "      <td>0.149521</td>\n",
       "      <td>0.153974</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>0.129201</td>\n",
       "      <td>0.146928</td>\n",
       "      <td>0.00929068</td>\n",
       "      <td>0.0969021</td>\n",
       "      <td>0.0852909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786706</td>\n",
       "      <td>0.95027</td>\n",
       "      <td>0.860787</td>\n",
       "      <td>1586</td>\n",
       "      <td>70</td>\n",
       "      <td>430</td>\n",
       "      <td>83</td>\n",
       "      <td>0.119254</td>\n",
       "      <td>0.148479</td>\n",
       "      <td>0.545135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0429099</td>\n",
       "      <td>0.044893</td>\n",
       "      <td>0.041254</td>\n",
       "      <td>0.0468559</td>\n",
       "      <td>0.0409729</td>\n",
       "      <td>0.0433772</td>\n",
       "      <td>0.00223186</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.043107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769834</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86995</td>\n",
       "      <td>1669</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0030746</td>\n",
       "      <td>0.0392386</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.0192792</td>\n",
       "      <td>0.0190589</td>\n",
       "      <td>0.0250399</td>\n",
       "      <td>0.0250349</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>0.00299166</td>\n",
       "      <td>0.0342829</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769834</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86995</td>\n",
       "      <td>1669</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0030746</td>\n",
       "      <td>0.0392386</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.472794</td>\n",
       "      <td>0.470186</td>\n",
       "      <td>0.461374</td>\n",
       "      <td>0.442494</td>\n",
       "      <td>0.232661</td>\n",
       "      <td>0.415902</td>\n",
       "      <td>0.092234</td>\n",
       "      <td>0.0982971</td>\n",
       "      <td>0.105675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869724</td>\n",
       "      <td>1669</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.210946</td>\n",
       "      <td>0.214963</td>\n",
       "      <td>0.147904</td>\n",
       "      <td>0.148283</td>\n",
       "      <td>0.109263</td>\n",
       "      <td>0.166272</td>\n",
       "      <td>0.0406881</td>\n",
       "      <td>0.127938</td>\n",
       "      <td>0.114421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786706</td>\n",
       "      <td>0.95027</td>\n",
       "      <td>0.860787</td>\n",
       "      <td>1586</td>\n",
       "      <td>70</td>\n",
       "      <td>430</td>\n",
       "      <td>83</td>\n",
       "      <td>0.119254</td>\n",
       "      <td>0.148479</td>\n",
       "      <td>0.545135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0455952</td>\n",
       "      <td>0.0500968</td>\n",
       "      <td>0.0441449</td>\n",
       "      <td>0.0445528</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>0.0447109</td>\n",
       "      <td>0.00348974</td>\n",
       "      <td>0.0384078</td>\n",
       "      <td>0.0370991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769834</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86995</td>\n",
       "      <td>1669</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0030746</td>\n",
       "      <td>0.0392386</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.0198491</td>\n",
       "      <td>0.0185578</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.0185957</td>\n",
       "      <td>0.0197573</td>\n",
       "      <td>0.0189472</td>\n",
       "      <td>0.0007332</td>\n",
       "      <td>0.0372689</td>\n",
       "      <td>0.0480251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769834</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86995</td>\n",
       "      <td>1669</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0030746</td>\n",
       "      <td>0.0392386</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.538392</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>0.510148</td>\n",
       "      <td>0.484741</td>\n",
       "      <td>0.325935</td>\n",
       "      <td>0.470445</td>\n",
       "      <td>0.0745518</td>\n",
       "      <td>0.109467</td>\n",
       "      <td>0.117995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869724</td>\n",
       "      <td>1669</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>47.3204</td>\n",
       "      <td>44.5059</td>\n",
       "      <td>46.0987</td>\n",
       "      <td>47.0465</td>\n",
       "      <td>23.1795</td>\n",
       "      <td>41.6302</td>\n",
       "      <td>9.27771</td>\n",
       "      <td>0.051466</td>\n",
       "      <td>0.0875132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821631</td>\n",
       "      <td>0.869383</td>\n",
       "      <td>0.844833</td>\n",
       "      <td>1451</td>\n",
       "      <td>185</td>\n",
       "      <td>315</td>\n",
       "      <td>218</td>\n",
       "      <td>0.256832</td>\n",
       "      <td>0.259214</td>\n",
       "      <td>0.619691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>1.48734</td>\n",
       "      <td>1.68297</td>\n",
       "      <td>1.51936</td>\n",
       "      <td>1.25074</td>\n",
       "      <td>0.980241</td>\n",
       "      <td>1.38413</td>\n",
       "      <td>0.244608</td>\n",
       "      <td>0.0858183</td>\n",
       "      <td>0.0585301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779925</td>\n",
       "      <td>0.991612</td>\n",
       "      <td>0.873121</td>\n",
       "      <td>1655</td>\n",
       "      <td>33</td>\n",
       "      <td>467</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0843868</td>\n",
       "      <td>0.166649</td>\n",
       "      <td>0.528806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.684537</td>\n",
       "      <td>0.534637</td>\n",
       "      <td>0.458838</td>\n",
       "      <td>0.606826</td>\n",
       "      <td>0.411268</td>\n",
       "      <td>0.539221</td>\n",
       "      <td>0.0985307</td>\n",
       "      <td>0.057116</td>\n",
       "      <td>0.0634112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771694</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.86977</td>\n",
       "      <td>1663</td>\n",
       "      <td>8</td>\n",
       "      <td>492</td>\n",
       "      <td>6</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>0.0652414</td>\n",
       "      <td>0.506203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>5.45261</td>\n",
       "      <td>5.83657</td>\n",
       "      <td>6.47541</td>\n",
       "      <td>5.97975</td>\n",
       "      <td>3.95986</td>\n",
       "      <td>5.54084</td>\n",
       "      <td>0.855664</td>\n",
       "      <td>0.095552</td>\n",
       "      <td>0.0833299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.997004</td>\n",
       "      <td>0.870065</td>\n",
       "      <td>1664</td>\n",
       "      <td>8</td>\n",
       "      <td>492</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0197363</td>\n",
       "      <td>0.0709579</td>\n",
       "      <td>0.506502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>247.878</td>\n",
       "      <td>251.917</td>\n",
       "      <td>255.351</td>\n",
       "      <td>247.511</td>\n",
       "      <td>156.032</td>\n",
       "      <td>231.738</td>\n",
       "      <td>37.9616</td>\n",
       "      <td>69.5551</td>\n",
       "      <td>69.4657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869724</td>\n",
       "      <td>1669</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>271.039</td>\n",
       "      <td>272.495</td>\n",
       "      <td>273.979</td>\n",
       "      <td>272.38</td>\n",
       "      <td>165.544</td>\n",
       "      <td>251.087</td>\n",
       "      <td>42.7817</td>\n",
       "      <td>76.4207</td>\n",
       "      <td>76.3011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.871246</td>\n",
       "      <td>1668</td>\n",
       "      <td>8</td>\n",
       "      <td>492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0234735</td>\n",
       "      <td>0.100904</td>\n",
       "      <td>0.5077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>97.8841</td>\n",
       "      <td>98.0125</td>\n",
       "      <td>98.0493</td>\n",
       "      <td>98.0465</td>\n",
       "      <td>61.229</td>\n",
       "      <td>90.6443</td>\n",
       "      <td>14.7078</td>\n",
       "      <td>29.2065</td>\n",
       "      <td>29.1093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772052</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.869997</td>\n",
       "      <td>1663</td>\n",
       "      <td>9</td>\n",
       "      <td>491</td>\n",
       "      <td>6</td>\n",
       "      <td>0.021816</td>\n",
       "      <td>0.0732081</td>\n",
       "      <td>0.507203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>863.504</td>\n",
       "      <td>862.601</td>\n",
       "      <td>876.403</td>\n",
       "      <td>858.257</td>\n",
       "      <td>532.415</td>\n",
       "      <td>798.636</td>\n",
       "      <td>133.248</td>\n",
       "      <td>221.983</td>\n",
       "      <td>220.532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869724</td>\n",
       "      <td>1669</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.267212</td>\n",
       "      <td>0.249196</td>\n",
       "      <td>0.367538</td>\n",
       "      <td>0.125506</td>\n",
       "      <td>0.235649</td>\n",
       "      <td>0.0838758</td>\n",
       "      <td>5.20661</td>\n",
       "      <td>5.21617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77012</td>\n",
       "      <td>0.997603</td>\n",
       "      <td>0.869225</td>\n",
       "      <td>1665</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00550388</td>\n",
       "      <td>0.0267574</td>\n",
       "      <td>0.501802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.0521071</td>\n",
       "      <td>0.0591431</td>\n",
       "      <td>0.0407391</td>\n",
       "      <td>0.0561349</td>\n",
       "      <td>0.0430658</td>\n",
       "      <td>0.050238</td>\n",
       "      <td>0.00720053</td>\n",
       "      <td>5.23368</td>\n",
       "      <td>5.24002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771402</td>\n",
       "      <td>0.998802</td>\n",
       "      <td>0.870496</td>\n",
       "      <td>1667</td>\n",
       "      <td>6</td>\n",
       "      <td>494</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0164812</td>\n",
       "      <td>0.0750468</td>\n",
       "      <td>0.505401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.0158238</td>\n",
       "      <td>0.0159616</td>\n",
       "      <td>0.0171568</td>\n",
       "      <td>0.0470991</td>\n",
       "      <td>0.0131083</td>\n",
       "      <td>0.0218299</td>\n",
       "      <td>0.0127039</td>\n",
       "      <td>2.08124</td>\n",
       "      <td>2.11043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776217</td>\n",
       "      <td>0.993409</td>\n",
       "      <td>0.871485</td>\n",
       "      <td>1658</td>\n",
       "      <td>22</td>\n",
       "      <td>478</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0555937</td>\n",
       "      <td>0.128717</td>\n",
       "      <td>0.518705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.58512</td>\n",
       "      <td>0.616132</td>\n",
       "      <td>0.789871</td>\n",
       "      <td>0.761811</td>\n",
       "      <td>0.444807</td>\n",
       "      <td>0.639548</td>\n",
       "      <td>0.125682</td>\n",
       "      <td>48.5696</td>\n",
       "      <td>48.8013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770545</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870404</td>\n",
       "      <td>1669</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00920398</td>\n",
       "      <td>0.0679946</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>36.5056</td>\n",
       "      <td>36.0894</td>\n",
       "      <td>37.5275</td>\n",
       "      <td>37.3186</td>\n",
       "      <td>22.5465</td>\n",
       "      <td>33.9975</td>\n",
       "      <td>5.74949</td>\n",
       "      <td>0.636615</td>\n",
       "      <td>0.624244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869724</td>\n",
       "      <td>1669</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>34.0441</td>\n",
       "      <td>33.3765</td>\n",
       "      <td>34.2719</td>\n",
       "      <td>33.9564</td>\n",
       "      <td>13.5019</td>\n",
       "      <td>29.8301</td>\n",
       "      <td>8.16946</td>\n",
       "      <td>0.292704</td>\n",
       "      <td>0.291632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869724</td>\n",
       "      <td>1669</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>20.5807</td>\n",
       "      <td>20.1294</td>\n",
       "      <td>20.6832</td>\n",
       "      <td>20.5013</td>\n",
       "      <td>8.95976</td>\n",
       "      <td>18.1709</td>\n",
       "      <td>4.60936</td>\n",
       "      <td>0.197865</td>\n",
       "      <td>0.107601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773488</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.870909</td>\n",
       "      <td>1663</td>\n",
       "      <td>13</td>\n",
       "      <td>487</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0337884</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.511203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>74.2017</td>\n",
       "      <td>75.5478</td>\n",
       "      <td>73.6541</td>\n",
       "      <td>74.6046</td>\n",
       "      <td>32.2927</td>\n",
       "      <td>66.0602</td>\n",
       "      <td>16.895</td>\n",
       "      <td>0.834507</td>\n",
       "      <td>0.513479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76916</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>0.86884</td>\n",
       "      <td>1666</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.00275734</td>\n",
       "      <td>-0.0203699</td>\n",
       "      <td>0.499101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.605897</td>\n",
       "      <td>0.712129</td>\n",
       "      <td>0.655724</td>\n",
       "      <td>0.634643</td>\n",
       "      <td>0.409229</td>\n",
       "      <td>0.603524</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.0961499</td>\n",
       "      <td>0.0817678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803942</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.861829</td>\n",
       "      <td>1550</td>\n",
       "      <td>122</td>\n",
       "      <td>378</td>\n",
       "      <td>119</td>\n",
       "      <td>0.210972</td>\n",
       "      <td>0.231442</td>\n",
       "      <td>0.58635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.246552</td>\n",
       "      <td>0.37278</td>\n",
       "      <td>0.313224</td>\n",
       "      <td>0.323033</td>\n",
       "      <td>0.290029</td>\n",
       "      <td>0.309124</td>\n",
       "      <td>0.0413333</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.0771241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812158</td>\n",
       "      <td>0.888556</td>\n",
       "      <td>0.848641</td>\n",
       "      <td>1483</td>\n",
       "      <td>157</td>\n",
       "      <td>343</td>\n",
       "      <td>186</td>\n",
       "      <td>0.227582</td>\n",
       "      <td>0.233809</td>\n",
       "      <td>0.601278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.112686</td>\n",
       "      <td>0.125234</td>\n",
       "      <td>0.114001</td>\n",
       "      <td>0.111831</td>\n",
       "      <td>0.184672</td>\n",
       "      <td>0.129685</td>\n",
       "      <td>0.027918</td>\n",
       "      <td>0.0343139</td>\n",
       "      <td>0.0588868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792597</td>\n",
       "      <td>0.872379</td>\n",
       "      <td>0.830576</td>\n",
       "      <td>1456</td>\n",
       "      <td>119</td>\n",
       "      <td>381</td>\n",
       "      <td>213</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0.129114</td>\n",
       "      <td>0.555189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>2.0947</td>\n",
       "      <td>2.31269</td>\n",
       "      <td>2.08918</td>\n",
       "      <td>2.2355</td>\n",
       "      <td>1.20575</td>\n",
       "      <td>1.98757</td>\n",
       "      <td>0.400053</td>\n",
       "      <td>0.0868239</td>\n",
       "      <td>0.089726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.818454</td>\n",
       "      <td>0.827127</td>\n",
       "      <td>1366</td>\n",
       "      <td>232</td>\n",
       "      <td>268</td>\n",
       "      <td>303</td>\n",
       "      <td>0.275696</td>\n",
       "      <td>0.275968</td>\n",
       "      <td>0.641227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>23.8682</td>\n",
       "      <td>47.272</td>\n",
       "      <td>34.2535</td>\n",
       "      <td>25.7467</td>\n",
       "      <td>32.5488</td>\n",
       "      <td>32.7378</td>\n",
       "      <td>8.25978</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.0775878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.87011</td>\n",
       "      <td>1668</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00827598</td>\n",
       "      <td>0.053017</td>\n",
       "      <td>0.5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>59.7592</td>\n",
       "      <td>59.7172</td>\n",
       "      <td>35.6946</td>\n",
       "      <td>42.4886</td>\n",
       "      <td>31.118</td>\n",
       "      <td>45.7555</td>\n",
       "      <td>11.9765</td>\n",
       "      <td>0.0820928</td>\n",
       "      <td>0.085541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769373</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.869429</td>\n",
       "      <td>1668</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000921091</td>\n",
       "      <td>-0.0117551</td>\n",
       "      <td>0.4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>6.49788</td>\n",
       "      <td>6.40686</td>\n",
       "      <td>6.05427</td>\n",
       "      <td>6.00473</td>\n",
       "      <td>3.99496</td>\n",
       "      <td>5.79174</td>\n",
       "      <td>0.918661</td>\n",
       "      <td>0.067564</td>\n",
       "      <td>0.0726042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771759</td>\n",
       "      <td>0.998802</td>\n",
       "      <td>0.870723</td>\n",
       "      <td>1667</td>\n",
       "      <td>7</td>\n",
       "      <td>493</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.0838749</td>\n",
       "      <td>0.506401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>225.483</td>\n",
       "      <td>200.144</td>\n",
       "      <td>184.859</td>\n",
       "      <td>185.551</td>\n",
       "      <td>108.86</td>\n",
       "      <td>180.979</td>\n",
       "      <td>38.9488</td>\n",
       "      <td>0.122285</td>\n",
       "      <td>0.132237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770264</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.868861</td>\n",
       "      <td>1663</td>\n",
       "      <td>4</td>\n",
       "      <td>496</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00670687</td>\n",
       "      <td>0.0273865</td>\n",
       "      <td>0.502203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>53.9703</td>\n",
       "      <td>62.5619</td>\n",
       "      <td>35.8417</td>\n",
       "      <td>41.0871</td>\n",
       "      <td>25.0326</td>\n",
       "      <td>43.6987</td>\n",
       "      <td>13.2537</td>\n",
       "      <td>1.01168</td>\n",
       "      <td>0.774234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.79106</td>\n",
       "      <td>0.943679</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>1575</td>\n",
       "      <td>84</td>\n",
       "      <td>416</td>\n",
       "      <td>94</td>\n",
       "      <td>0.144201</td>\n",
       "      <td>0.171371</td>\n",
       "      <td>0.555839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>16.6876</td>\n",
       "      <td>16.8313</td>\n",
       "      <td>17.6103</td>\n",
       "      <td>17.7864</td>\n",
       "      <td>17.4274</td>\n",
       "      <td>17.2686</td>\n",
       "      <td>0.433343</td>\n",
       "      <td>0.410937</td>\n",
       "      <td>0.424165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794652</td>\n",
       "      <td>0.943679</td>\n",
       "      <td>0.862777</td>\n",
       "      <td>1575</td>\n",
       "      <td>93</td>\n",
       "      <td>407</td>\n",
       "      <td>94</td>\n",
       "      <td>0.166091</td>\n",
       "      <td>0.194585</td>\n",
       "      <td>0.564839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>5.82929</td>\n",
       "      <td>8.34168</td>\n",
       "      <td>7.70565</td>\n",
       "      <td>5.40848</td>\n",
       "      <td>3.62148</td>\n",
       "      <td>6.18132</td>\n",
       "      <td>1.68907</td>\n",
       "      <td>0.16689</td>\n",
       "      <td>0.0886142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785245</td>\n",
       "      <td>0.931096</td>\n",
       "      <td>0.851974</td>\n",
       "      <td>1554</td>\n",
       "      <td>75</td>\n",
       "      <td>425</td>\n",
       "      <td>115</td>\n",
       "      <td>0.103589</td>\n",
       "      <td>0.120813</td>\n",
       "      <td>0.540548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>163.045</td>\n",
       "      <td>118.365</td>\n",
       "      <td>113.336</td>\n",
       "      <td>157.956</td>\n",
       "      <td>86.9965</td>\n",
       "      <td>127.94</td>\n",
       "      <td>28.6868</td>\n",
       "      <td>1.09991</td>\n",
       "      <td>1.89776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779284</td>\n",
       "      <td>0.964649</td>\n",
       "      <td>0.862115</td>\n",
       "      <td>1610</td>\n",
       "      <td>44</td>\n",
       "      <td>456</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0729285</td>\n",
       "      <td>0.104261</td>\n",
       "      <td>0.526325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.154731     0.149521     0.153974     0.147213   \n",
       "0    NB_WordLevel_TF-IDF    0.0429099     0.044893     0.041254    0.0468559   \n",
       "0       NB_N-Gram_TF-IDF     0.018517    0.0192792    0.0190589    0.0250399   \n",
       "0    NB_CharLevel_TF-IDF     0.472794     0.470186     0.461374     0.442494   \n",
       "0       NB_Count_Vectors     0.210946     0.214963     0.147904     0.148283   \n",
       "0    NB_WordLevel_TF-IDF    0.0455952    0.0500968    0.0441449    0.0445528   \n",
       "0       NB_N-Gram_TF-IDF    0.0198491    0.0185578     0.017976    0.0185957   \n",
       "0    NB_CharLevel_TF-IDF     0.538392     0.493011     0.510148     0.484741   \n",
       "0       LR_Count_Vectors      47.3204      44.5059      46.0987      47.0465   \n",
       "0    LR_WordLevel_TF-IDF      1.48734      1.68297      1.51936      1.25074   \n",
       "0       LR_N-Gram_TF-IDF     0.684537     0.534637     0.458838     0.606826   \n",
       "0    LR_CharLevel_TF-IDF      5.45261      5.83657      6.47541      5.97975   \n",
       "0      SVM_Count_Vectors      247.878      251.917      255.351      247.511   \n",
       "0   SVM_WordLevel_TF-IDF      271.039      272.495      273.979       272.38   \n",
       "0      SVM_N-Gram_TF-IDF      97.8841      98.0125      98.0493      98.0465   \n",
       "0   SVM_CharLevel_TF-IDF      863.504      862.601      876.403      858.257   \n",
       "0      kNN_Count_Vectors     0.168792     0.267212     0.249196     0.367538   \n",
       "0   kNN_WordLevel_TF-IDF    0.0521071    0.0591431    0.0407391    0.0561349   \n",
       "0      kNN_N-Gram_TF-IDF    0.0158238    0.0159616    0.0171568    0.0470991   \n",
       "0   kNN_CharLevel_TF-IDF      0.58512     0.616132     0.789871     0.761811   \n",
       "0       RF_Count_Vectors      36.5056      36.0894      37.5275      37.3186   \n",
       "0    RF_WordLevel_TF-IDF      34.0441      33.3765      34.2719      33.9564   \n",
       "0       RF_N-Gram_TF-IDF      20.5807      20.1294      20.6832      20.5013   \n",
       "0    RF_CharLevel_TF-IDF      74.2017      75.5478      73.6541      74.6046   \n",
       "0      SGD_Count_Vectors     0.605897     0.712129     0.655724     0.634643   \n",
       "0   SGD_WordLevel_TF-IDF     0.246552      0.37278     0.313224     0.323033   \n",
       "0     SGD_N-Gram_Vectors     0.112686     0.125234     0.114001     0.111831   \n",
       "0  SGD_CharLevel_Vectors       2.0947      2.31269      2.08918       2.2355   \n",
       "0       GB_Count_Vectors      23.8682       47.272      34.2535      25.7467   \n",
       "0    GB_WordLevel_TF-IDF      59.7592      59.7172      35.6946      42.4886   \n",
       "0       GB_N-Gram_TF-IDF      6.49788      6.40686      6.05427      6.00473   \n",
       "0    GB_CharLevel_TF-IDF      225.483      200.144      184.859      185.551   \n",
       "0      XGB_Count_Vectors      53.9703      62.5619      35.8417      41.0871   \n",
       "0   XGB_WordLevel_TF-IDF      16.6876      16.8313      17.6103      17.7864   \n",
       "0      XGB_N-Gram_TF-IDF      5.82929      8.34168      7.70565      5.40848   \n",
       "0   XGB_CharLevel_TF-IDF      163.045      118.365      113.336      157.956   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0     0.129201      0.146928   0.00929068      0.0969021      0.0852909  ...   \n",
       "0    0.0409729     0.0433772   0.00223186       0.043998       0.043107  ...   \n",
       "0    0.0250349      0.021386   0.00299166      0.0342829       0.040082  ...   \n",
       "0     0.232661      0.415902     0.092234      0.0982971       0.105675  ...   \n",
       "0     0.109263      0.166272    0.0406881       0.127938       0.114421  ...   \n",
       "0     0.039165     0.0447109   0.00348974      0.0384078      0.0370991  ...   \n",
       "0    0.0197573     0.0189472    0.0007332      0.0372689      0.0480251  ...   \n",
       "0     0.325935      0.470445    0.0745518       0.109467       0.117995  ...   \n",
       "0      23.1795       41.6302      9.27771       0.051466      0.0875132  ...   \n",
       "0     0.980241       1.38413     0.244608      0.0858183      0.0585301  ...   \n",
       "0     0.411268      0.539221    0.0985307       0.057116      0.0634112  ...   \n",
       "0      3.95986       5.54084     0.855664       0.095552      0.0833299  ...   \n",
       "0      156.032       231.738      37.9616        69.5551        69.4657  ...   \n",
       "0      165.544       251.087      42.7817        76.4207        76.3011  ...   \n",
       "0       61.229       90.6443      14.7078        29.2065        29.1093  ...   \n",
       "0      532.415       798.636      133.248        221.983        220.532  ...   \n",
       "0     0.125506      0.235649    0.0838758        5.20661        5.21617  ...   \n",
       "0    0.0430658      0.050238   0.00720053        5.23368        5.24002  ...   \n",
       "0    0.0131083     0.0218299    0.0127039        2.08124        2.11043  ...   \n",
       "0     0.444807      0.639548     0.125682        48.5696        48.8013  ...   \n",
       "0      22.5465       33.9975      5.74949       0.636615       0.624244  ...   \n",
       "0      13.5019       29.8301      8.16946       0.292704       0.291632  ...   \n",
       "0      8.95976       18.1709      4.60936       0.197865       0.107601  ...   \n",
       "0      32.2927       66.0602       16.895       0.834507       0.513479  ...   \n",
       "0     0.409229      0.603524     0.103193      0.0961499      0.0817678  ...   \n",
       "0     0.290029      0.309124    0.0413333       0.086868      0.0771241  ...   \n",
       "0     0.184672      0.129685     0.027918      0.0343139      0.0588868  ...   \n",
       "0      1.20575       1.98757     0.400053      0.0868239       0.089726  ...   \n",
       "0      32.5488       32.7378      8.25978       0.102142      0.0775878  ...   \n",
       "0       31.118       45.7555      11.9765      0.0820928       0.085541  ...   \n",
       "0      3.99496       5.79174     0.918661       0.067564      0.0726042  ...   \n",
       "0       108.86       180.979      38.9488       0.122285       0.132237  ...   \n",
       "0      25.0326       43.6987      13.2537        1.01168       0.774234  ...   \n",
       "0      17.4274       17.2686     0.433343       0.410937       0.424165  ...   \n",
       "0      3.62148       6.18132      1.68907        0.16689      0.0886142  ...   \n",
       "0      86.9965        127.94      28.6868        1.09991        1.89776  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.786706        0.95027         0.860787       1586         70   \n",
       "0     0.769834              1          0.86995       1669          1   \n",
       "0     0.769834              1          0.86995       1669          1   \n",
       "0     0.769479              1         0.869724       1669          0   \n",
       "0     0.786706        0.95027         0.860787       1586         70   \n",
       "0     0.769834              1          0.86995       1669          1   \n",
       "0     0.769834              1          0.86995       1669          1   \n",
       "0     0.769479              1         0.869724       1669          0   \n",
       "0     0.821631       0.869383         0.844833       1451        185   \n",
       "0     0.779925       0.991612         0.873121       1655         33   \n",
       "0     0.771694       0.996405          0.86977       1663          8   \n",
       "0       0.7718       0.997004         0.870065       1664          8   \n",
       "0     0.769479              1         0.869724       1669          0   \n",
       "0     0.772222       0.999401         0.871246       1668          8   \n",
       "0     0.772052       0.996405         0.869997       1663          9   \n",
       "0     0.769479              1         0.869724       1669          0   \n",
       "0      0.77012       0.997603         0.869225       1665          3   \n",
       "0     0.771402       0.998802         0.870496       1667          6   \n",
       "0     0.776217       0.993409         0.871485       1658         22   \n",
       "0     0.770545              1         0.870404       1669          3   \n",
       "0     0.769479              1         0.869724       1669          0   \n",
       "0     0.769479              1         0.869724       1669          0   \n",
       "0     0.773488       0.996405         0.870909       1663         13   \n",
       "0      0.76916       0.998203          0.86884       1666          0   \n",
       "0     0.803942         0.9287         0.861829       1550        122   \n",
       "0     0.812158       0.888556         0.848641       1483        157   \n",
       "0     0.792597       0.872379         0.830576       1456        119   \n",
       "0     0.835985       0.818454         0.827127       1366        232   \n",
       "0     0.770439       0.999401          0.87011       1668          3   \n",
       "0     0.769373       0.999401         0.869429       1668          0   \n",
       "0     0.771759       0.998802         0.870723       1667          7   \n",
       "0     0.770264       0.996405         0.868861       1663          4   \n",
       "0      0.79106       0.943679         0.860656       1575         84   \n",
       "0     0.794652       0.943679         0.862777       1575         93   \n",
       "0     0.785245       0.931096         0.851974       1554         75   \n",
       "0     0.779284       0.964649         0.862115       1610         44   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        430         83             0.119254                  0.148479   \n",
       "0        499          0            0.0030746                 0.0392386   \n",
       "0        499          0            0.0030746                 0.0392386   \n",
       "0        500          0                    0                         0   \n",
       "0        430         83             0.119254                  0.148479   \n",
       "0        499          0            0.0030746                 0.0392386   \n",
       "0        499          0            0.0030746                 0.0392386   \n",
       "0        500          0                    0                         0   \n",
       "0        315        218             0.256832                  0.259214   \n",
       "0        467         14            0.0843868                  0.166649   \n",
       "0        492          6             0.018807                 0.0652414   \n",
       "0        492          5            0.0197363                 0.0709579   \n",
       "0        500          0                    0                         0   \n",
       "0        492          1            0.0234735                  0.100904   \n",
       "0        491          6             0.021816                 0.0732081   \n",
       "0        500          0                    0                         0   \n",
       "0        497          4           0.00550388                 0.0267574   \n",
       "0        494          2            0.0164812                 0.0750468   \n",
       "0        478         11            0.0555937                  0.128717   \n",
       "0        497          0           0.00920398                 0.0679946   \n",
       "0        500          0                    0                         0   \n",
       "0        500          0                    0                         0   \n",
       "0        487          6            0.0337884                  0.101266   \n",
       "0        500          3          -0.00275734                -0.0203699   \n",
       "0        378        119             0.210972                  0.231442   \n",
       "0        343        186             0.227582                  0.233809   \n",
       "0        381        213             0.125099                  0.129114   \n",
       "0        268        303             0.275696                  0.275968   \n",
       "0        497          1           0.00827598                  0.053017   \n",
       "0        500          1         -0.000921091                -0.0117551   \n",
       "0        493          2             0.019512                 0.0838749   \n",
       "0        496          6           0.00670687                 0.0273865   \n",
       "0        416         94             0.144201                  0.171371   \n",
       "0        407         94             0.166091                  0.194585   \n",
       "0        425        115             0.103589                  0.120813   \n",
       "0        456         59            0.0729285                  0.104261   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0        0.545135  \n",
       "0           0.501  \n",
       "0           0.501  \n",
       "0             0.5  \n",
       "0        0.545135  \n",
       "0           0.501  \n",
       "0           0.501  \n",
       "0             0.5  \n",
       "0        0.619691  \n",
       "0        0.528806  \n",
       "0        0.506203  \n",
       "0        0.506502  \n",
       "0             0.5  \n",
       "0          0.5077  \n",
       "0        0.507203  \n",
       "0             0.5  \n",
       "0        0.501802  \n",
       "0        0.505401  \n",
       "0        0.518705  \n",
       "0           0.503  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0        0.511203  \n",
       "0        0.499101  \n",
       "0         0.58635  \n",
       "0        0.601278  \n",
       "0        0.555189  \n",
       "0        0.641227  \n",
       "0          0.5027  \n",
       "0          0.4997  \n",
       "0        0.506401  \n",
       "0        0.502203  \n",
       "0        0.555839  \n",
       "0        0.564839  \n",
       "0        0.540548  \n",
       "0        0.526325  \n",
       "\n",
       "[36 rows x 113 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Catboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if catboost_classifier:\n",
    "    # work in progress\n",
    "    if num_gpu>0:  # test gpu available\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "    else:\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Adaboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if adaboost_classifier:\n",
    "    # work in progress\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_count, y_train, x_test_count, y_test, name='Adaboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Adaboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Adaboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Adaboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lgbm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>LightGBM</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if lightgbm_classifier:\n",
    "    \n",
    "    # work in progress\n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.083657</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>0.0876408</td>\n",
       "      <td>0.0806589</td>\n",
       "      <td>0.0848186</td>\n",
       "      <td>0.00268425</td>\n",
       "      <td>0.0557067</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791759</td>\n",
       "      <td>0.714573</td>\n",
       "      <td>0.751189</td>\n",
       "      <td>711</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>284</td>\n",
       "      <td>0.559446</td>\n",
       "      <td>0.561763</td>\n",
       "      <td>0.777644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0570619</td>\n",
       "      <td>0.0624511</td>\n",
       "      <td>0.0512819</td>\n",
       "      <td>0.0743842</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.0559588</td>\n",
       "      <td>0.0131171</td>\n",
       "      <td>0.0463171</td>\n",
       "      <td>0.0433159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.58191</td>\n",
       "      <td>0.681176</td>\n",
       "      <td>579</td>\n",
       "      <td>1048</td>\n",
       "      <td>126</td>\n",
       "      <td>416</td>\n",
       "      <td>0.485369</td>\n",
       "      <td>0.504886</td>\n",
       "      <td>0.737292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.0146081</td>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.0386279</td>\n",
       "      <td>0.0167282</td>\n",
       "      <td>0.0169821</td>\n",
       "      <td>0.0236558</td>\n",
       "      <td>0.00956548</td>\n",
       "      <td>0.0413001</td>\n",
       "      <td>0.0396559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730563</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>0.626077</td>\n",
       "      <td>545</td>\n",
       "      <td>973</td>\n",
       "      <td>201</td>\n",
       "      <td>450</td>\n",
       "      <td>0.383852</td>\n",
       "      <td>0.394977</td>\n",
       "      <td>0.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.849732</td>\n",
       "      <td>0.797022</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.13063</td>\n",
       "      <td>0.286705</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.303969</td>\n",
       "      <td>0.125775</td>\n",
       "      <td>0.0687079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.188948</td>\n",
       "      <td>106</td>\n",
       "      <td>1153</td>\n",
       "      <td>21</td>\n",
       "      <td>889</td>\n",
       "      <td>0.0949604</td>\n",
       "      <td>0.188135</td>\n",
       "      <td>0.544323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>18.4121</td>\n",
       "      <td>19.1657</td>\n",
       "      <td>17.922</td>\n",
       "      <td>19.1001</td>\n",
       "      <td>9.19119</td>\n",
       "      <td>16.7582</td>\n",
       "      <td>3.81129</td>\n",
       "      <td>0.0704119</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738872</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.744766</td>\n",
       "      <td>747</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>248</td>\n",
       "      <td>0.525238</td>\n",
       "      <td>0.525295</td>\n",
       "      <td>0.762941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.667456</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.61599</td>\n",
       "      <td>0.690176</td>\n",
       "      <td>0.490711</td>\n",
       "      <td>0.626685</td>\n",
       "      <td>0.0722426</td>\n",
       "      <td>0.0417788</td>\n",
       "      <td>0.0403888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771372</td>\n",
       "      <td>0.779899</td>\n",
       "      <td>0.775612</td>\n",
       "      <td>776</td>\n",
       "      <td>944</td>\n",
       "      <td>230</td>\n",
       "      <td>219</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>0.583527</td>\n",
       "      <td>0.791994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.495503</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.355603</td>\n",
       "      <td>0.409944</td>\n",
       "      <td>0.34902</td>\n",
       "      <td>0.407818</td>\n",
       "      <td>0.0535282</td>\n",
       "      <td>0.0507712</td>\n",
       "      <td>0.0475352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701226</td>\n",
       "      <td>0.632161</td>\n",
       "      <td>0.664905</td>\n",
       "      <td>629</td>\n",
       "      <td>906</td>\n",
       "      <td>268</td>\n",
       "      <td>366</td>\n",
       "      <td>0.406937</td>\n",
       "      <td>0.408658</td>\n",
       "      <td>0.701941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>4.52604</td>\n",
       "      <td>5.30917</td>\n",
       "      <td>5.46984</td>\n",
       "      <td>5.11398</td>\n",
       "      <td>3.58113</td>\n",
       "      <td>4.80003</td>\n",
       "      <td>0.68807</td>\n",
       "      <td>0.07074</td>\n",
       "      <td>0.0679901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.730192</td>\n",
       "      <td>705</td>\n",
       "      <td>943</td>\n",
       "      <td>231</td>\n",
       "      <td>290</td>\n",
       "      <td>0.514104</td>\n",
       "      <td>0.514884</td>\n",
       "      <td>0.75589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>142.24</td>\n",
       "      <td>143.032</td>\n",
       "      <td>143.314</td>\n",
       "      <td>144.206</td>\n",
       "      <td>113.892</td>\n",
       "      <td>137.337</td>\n",
       "      <td>11.7393</td>\n",
       "      <td>45.6131</td>\n",
       "      <td>45.6117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750984</td>\n",
       "      <td>0.766834</td>\n",
       "      <td>0.758826</td>\n",
       "      <td>763</td>\n",
       "      <td>921</td>\n",
       "      <td>253</td>\n",
       "      <td>232</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.55055</td>\n",
       "      <td>0.775666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>171.507</td>\n",
       "      <td>164.749</td>\n",
       "      <td>171.223</td>\n",
       "      <td>171.68</td>\n",
       "      <td>118.665</td>\n",
       "      <td>159.565</td>\n",
       "      <td>20.6152</td>\n",
       "      <td>51.6323</td>\n",
       "      <td>49.8943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760194</td>\n",
       "      <td>0.786935</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>783</td>\n",
       "      <td>927</td>\n",
       "      <td>247</td>\n",
       "      <td>212</td>\n",
       "      <td>0.575001</td>\n",
       "      <td>0.575303</td>\n",
       "      <td>0.788271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>67.1528</td>\n",
       "      <td>65.9447</td>\n",
       "      <td>66.7256</td>\n",
       "      <td>65.973</td>\n",
       "      <td>32.62</td>\n",
       "      <td>59.6832</td>\n",
       "      <td>13.5394</td>\n",
       "      <td>24.1372</td>\n",
       "      <td>25.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.621106</td>\n",
       "      <td>0.658147</td>\n",
       "      <td>618</td>\n",
       "      <td>909</td>\n",
       "      <td>265</td>\n",
       "      <td>377</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.401015</td>\n",
       "      <td>0.697691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>567.083</td>\n",
       "      <td>577.67</td>\n",
       "      <td>578.479</td>\n",
       "      <td>579.833</td>\n",
       "      <td>336.997</td>\n",
       "      <td>528.013</td>\n",
       "      <td>95.6156</td>\n",
       "      <td>186.596</td>\n",
       "      <td>184.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>0.759799</td>\n",
       "      <td>0.758655</td>\n",
       "      <td>756</td>\n",
       "      <td>932</td>\n",
       "      <td>242</td>\n",
       "      <td>239</td>\n",
       "      <td>0.553539</td>\n",
       "      <td>0.553541</td>\n",
       "      <td>0.776833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.072355</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0731351</td>\n",
       "      <td>0.0939651</td>\n",
       "      <td>0.0768281</td>\n",
       "      <td>0.00857692</td>\n",
       "      <td>3.7683</td>\n",
       "      <td>3.65425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586263</td>\n",
       "      <td>0.720603</td>\n",
       "      <td>0.646528</td>\n",
       "      <td>717</td>\n",
       "      <td>668</td>\n",
       "      <td>506</td>\n",
       "      <td>278</td>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.290993</td>\n",
       "      <td>0.644799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.049027</td>\n",
       "      <td>0.0487049</td>\n",
       "      <td>0.051249</td>\n",
       "      <td>0.0619388</td>\n",
       "      <td>0.0523279</td>\n",
       "      <td>0.0526495</td>\n",
       "      <td>0.00483814</td>\n",
       "      <td>4.03613</td>\n",
       "      <td>3.9824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751938</td>\n",
       "      <td>0.0974874</td>\n",
       "      <td>0.172598</td>\n",
       "      <td>97</td>\n",
       "      <td>1142</td>\n",
       "      <td>32</td>\n",
       "      <td>898</td>\n",
       "      <td>0.0752212</td>\n",
       "      <td>0.147965</td>\n",
       "      <td>0.535115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.0136449</td>\n",
       "      <td>0.020715</td>\n",
       "      <td>0.0663297</td>\n",
       "      <td>0.0168662</td>\n",
       "      <td>0.0261462</td>\n",
       "      <td>0.0202725</td>\n",
       "      <td>1.33747</td>\n",
       "      <td>1.33642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584323</td>\n",
       "      <td>0.494472</td>\n",
       "      <td>0.535656</td>\n",
       "      <td>492</td>\n",
       "      <td>824</td>\n",
       "      <td>350</td>\n",
       "      <td>503</td>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.20076</td>\n",
       "      <td>0.598173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.655702</td>\n",
       "      <td>0.672649</td>\n",
       "      <td>0.65748</td>\n",
       "      <td>0.667741</td>\n",
       "      <td>0.195899</td>\n",
       "      <td>0.569894</td>\n",
       "      <td>0.187104</td>\n",
       "      <td>48.1581</td>\n",
       "      <td>48.1758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739785</td>\n",
       "      <td>0.345729</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>344</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>651</td>\n",
       "      <td>0.252933</td>\n",
       "      <td>0.294636</td>\n",
       "      <td>0.621331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.458804</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.420624</td>\n",
       "      <td>0.444952</td>\n",
       "      <td>0.266678</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.0751494</td>\n",
       "      <td>0.0419219</td>\n",
       "      <td>0.0525622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>0.872362</td>\n",
       "      <td>0.747631</td>\n",
       "      <td>868</td>\n",
       "      <td>715</td>\n",
       "      <td>459</td>\n",
       "      <td>127</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>0.492212</td>\n",
       "      <td>0.740695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>0.269268</td>\n",
       "      <td>0.277042</td>\n",
       "      <td>0.274197</td>\n",
       "      <td>0.175028</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.0459828</td>\n",
       "      <td>0.03966</td>\n",
       "      <td>0.039535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742604</td>\n",
       "      <td>0.756784</td>\n",
       "      <td>0.749627</td>\n",
       "      <td>753</td>\n",
       "      <td>913</td>\n",
       "      <td>261</td>\n",
       "      <td>242</td>\n",
       "      <td>0.53369</td>\n",
       "      <td>0.533773</td>\n",
       "      <td>0.767234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.118953</td>\n",
       "      <td>0.126627</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.105808</td>\n",
       "      <td>0.0728922</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>0.0183905</td>\n",
       "      <td>0.0420618</td>\n",
       "      <td>0.03441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.678392</td>\n",
       "      <td>0.65534</td>\n",
       "      <td>675</td>\n",
       "      <td>784</td>\n",
       "      <td>390</td>\n",
       "      <td>320</td>\n",
       "      <td>0.344348</td>\n",
       "      <td>0.345069</td>\n",
       "      <td>0.673097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.92091</td>\n",
       "      <td>1.93717</td>\n",
       "      <td>1.7542</td>\n",
       "      <td>1.82993</td>\n",
       "      <td>1.36733</td>\n",
       "      <td>1.76191</td>\n",
       "      <td>0.208027</td>\n",
       "      <td>0.0974619</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828369</td>\n",
       "      <td>0.586935</td>\n",
       "      <td>0.687059</td>\n",
       "      <td>584</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>411</td>\n",
       "      <td>0.494864</td>\n",
       "      <td>0.514763</td>\n",
       "      <td>0.741934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>62.9065</td>\n",
       "      <td>55.3991</td>\n",
       "      <td>56.9035</td>\n",
       "      <td>52.061</td>\n",
       "      <td>27.6382</td>\n",
       "      <td>50.9817</td>\n",
       "      <td>12.189</td>\n",
       "      <td>0.0316858</td>\n",
       "      <td>0.0516238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698055</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.691684</td>\n",
       "      <td>682</td>\n",
       "      <td>879</td>\n",
       "      <td>295</td>\n",
       "      <td>313</td>\n",
       "      <td>0.434749</td>\n",
       "      <td>0.43481</td>\n",
       "      <td>0.717075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>5.80552</td>\n",
       "      <td>7.31059</td>\n",
       "      <td>7.17226</td>\n",
       "      <td>6.93111</td>\n",
       "      <td>5.58752</td>\n",
       "      <td>6.5614</td>\n",
       "      <td>0.719849</td>\n",
       "      <td>0.0432222</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.566632</td>\n",
       "      <td>540</td>\n",
       "      <td>803</td>\n",
       "      <td>371</td>\n",
       "      <td>455</td>\n",
       "      <td>0.228168</td>\n",
       "      <td>0.228875</td>\n",
       "      <td>0.61335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>220.842</td>\n",
       "      <td>238.157</td>\n",
       "      <td>282.195</td>\n",
       "      <td>257.065</td>\n",
       "      <td>184.308</td>\n",
       "      <td>236.513</td>\n",
       "      <td>33.1146</td>\n",
       "      <td>0.0761411</td>\n",
       "      <td>0.0745621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719447</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.69938</td>\n",
       "      <td>677</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>318</td>\n",
       "      <td>0.457422</td>\n",
       "      <td>0.458003</td>\n",
       "      <td>0.727765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>92.4755</td>\n",
       "      <td>84.2349</td>\n",
       "      <td>73.6215</td>\n",
       "      <td>84.1027</td>\n",
       "      <td>48.617</td>\n",
       "      <td>76.6103</td>\n",
       "      <td>15.2219</td>\n",
       "      <td>0.0370519</td>\n",
       "      <td>0.0738072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>700</td>\n",
       "      <td>894</td>\n",
       "      <td>280</td>\n",
       "      <td>295</td>\n",
       "      <td>0.465552</td>\n",
       "      <td>0.465597</td>\n",
       "      <td>0.732508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>74.1953</td>\n",
       "      <td>65.7518</td>\n",
       "      <td>84.2132</td>\n",
       "      <td>65.1882</td>\n",
       "      <td>32.8804</td>\n",
       "      <td>64.4458</td>\n",
       "      <td>17.2304</td>\n",
       "      <td>0.516725</td>\n",
       "      <td>0.95696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.751759</td>\n",
       "      <td>0.748749</td>\n",
       "      <td>748</td>\n",
       "      <td>919</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>0.534225</td>\n",
       "      <td>0.53424</td>\n",
       "      <td>0.767276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>46.6946</td>\n",
       "      <td>49.373</td>\n",
       "      <td>52.0918</td>\n",
       "      <td>37.7714</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>42.1696</td>\n",
       "      <td>9.87732</td>\n",
       "      <td>0.425461</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727728</td>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.729188</td>\n",
       "      <td>727</td>\n",
       "      <td>902</td>\n",
       "      <td>272</td>\n",
       "      <td>268</td>\n",
       "      <td>0.498814</td>\n",
       "      <td>0.498817</td>\n",
       "      <td>0.749483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>14.4524</td>\n",
       "      <td>14.8459</td>\n",
       "      <td>10.1767</td>\n",
       "      <td>6.22884</td>\n",
       "      <td>11.4074</td>\n",
       "      <td>11.4223</td>\n",
       "      <td>3.14409</td>\n",
       "      <td>0.109201</td>\n",
       "      <td>0.0818152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640324</td>\n",
       "      <td>0.635176</td>\n",
       "      <td>0.63774</td>\n",
       "      <td>632</td>\n",
       "      <td>819</td>\n",
       "      <td>355</td>\n",
       "      <td>363</td>\n",
       "      <td>0.332995</td>\n",
       "      <td>0.333004</td>\n",
       "      <td>0.666395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>307.123</td>\n",
       "      <td>379.482</td>\n",
       "      <td>492.242</td>\n",
       "      <td>429.361</td>\n",
       "      <td>213.657</td>\n",
       "      <td>364.373</td>\n",
       "      <td>96.7421</td>\n",
       "      <td>3.22035</td>\n",
       "      <td>2.00201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744376</td>\n",
       "      <td>0.731658</td>\n",
       "      <td>0.737962</td>\n",
       "      <td>728</td>\n",
       "      <td>924</td>\n",
       "      <td>250</td>\n",
       "      <td>267</td>\n",
       "      <td>0.519388</td>\n",
       "      <td>0.519452</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>31.9763</td>\n",
       "      <td>31.7864</td>\n",
       "      <td>31.8414</td>\n",
       "      <td>31.192</td>\n",
       "      <td>8.30906</td>\n",
       "      <td>27.021</td>\n",
       "      <td>9.35985</td>\n",
       "      <td>0.502497</td>\n",
       "      <td>0.662884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.527638</td>\n",
       "      <td>0.629874</td>\n",
       "      <td>525</td>\n",
       "      <td>1027</td>\n",
       "      <td>147</td>\n",
       "      <td>470</td>\n",
       "      <td>0.412637</td>\n",
       "      <td>0.433646</td>\n",
       "      <td>0.701213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>24.037</td>\n",
       "      <td>24.4264</td>\n",
       "      <td>24.231</td>\n",
       "      <td>24.4041</td>\n",
       "      <td>6.19139</td>\n",
       "      <td>20.658</td>\n",
       "      <td>7.23465</td>\n",
       "      <td>0.252068</td>\n",
       "      <td>0.331617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751989</td>\n",
       "      <td>0.569849</td>\n",
       "      <td>0.64837</td>\n",
       "      <td>567</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>428</td>\n",
       "      <td>0.418288</td>\n",
       "      <td>0.429599</td>\n",
       "      <td>0.705282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>16.2884</td>\n",
       "      <td>16.1502</td>\n",
       "      <td>16.4692</td>\n",
       "      <td>16.4074</td>\n",
       "      <td>4.51221</td>\n",
       "      <td>13.9655</td>\n",
       "      <td>4.72788</td>\n",
       "      <td>0.128438</td>\n",
       "      <td>0.119955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615965</td>\n",
       "      <td>0.573869</td>\n",
       "      <td>0.594173</td>\n",
       "      <td>571</td>\n",
       "      <td>818</td>\n",
       "      <td>356</td>\n",
       "      <td>424</td>\n",
       "      <td>0.27205</td>\n",
       "      <td>0.272599</td>\n",
       "      <td>0.635316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>76.4607</td>\n",
       "      <td>76.9057</td>\n",
       "      <td>76.4275</td>\n",
       "      <td>76.0929</td>\n",
       "      <td>17.0665</td>\n",
       "      <td>64.5907</td>\n",
       "      <td>23.7635</td>\n",
       "      <td>1.07035</td>\n",
       "      <td>0.870651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729829</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.658577</td>\n",
       "      <td>597</td>\n",
       "      <td>953</td>\n",
       "      <td>221</td>\n",
       "      <td>398</td>\n",
       "      <td>0.417415</td>\n",
       "      <td>0.423331</td>\n",
       "      <td>0.705877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.083657     0.087841     0.084295    0.0876408   \n",
       "0    NB_WordLevel_TF-IDF    0.0570619    0.0624511    0.0512819    0.0743842   \n",
       "0       NB_N-Gram_TF-IDF    0.0146081     0.031333    0.0386279    0.0167282   \n",
       "0    NB_CharLevel_TF-IDF     0.849732     0.797022       1.1039      1.13063   \n",
       "0       LR_Count_Vectors      18.4121      19.1657       17.922      19.1001   \n",
       "0    LR_WordLevel_TF-IDF     0.667456     0.669091      0.61599     0.690176   \n",
       "0       LR_N-Gram_TF-IDF     0.495503     0.429021     0.355603     0.409944   \n",
       "0    LR_CharLevel_TF-IDF      4.52604      5.30917      5.46984      5.11398   \n",
       "0      SVM_Count_Vectors       142.24      143.032      143.314      144.206   \n",
       "0   SVM_WordLevel_TF-IDF      171.507      164.749      171.223       171.68   \n",
       "0      SVM_N-Gram_TF-IDF      67.1528      65.9447      66.7256       65.973   \n",
       "0   SVM_CharLevel_TF-IDF      567.083       577.67      578.479      579.833   \n",
       "0      kNN_Count_Vectors     0.071985     0.072355       0.0727    0.0731351   \n",
       "0   kNN_WordLevel_TF-IDF     0.049027    0.0487049     0.051249    0.0619388   \n",
       "0      kNN_N-Gram_TF-IDF     0.013175    0.0136449     0.020715    0.0663297   \n",
       "0   kNN_CharLevel_TF-IDF     0.655702     0.672649      0.65748     0.667741   \n",
       "0      SGD_Count_Vectors     0.458804     0.473416     0.420624     0.444952   \n",
       "0   SGD_WordLevel_TF-IDF       0.3128     0.269268     0.277042     0.274197   \n",
       "0     SGD_N-Gram_Vectors     0.118953     0.126627       0.1075     0.105808   \n",
       "0  SGD_CharLevel_Vectors      1.92091      1.93717       1.7542      1.82993   \n",
       "0       GB_Count_Vectors      62.9065      55.3991      56.9035       52.061   \n",
       "0       GB_N-Gram_TF-IDF      5.80552      7.31059      7.17226      6.93111   \n",
       "0    GB_CharLevel_TF-IDF      220.842      238.157      282.195      257.065   \n",
       "0    GB_WordLevel_TF-IDF      92.4755      84.2349      73.6215      84.1027   \n",
       "0      XGB_Count_Vectors      74.1953      65.7518      84.2132      65.1882   \n",
       "0   XGB_WordLevel_TF-IDF      46.6946       49.373      52.0918      37.7714   \n",
       "0      XGB_N-Gram_TF-IDF      14.4524      14.8459      10.1767      6.22884   \n",
       "0   XGB_CharLevel_TF-IDF      307.123      379.482      492.242      429.361   \n",
       "0       RF_Count_Vectors      31.9763      31.7864      31.8414       31.192   \n",
       "0    RF_WordLevel_TF-IDF       24.037      24.4264       24.231      24.4041   \n",
       "0       RF_N-Gram_TF-IDF      16.2884      16.1502      16.4692      16.4074   \n",
       "0    RF_CharLevel_TF-IDF      76.4607      76.9057      76.4275      76.0929   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0806589     0.0848186   0.00268425      0.0557067      0.0472348  ...   \n",
       "0     0.034615     0.0559588    0.0131171      0.0463171      0.0433159  ...   \n",
       "0    0.0169821     0.0236558   0.00956548      0.0413001      0.0396559  ...   \n",
       "0     0.286705      0.833599     0.303969       0.125775      0.0687079  ...   \n",
       "0      9.19119       16.7582      3.81129      0.0704119      0.0472348  ...   \n",
       "0     0.490711      0.626685    0.0722426      0.0417788      0.0403888  ...   \n",
       "0      0.34902      0.407818    0.0535282      0.0507712      0.0475352  ...   \n",
       "0      3.58113       4.80003      0.68807        0.07074      0.0679901  ...   \n",
       "0      113.892       137.337      11.7393        45.6131        45.6117  ...   \n",
       "0      118.665       159.565      20.6152        51.6323        49.8943  ...   \n",
       "0        32.62       59.6832      13.5394        24.1372        25.0592  ...   \n",
       "0      336.997       528.013      95.6156        186.596        184.288  ...   \n",
       "0    0.0939651     0.0768281   0.00857692         3.7683        3.65425  ...   \n",
       "0    0.0523279     0.0526495   0.00483814        4.03613         3.9824  ...   \n",
       "0    0.0168662     0.0261462    0.0202725        1.33747        1.33642  ...   \n",
       "0     0.195899      0.569894     0.187104        48.1581        48.1758  ...   \n",
       "0     0.266678      0.412895    0.0751494      0.0419219      0.0525622  ...   \n",
       "0     0.175028      0.261667    0.0459828        0.03966       0.039535  ...   \n",
       "0    0.0728922      0.106356    0.0183905      0.0420618        0.03441  ...   \n",
       "0      1.36733       1.76191     0.208027      0.0974619       0.103405  ...   \n",
       "0      27.6382       50.9817       12.189      0.0316858      0.0516238  ...   \n",
       "0      5.58752        6.5614     0.719849      0.0432222       0.025759  ...   \n",
       "0      184.308       236.513      33.1146      0.0761411      0.0745621  ...   \n",
       "0       48.617       76.6103      15.2219      0.0370519      0.0738072  ...   \n",
       "0      32.8804       64.4458      17.2304       0.516725        0.95696  ...   \n",
       "0      24.9171       42.1696      9.87732       0.425461       0.305956  ...   \n",
       "0      11.4074       11.4223      3.14409       0.109201      0.0818152  ...   \n",
       "0      213.657       364.373      96.7421        3.22035        2.00201  ...   \n",
       "0      8.30906        27.021      9.35985       0.502497       0.662884  ...   \n",
       "0      6.19139        20.658      7.23465       0.252068       0.331617  ...   \n",
       "0      4.51221       13.9655      4.72788       0.128438       0.119955  ...   \n",
       "0      17.0665       64.5907      23.7635        1.07035       0.870651  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.791759       0.714573         0.751189        711        987   \n",
       "0     0.821277        0.58191         0.681176        579       1048   \n",
       "0     0.730563       0.547739         0.626077        545        973   \n",
       "0     0.834646       0.106533         0.188948        106       1153   \n",
       "0     0.738872       0.750754         0.744766        747        910   \n",
       "0     0.771372       0.779899         0.775612        776        944   \n",
       "0     0.701226       0.632161         0.664905        629        906   \n",
       "0     0.753205       0.708543         0.730192        705        943   \n",
       "0     0.750984       0.766834         0.758826        763        921   \n",
       "0     0.760194       0.786935         0.773333        783        927   \n",
       "0     0.699887       0.621106         0.658147        618        909   \n",
       "0     0.757515       0.759799         0.758655        756        932   \n",
       "0     0.586263       0.720603         0.646528        717        668   \n",
       "0     0.751938      0.0974874         0.172598         97       1142   \n",
       "0     0.584323       0.494472         0.535656        492        824   \n",
       "0     0.739785       0.345729         0.471233        344       1053   \n",
       "0     0.654107       0.872362         0.747631        868        715   \n",
       "0     0.742604       0.756784         0.749627        753        913   \n",
       "0     0.633803       0.678392          0.65534        675        784   \n",
       "0     0.828369       0.586935         0.687059        584       1053   \n",
       "0     0.698055       0.685427         0.691684        682        879   \n",
       "0     0.592755       0.542714         0.566632        540        803   \n",
       "0     0.719447       0.680402          0.69938        677        910   \n",
       "0     0.714286       0.703518         0.708861        700        894   \n",
       "0     0.745763       0.751759         0.748749        748        919   \n",
       "0     0.727728       0.730653         0.729188        727        902   \n",
       "0     0.640324       0.635176          0.63774        632        819   \n",
       "0     0.744376       0.731658         0.737962        728        924   \n",
       "0      0.78125       0.527638         0.629874        525       1027   \n",
       "0     0.751989       0.569849          0.64837        567        987   \n",
       "0     0.615965       0.573869         0.594173        571        818   \n",
       "0     0.729829            0.6         0.658577        597        953   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        187        284             0.559446                  0.561763   \n",
       "0        126        416             0.485369                  0.504886   \n",
       "0        201        450             0.383852                  0.394977   \n",
       "0         21        889            0.0949604                  0.188135   \n",
       "0        264        248             0.525238                  0.525295   \n",
       "0        230        219             0.583496                  0.583527   \n",
       "0        268        366             0.406937                  0.408658   \n",
       "0        231        290             0.514104                  0.514884   \n",
       "0        253        232             0.550446                   0.55055   \n",
       "0        247        212             0.575001                  0.575303   \n",
       "0        265        377             0.398804                  0.401015   \n",
       "0        242        239             0.553539                  0.553541   \n",
       "0        506        278             0.284626                  0.290993   \n",
       "0         32        898            0.0752212                  0.147965   \n",
       "0        350        503             0.198675                   0.20076   \n",
       "0        121        651             0.252933                  0.294636   \n",
       "0        459        127             0.469449                  0.492212   \n",
       "0        261        242              0.53369                  0.533773   \n",
       "0        390        320             0.344348                  0.345069   \n",
       "0        121        411             0.494864                  0.514763   \n",
       "0        295        313             0.434749                   0.43481   \n",
       "0        371        455             0.228168                  0.228875   \n",
       "0        264        318             0.457422                  0.458003   \n",
       "0        280        295             0.465552                  0.465597   \n",
       "0        255        247             0.534225                   0.53424   \n",
       "0        272        268             0.498814                  0.498817   \n",
       "0        355        363             0.332995                  0.333004   \n",
       "0        250        267             0.519388                  0.519452   \n",
       "0        147        470             0.412637                  0.433646   \n",
       "0        187        428             0.418288                  0.429599   \n",
       "0        356        424              0.27205                  0.272599   \n",
       "0        221        398             0.417415                  0.423331   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0        0.777644  \n",
       "0        0.737292  \n",
       "0        0.688265  \n",
       "0        0.544323  \n",
       "0        0.762941  \n",
       "0        0.791994  \n",
       "0        0.701941  \n",
       "0         0.75589  \n",
       "0        0.775666  \n",
       "0        0.788271  \n",
       "0        0.697691  \n",
       "0        0.776833  \n",
       "0        0.644799  \n",
       "0        0.535115  \n",
       "0        0.598173  \n",
       "0        0.621331  \n",
       "0        0.740695  \n",
       "0        0.767234  \n",
       "0        0.673097  \n",
       "0        0.741934  \n",
       "0        0.717075  \n",
       "0         0.61335  \n",
       "0        0.727765  \n",
       "0        0.732508  \n",
       "0        0.767276  \n",
       "0        0.749483  \n",
       "0        0.666395  \n",
       "0        0.759356  \n",
       "0        0.701213  \n",
       "0        0.705282  \n",
       "0        0.635316  \n",
       "0        0.705877  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dl'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "pretrained = fasttext.FastText.load_model('/Users/diego/Documents/NLP/crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88180/88180 [00:04<00:00, 18509.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.54 s, sys: 1.81 s, total: 11.4 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a tokenizer \n",
    "token = Tokenizer(oov_token='<OOV>')\n",
    "token.fit_on_texts(df[TEXT])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=300)\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "words = []\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    words.append(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "if save_model:\n",
    "    filename = NAME_TOKEN_EMBEDDINGS\n",
    "    pickle.dump(token, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_w = {}\n",
    "for i in zip(range(len(class_weights)), class_weights):\n",
    "    class_w[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_NN(model, X, y, X_test, y_test,name=\"NN\", fit_params=None, scoring=None, n_splits=5, save=save_model, batch_size = 32,  use_multiprocessing=True):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param model: (model) neural network model\n",
    "    @param X: (list or matrix or tensor) training X data\n",
    "    @param y: (list) label data \n",
    "    @param X_test: (list or matrix or tensor) testing X data\n",
    "    @param y_test: (list) label test data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param scoring: (dict) dictionary of metrics and names\n",
    "    @param n_splits: (int) number of fold for cross-validation (default 5)\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    # ---- Parameters initialisation\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='auto', patience=3)\n",
    "    seed = 42\n",
    "    k = 1\n",
    "    np.random.seed(seed)\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Creation of list for each metric\n",
    "    if scoring==None:        # create a dictionary if none is passed\n",
    "        dic_scoring = {}\n",
    "    if scoring!=None:        # save the dict \n",
    "        dic_score = scoring.copy()\n",
    "    \n",
    "    dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "    dic_score[\"score_time\"] = None\n",
    "    scorer = {}\n",
    "    for i in dic_score.keys(): \n",
    "        scorer[i] = []\n",
    "    \n",
    "    index = [\"Model\"]\n",
    "    results = [name]\n",
    "    # ---- Loop on k-fold for cross-valisation\n",
    "    for train, test in kfold.split(X, y):   # training NN on each fold \n",
    "        # create model\n",
    "        print(f\"k-fold : {k}\")\n",
    "        fit_start = time.time()\n",
    "        _model = tf.keras.models.clone_model(model)\n",
    "        if len(np.unique(y))==2: # binary\n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        else:  # multiclass \n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "        fit_end = time.time() - fit_start\n",
    "\n",
    "        score_start = time.time()\n",
    "        y_pred = (_model.predict(X[test])>0.5).astype(int)\n",
    "        score_end = time.time() - score_start\n",
    "        #if len(set(y))>2:\n",
    "        #    y_pred =np.argmax(y_pred,axis=1)\n",
    "        #print(y_test[0], y_pred[0])\n",
    "        if len(set(y))==2:\n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], y_pred), 3)}% , Recall: {round(100*recall_score(y.iloc[test], y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        else: \n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        \n",
    "        \n",
    "        # ---- save each metric\n",
    "        for i in dic_score.keys():    # compute metrics \n",
    "            if i == \"fit_time\":\n",
    "                scorer[i].append(fit_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(fit_end)\n",
    "                continue\n",
    "            if i == \"score_time\":\n",
    "                scorer[i].append(score_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(score_end)\n",
    "                continue\n",
    "            \n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    scorer[i].append(dic_score[i](y.iloc[test], np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "                elif i==\"roc_auc\":\n",
    "                    scorer[i].append(dic_score[i](to_categorical(y.iloc[test]), y_pred, average = 'macro', multi_class=\"ovo\")) # make each function scorer\n",
    "                else:\n",
    "                    scorer[i].append(dic_score[i]( y.iloc[test], np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i]( y.iloc[test], y_pred)) # make each function scorer\n",
    "            #scorer[i].append(dic_score[i]( y.iloc[test], y_pred))\n",
    "            index.append(\"test_\"+i+'_cv'+str(k))\n",
    "            results.append(scorer[i][-1])\n",
    "        K.clear_session()\n",
    "        del _model\n",
    "        k+=1\n",
    "    \n",
    "    # Train test on the overall data\n",
    "    print(\"Overall train-test data\")\n",
    "    fit_start = time.time()\n",
    "    _model =  tf.keras.models.clone_model(model)\n",
    "    if len(np.unique(y))==2: # binary\n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    else:  # multiclass \n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False)\n",
    "    if save:\n",
    "        check_p = tf.keras.callbacks.ModelCheckpoint(os.path.join(root_dir, dir_name, name+\".h5\"), save_best_only=True)\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es, check_p], validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    else:\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es],  validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    #_acc = _model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    score_start = time.time()\n",
    "    y_pred = (_model.predict(X_test)>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    #if len(set(y))>2:\n",
    "    #    y_pred =np.argmax(y_pred,axis=1)\n",
    "    if len(set(y))==2:\n",
    "        print(f\"Precision: {round(100*precision_score(y_test, y_pred), 3)}% , Recall: {round(100*recall_score(y_test, y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "    else: \n",
    "        print(f\"Precision: {round(100*precision_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "\n",
    "    # Compute mean and std for each metric\n",
    "    for i in scorer: \n",
    "        \n",
    "        results.append(np.mean(scorer[i]))\n",
    "        results.append(np.std(scorer[i]))\n",
    "        if i == \"fit_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        \n",
    "        index.append(\"test_\"+i+\"_mean\")\n",
    "        index.append(\"test_\"+i+\"_std\")\n",
    "        \n",
    "    # add metrics averall dataset on the dictionary \n",
    "    for i in dic_score.keys():    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            scorer[i].append(fit_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            scorer[i].append(score_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(score_end)\n",
    "            continue\n",
    "        \n",
    "        if len(set(y))>2:\n",
    "            if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "            elif i==\"roc_auc\":\n",
    "                scorer[i].append(dic_score[i](to_categorical(y_test), y_pred, average = 'weighted', multi_class=\"ovo\")) # make each function scorer\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "        else:\n",
    "            scorer[i].append(dic_score[i](y.iloc[test], y_pred))                             \n",
    "            #scorer[i].append(dic_score[i](_model, X_test, y_test))\n",
    "        index.append(i+'_overall')\n",
    "        results.append(scorer[i][-1])\n",
    "    \n",
    "            \n",
    "    return pd.DataFrame(results, index=index).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='snn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Shallow Neural Network</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a shallow neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) shallow neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 16)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      \n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "        \n",
    "      #keras.layers.Dense(6, activation=\"relu\"),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = score_metrics\n",
    "\n",
    "# Creation of list for each metric\n",
    "if scoring==None:        # create a dictionary if none is passed\n",
    "    dic_scoring = {}\n",
    "if scoring!=None:        # save the dict \n",
    "    dic_score = scoring.copy()\n",
    "\n",
    "dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "dic_score[\"score_time\"] = None\n",
    "scorer = {}\n",
    "for i in dic_score.keys(): \n",
    "    scorer[i] = []\n",
    "\n",
    "index = [\"Model\"]\n",
    "results = ['Shallow_NN_WE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dic_score.keys():\n",
    "    scorer[i].append(dic_score[i](_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score['acc'](y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430    fanboy ism h k stuff scar h acr exactly aesthe...\n",
       "5089    curse aye awww good old bargaining alarm clock...\n",
       "111     usually psychic quality knowing come specific ...\n",
       "7536    thank advice appreciate just fantastic advice ...\n",
       "1113    trying understand difference sx sx just differ...\n",
       "                              ...                        \n",
       "2926    true comfortable contradiction hi following re...\n",
       "3821    pretty christian ish t believe jesus christ go...\n",
       "5805    bat sherlock holmes quite alright agnostic spi...\n",
       "3140    try lang studying japanese sent gt using tapat...\n",
       "2770    ugh honestly inclined think behavior abusive t...\n",
       "Name: text_clean_joined, Length: 2169, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score[i](_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 71.454% , Recall: 67.391%, Time \t 237.6945 ms\n",
      "k-fold : 2\n",
      "Precision: 72.438% , Recall: 68.677%, Time \t 365.2575 ms\n",
      "k-fold : 3\n",
      "Precision: 70.662% , Recall: 66.164%, Time \t 396.282 ms\n",
      "k-fold : 4\n",
      "Precision: 73.179% , Recall: 69.012%, Time \t 263.4461 ms\n",
      "k-fold : 5\n",
      "Precision: 71.357% , Recall: 71.357%, Time \t 327.027 ms\n",
      "Overall train-test data\n",
      "Precision: 70.657% , Recall: 67.035%, Time \t 338.0158 ms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1301, 2169]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-40eebde9b6a9>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mscorer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;31m#scorer[i].append(dic_score[i](_model, X_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_overall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1301, 2169]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if shallow_network:\n",
    "    df_results = df_results.append(cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Shallow_NN_WE\", scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dnn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Deep Neural Net</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_WE\",scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits , save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var1(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_var1_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Transformers</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](img/transformers_model_architecture.png)\n",
    "\n",
    "The Transformer – Model Architecture - [Source](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=vidya></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
