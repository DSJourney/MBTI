{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project - Modeling (step 5)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Introduction</h3>\n",
    "    <p>This notebook contains the <b>Modeling</b> step which comes after the <b>Feature Engineering & Preprocessing</b> step. The main goal of this step involves selecting, training and deploying a model to make predictive insights.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-danger\">\n",
    "\n",
    "<h3>Disclaimer</h3>\n",
    "    <p>The purpose of this notebook is to go over certain aspects of Natural Language Processing. There might be some parts of the notebook that do not have particular use for the future of this project but they are useful for learning purposes so I left them inside. I also would like to mention that some of the code here is recycled from online articles and notebooks on GitHub, I will try to mention every source as best as possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Summarized goals](#goals)\n",
    "- [Importing Libraries](#importing)\n",
    "- [Review of our Dataset](#review)\n",
    "- [Models Introduction](#model)\n",
    "- [Parameters and Models](#parameters)\n",
    "- [Stopwords](#stopwords)\n",
    "- [Train Test Split](#train_test)\n",
    "- [CountVectorizer and tf-id](#cv)\n",
    "- [Report Function](#report)\n",
    "- [Let's Start Modeling](#modeling) Every model is created with CountVectorizer, TF-IDF words, TF-IDF n_grams, TF-IDF characters\n",
    "    - [MACHINE LEARNING](#ml)\n",
    "        - [Multinomial Naive Bayes Models](#nb)  \n",
    "        - [Logistic Regression](#lr)  \n",
    "        - [Support Vector Machines](#svm)  \n",
    "        - [K-Nearest Neightbors](#knn)  \n",
    "        - [Random Forest](#NB)      \n",
    "        - [Stocastic Gradient Descent](#sgd)\n",
    "        - [Boosting](#boost)\n",
    "            - [Gradient Boosting Classifier](#gbc)\n",
    "            - [XGBoost](#xgb)\n",
    "            - [Catboost](#cb) - Pending\n",
    "            - [Adaboost](#ab) - Pending        \n",
    "            - [LightGBM](#lgbm) - Pending       \n",
    "    - [DEEP LEARNING](#dl) - Pending all section\n",
    "        - [Shallow Neural Network](#snn)\n",
    "        - [Deep Neural Network](#dnn)\n",
    "        - [Transformers](#trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=goals></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarized Goals\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best model that classifies each post into the pair of attributes of the MBTI:\n",
    " - Introversion vs. Extraversion (I vs. E)\n",
    " - Intuition vs. Sensing (N vs. S)\n",
    " - Thinking vs. Feeling (T vs. F)\n",
    " - Judging vs. Perceiving (J vs. P) --> In this notebook we will focus on this category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=importing></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# data wrangiling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.transforms\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set() #\n",
    "\n",
    "# natural language processing libraries\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "import textstat\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas(desc=\"Progress!\")\n",
    "import time\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/mbti_nlp.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Models Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the columns we need\n",
    "J = df[['J','text_clean_joined']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** I will be using `Christophe Pere's` notebook as the basis for this model. All credits go to him, [here is the original notebook](https://github.com/Christophe-pere/Model-Selection/blob/master/Text_Classification_Compare_Models.ipynb) and here is his [TowardsDataScience article](https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract the true, false positive and true false negative\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=parameters></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Models\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT           = \"text_clean_joined\"\n",
    "LABEL          = \"J\"\n",
    "NAME_SAVE_FILE = \"model_selection_results_JP\" # put just the name the .csv will be added at the end\n",
    "\n",
    "# global parameters\n",
    "num_gpu                = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "CV_splits              = 5        # Number of splits for cross-validation and k-folds\n",
    "save_results           = True     # if you want an output file containing all the results\n",
    "lang                   = False    # test if you want to use Google API detection (you will need to \"import from googletrans import Translator\")\n",
    "sample                 = True     # use just a sample of data\n",
    "nb_sample              = 6000     # default value of rows if sample selected\n",
    "save_model             = True     # concat all the data representation\n",
    "root_dir               = \"models/\"       # Place here the path where you want your models stored or use /path/to/your/folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names how the files will be saved as \n",
    "NAME_ENCODER                  = \"encoder.sav\"\n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_TOKEN_EMBEDDINGS         = \"token_embeddings.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "multinomial_naive_bayes= True\n",
    "logistic_regression    = True\n",
    "svm_model              = True\n",
    "k_nn_model             = True\n",
    "sgd                    = True\n",
    "random_forest          = True\n",
    "gradient_boosting      = True\n",
    "xgboost_classifier     = True\n",
    "adaboost_classifier    = True \n",
    "catboost_classifier    = True \n",
    "lightgbm_classifier    = True \n",
    "extratrees_classifier  = True\n",
    "shallow_network        = True\n",
    "deep_nn                = True\n",
    "rnn                    = True\n",
    "lstm                   = True\n",
    "cnn                    = True\n",
    "gru                    = True\n",
    "cnn_lstm               = True\n",
    "cnn_gru                = True\n",
    "bidirectional_rnn      = True\n",
    "bidirectional_lstm     = True\n",
    "bidirectional_gru      = True\n",
    "rcnn                   = True\n",
    "transformers           = False\n",
    "pre_trained            = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is created\n"
     ]
    }
   ],
   "source": [
    "if save_model:\n",
    "    # will create the folder to save all the models\n",
    "    try:\n",
    "        dir_name =  NAME_SAVE_FILE\n",
    "        os.makedirs(os.path.join(root_dir,dir_name))\n",
    "        print(\"The folder is created\")\n",
    "    except:\n",
    "        print(\"The folder can not be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can put all the metrics you want (included in sklearn.metrics).\n",
    "score_metrics = {'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Christophe Pere` has a set of functions to clean the text but we have already done that so I will not add them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do add a remove stop words function\n",
    "def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBTI types are rarely discussed in day to day converstaions, we will take them out since they would have low prediction power\n",
    "types = [x.lower() for x in df['type'].unique()] \n",
    "types_plural = [x+'s' for x in types]\n",
    "\n",
    "# some words that appear a lot but do not add value\n",
    "additional_stop_words = ['ll','type','fe','ni','na','wa','ve','don','nt','nf', 'ti','se','op','ne'] \n",
    "\n",
    "# We put these together and include the normal stopwords from the English language\n",
    "stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS.union(additional_stop_words + types + types_plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress!: 100%|██████████| 8675/8675 [00:02<00:00, 3040.69it/s]\n"
     ]
    }
   ],
   "source": [
    "J[TEXT] = J.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the code simple (for the time being), I will start by focusing on the `Thinking / Feeling` and later implement the same process for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = J.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[TEXT], df[LABEL], test_size=0.25, random_state=42, stratify=df[LABEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Personal note on stratify:** if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(y_train),\n",
    "                                                 y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 0.8275\tclass: 0\n",
      "Class weight: 1.2633\tclass: 1\n"
     ]
    }
   ],
   "source": [
    "print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset is balanced (ratio=0.655)\n"
     ]
    }
   ],
   "source": [
    "# Determined if the dataset is balanced or imbalanced \n",
    "ratio = np.min(df[LABEL].value_counts()) / np.max(df[LABEL].value_counts())\n",
    "if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "    balanced = True\n",
    "    print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "else:\n",
    "    balanced = False\n",
    "    print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "    #from imblearn.over_sampling import ADASYN\n",
    "    # put class for debalanced data \n",
    "    # in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer & TF-IDF\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section transforms our data into something interpretable by the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.55 s, sys: 156 ms, total: 7.7 s\n",
      "Wall time: 7.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df[TEXT])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "x_train_count =  count_vect.transform(X_train)\n",
    "x_test_count =  count_vect.transform(X_test)\n",
    "\n",
    "if save_model:\n",
    "    # save the model to disk\n",
    "    filename = NAME_COUNT_VECT_MODEL\n",
    "    pickle.dump(count_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n",
      "CPU times: user 2min 21s, sys: 2.77 s, total: 2min 24s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "tfidf_vect.fit(df[TEXT])\n",
    "x_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "x_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "print(\"word level tf-idf done\")\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect_ngram.fit(df[TEXT])\n",
    "x_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "x_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "print(\"ngram level tf-idf done\")\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) \n",
    "tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "x_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "x_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) \n",
    "print(\"characters level tf-idf done\")\n",
    "\n",
    "if save_model:\n",
    "    # save the model tf-idf to disk\n",
    "    filename = NAME_TF_IDF_MODEL\n",
    "    pickle.dump(tfidf_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "\n",
    "    # save the model ngram to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # save the model ngram char to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_CHAR_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram_chars, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Function\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followign function will generate, for each model we create, a set of metrics that evaluate how well that model did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, x, y, X_test, y_test, name='classifier', cv=5, dict_scoring=None, fit_params=None, save=save_model):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param clf: (model) classifier\n",
    "    @param x: (list or matrix or tensor) training x data\n",
    "    @param y: (list) label data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param cv: (int) number of fold for cross-validation (default 5)\n",
    "    @param dict_scoring: (dict) dictionary of metrics and names\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param save: (bool) determine if the model need to be saved\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    \n",
    "    '''{'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}'''\n",
    "    \n",
    "    \n",
    "    if dict_scoring!=None:\n",
    "        score = dict_scoring.copy() # save the original dictionary\n",
    "        for i in score.keys():\n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted') # make each function scorer\n",
    "                elif i==\"roc_auc\":\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted', multi_class=\"ovo\",needs_proba=True) # make each function scorer\n",
    "                else:\n",
    "                    score[i] = make_scorer(score[i]) # make each function scorer\n",
    "                    \n",
    "            else:\n",
    "                score[i] = make_scorer(score[i]) # make each function scorer\n",
    "            \n",
    "    try:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False, n_jobs=-1,  fit_params=fit_params)\n",
    "    except:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False,  fit_params=fit_params)\n",
    "        \n",
    "    # Train test on the overall data\n",
    "    fit_start = time.time()\n",
    "    _model = clf\n",
    "    _model.fit(x, y)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    \n",
    "    score_start = time.time()\n",
    "    y_pred = _model.predict(X_test)#>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    \n",
    "    # this saves the model for reuse\n",
    "    if save:\n",
    "        filename= name+\".sav\"\n",
    "        pickle.dump(_model, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # initialisation \n",
    "    index = []\n",
    "    value = []\n",
    "    index.append(\"Model\")\n",
    "    value.append(name)\n",
    "    for i in scores:  # loop on each metric generate text and values\n",
    "        if i == \"estimator\":\n",
    "            continue\n",
    "        for j in enumerate(scores[i]):\n",
    "            index.append(i+\"_cv\"+str(j[0]+1))\n",
    "            value.append(j[1])\n",
    "        \n",
    "        \n",
    "        index.append(i+\"_mean\")\n",
    "        value.append(np.mean(scores[i]))\n",
    "        index.append(i+\"_std\")\n",
    "        value.append(np.std(scores[i]))\n",
    "    \n",
    "     # add metrics averall dataset on the dictionary \n",
    "    \n",
    "    for i in scores:    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,fit_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,score_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(score_end)\n",
    "            continue\n",
    "              \n",
    "        \n",
    "        scores[i] = np.append(scores[i] ,score[i.split(\"test_\")[-1]](_model, X_test, y_test))\n",
    "        index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "        value.append(scores[i][-1])\n",
    "    \n",
    "    return pd.DataFrame(data=value, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Modeling!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating the empty dataframe we will use to put the results of each model we create\n",
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Multinomial Naïve Bayes</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 811 ms, sys: 276 ms, total: 1.09 s\n",
      "Wall time: 5.61 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if multinomial_naive_bayes:\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_count, y_train, x_test_count, y_test, name='NB_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf, y_train, x_test_tfidf, y_test, name='NB_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='NB_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='NB_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Logistic Regression</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 s, sys: 4.39 s, total: 50.6 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if logistic_regression:\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_count, y_train, x_test_count, y_test, name='LR_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='LR_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='LR_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='LR_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Support Vector Machine</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 20s, sys: 6.74 s, total: 1h 13min 27s\n",
      "Wall time: 1h 43min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if svm_model:\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_count, y_train, x_test_count, y_test, name='SVM_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SVM_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SVM_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SVM_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>K-Nearest Neighbors</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 28s, sys: 18.6 s, total: 16min 46s\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if k_nn_model:\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_count, y_train, x_test_count, y_test, name='kNN_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf, y_train, x_test_tfidf, y_test, name='kNN_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='kNN_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='kNN_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Random Forest</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 1.3 s, total: 2min 53s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_count, y_train, x_test_count, y_test, name='RF_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf, y_train, x_test_tfidf, y_test, name='RF_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='RF_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='RF_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Stocastis Gradient Descent</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 215 ms, total: 1.94 s\n",
      "Wall time: 5.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if sgd:\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_count, y_train, x_test_count, y_test, name='SGD_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SGD_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SGD_N-Gram_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SGD_CharLevel_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boost'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gbc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Gradient Boosting Classifier</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.44 s, sys: 133 ms, total: 7.57 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_count, y_train, x_test_count, y_test,\n",
    "                                          name='GB_Count_Vectors', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 s, sys: 81.6 ms, total: 9.36 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf, y_train, x_test_tfidf, y_test,\n",
    "                                          name='GB_WordLevel_TF-IDF', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 35.3 ms, total: 1.9 s\n",
      "Wall time: 6.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test,\n",
    "                                          name='GB_N-Gram_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 s, sys: 403 ms, total: 50 s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,\n",
    "                                          name='GB_CharLevel_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>XGBoost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 52s, sys: 2.21 s, total: 13min 54s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        # run on CPU\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 58s, sys: 1.43 s, total: 12min 59s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist', n_estimators=1000, subsample=0.8), x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8),x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 48s, sys: 762 ms, total: 5min 49s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 45s, sys: 5.08 s, total: 50min 50s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.082927</td>\n",
       "      <td>0.0763922</td>\n",
       "      <td>0.0773358</td>\n",
       "      <td>0.0712838</td>\n",
       "      <td>0.0557861</td>\n",
       "      <td>0.072745</td>\n",
       "      <td>0.00924974</td>\n",
       "      <td>0.044898</td>\n",
       "      <td>0.0429771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612013</td>\n",
       "      <td>0.438882</td>\n",
       "      <td>0.511186</td>\n",
       "      <td>377</td>\n",
       "      <td>1071</td>\n",
       "      <td>239</td>\n",
       "      <td>482</td>\n",
       "      <td>0.269566</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.62822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0373771</td>\n",
       "      <td>0.0372999</td>\n",
       "      <td>0.0487292</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>0.0494978</td>\n",
       "      <td>0.0425766</td>\n",
       "      <td>0.00542925</td>\n",
       "      <td>0.035882</td>\n",
       "      <td>0.0368619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.0442375</td>\n",
       "      <td>0.0833333</td>\n",
       "      <td>38</td>\n",
       "      <td>1295</td>\n",
       "      <td>15</td>\n",
       "      <td>821</td>\n",
       "      <td>0.0391029</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.516394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.0160372</td>\n",
       "      <td>0.0153599</td>\n",
       "      <td>0.0191908</td>\n",
       "      <td>0.0156741</td>\n",
       "      <td>0.010529</td>\n",
       "      <td>0.0153582</td>\n",
       "      <td>0.00277742</td>\n",
       "      <td>0.0303328</td>\n",
       "      <td>0.0319908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52439</td>\n",
       "      <td>0.0500582</td>\n",
       "      <td>0.0913921</td>\n",
       "      <td>43</td>\n",
       "      <td>1271</td>\n",
       "      <td>39</td>\n",
       "      <td>816</td>\n",
       "      <td>0.0240286</td>\n",
       "      <td>0.0520219</td>\n",
       "      <td>0.510144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.359745</td>\n",
       "      <td>0.315998</td>\n",
       "      <td>0.312276</td>\n",
       "      <td>0.302991</td>\n",
       "      <td>0.208773</td>\n",
       "      <td>0.299957</td>\n",
       "      <td>0.0496133</td>\n",
       "      <td>0.135056</td>\n",
       "      <td>0.0519991</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1310</td>\n",
       "      <td>0</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>26.1478</td>\n",
       "      <td>25.4018</td>\n",
       "      <td>26.7334</td>\n",
       "      <td>27.016</td>\n",
       "      <td>9.79909</td>\n",
       "      <td>23.0196</td>\n",
       "      <td>6.63334</td>\n",
       "      <td>0.0384159</td>\n",
       "      <td>0.045301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564904</td>\n",
       "      <td>0.547148</td>\n",
       "      <td>0.555884</td>\n",
       "      <td>470</td>\n",
       "      <td>948</td>\n",
       "      <td>362</td>\n",
       "      <td>389</td>\n",
       "      <td>0.272285</td>\n",
       "      <td>0.272378</td>\n",
       "      <td>0.635406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.973822</td>\n",
       "      <td>0.75368</td>\n",
       "      <td>0.703657</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.480079</td>\n",
       "      <td>0.745387</td>\n",
       "      <td>0.160826</td>\n",
       "      <td>0.0253992</td>\n",
       "      <td>0.0374689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612632</td>\n",
       "      <td>0.338766</td>\n",
       "      <td>0.436282</td>\n",
       "      <td>291</td>\n",
       "      <td>1126</td>\n",
       "      <td>184</td>\n",
       "      <td>568</td>\n",
       "      <td>0.21484</td>\n",
       "      <td>0.234514</td>\n",
       "      <td>0.599154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.424682</td>\n",
       "      <td>0.443675</td>\n",
       "      <td>0.442271</td>\n",
       "      <td>0.435474</td>\n",
       "      <td>0.225618</td>\n",
       "      <td>0.394344</td>\n",
       "      <td>0.0846299</td>\n",
       "      <td>0.0359197</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582759</td>\n",
       "      <td>0.19674</td>\n",
       "      <td>0.294169</td>\n",
       "      <td>169</td>\n",
       "      <td>1189</td>\n",
       "      <td>121</td>\n",
       "      <td>690</td>\n",
       "      <td>0.117807</td>\n",
       "      <td>0.14999</td>\n",
       "      <td>0.552187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>3.56081</td>\n",
       "      <td>3.63394</td>\n",
       "      <td>3.33128</td>\n",
       "      <td>3.94866</td>\n",
       "      <td>2.21036</td>\n",
       "      <td>3.33701</td>\n",
       "      <td>0.59695</td>\n",
       "      <td>0.0403571</td>\n",
       "      <td>0.041537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.185099</td>\n",
       "      <td>0.284691</td>\n",
       "      <td>159</td>\n",
       "      <td>1211</td>\n",
       "      <td>99</td>\n",
       "      <td>700</td>\n",
       "      <td>0.124524</td>\n",
       "      <td>0.165467</td>\n",
       "      <td>0.554763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>154.263</td>\n",
       "      <td>154.233</td>\n",
       "      <td>154.347</td>\n",
       "      <td>154.779</td>\n",
       "      <td>89.7542</td>\n",
       "      <td>141.475</td>\n",
       "      <td>25.8613</td>\n",
       "      <td>40.0103</td>\n",
       "      <td>39.8527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.267753</td>\n",
       "      <td>0.374898</td>\n",
       "      <td>230</td>\n",
       "      <td>1172</td>\n",
       "      <td>138</td>\n",
       "      <td>629</td>\n",
       "      <td>0.180134</td>\n",
       "      <td>0.211623</td>\n",
       "      <td>0.581205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>139.987</td>\n",
       "      <td>139.777</td>\n",
       "      <td>139.179</td>\n",
       "      <td>139.687</td>\n",
       "      <td>84.6814</td>\n",
       "      <td>128.662</td>\n",
       "      <td>21.992</td>\n",
       "      <td>37.3232</td>\n",
       "      <td>37.2859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639024</td>\n",
       "      <td>0.305006</td>\n",
       "      <td>0.412924</td>\n",
       "      <td>262</td>\n",
       "      <td>1162</td>\n",
       "      <td>148</td>\n",
       "      <td>597</td>\n",
       "      <td>0.211015</td>\n",
       "      <td>0.239869</td>\n",
       "      <td>0.596014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>54.1892</td>\n",
       "      <td>54.1899</td>\n",
       "      <td>54.1159</td>\n",
       "      <td>54.2445</td>\n",
       "      <td>31.1127</td>\n",
       "      <td>49.5705</td>\n",
       "      <td>9.22896</td>\n",
       "      <td>13.9332</td>\n",
       "      <td>14.0377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.0768335</td>\n",
       "      <td>0.137214</td>\n",
       "      <td>66</td>\n",
       "      <td>1273</td>\n",
       "      <td>37</td>\n",
       "      <td>793</td>\n",
       "      <td>0.0572647</td>\n",
       "      <td>0.111735</td>\n",
       "      <td>0.524295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>534.433</td>\n",
       "      <td>535.874</td>\n",
       "      <td>536.68</td>\n",
       "      <td>538.468</td>\n",
       "      <td>330.181</td>\n",
       "      <td>495.127</td>\n",
       "      <td>82.4832</td>\n",
       "      <td>139.72</td>\n",
       "      <td>139.997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.105937</td>\n",
       "      <td>0.184959</td>\n",
       "      <td>91</td>\n",
       "      <td>1276</td>\n",
       "      <td>34</td>\n",
       "      <td>768</td>\n",
       "      <td>0.0937763</td>\n",
       "      <td>0.167855</td>\n",
       "      <td>0.539991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>0.0572631</td>\n",
       "      <td>0.0568771</td>\n",
       "      <td>0.0570328</td>\n",
       "      <td>0.05635</td>\n",
       "      <td>0.0569968</td>\n",
       "      <td>0.000379583</td>\n",
       "      <td>3.44784</td>\n",
       "      <td>3.55118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.102445</td>\n",
       "      <td>0.171373</td>\n",
       "      <td>88</td>\n",
       "      <td>1230</td>\n",
       "      <td>80</td>\n",
       "      <td>771</td>\n",
       "      <td>0.0480263</td>\n",
       "      <td>0.0757012</td>\n",
       "      <td>0.520688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.0327489</td>\n",
       "      <td>0.041364</td>\n",
       "      <td>0.0321758</td>\n",
       "      <td>0.03654</td>\n",
       "      <td>0.0297956</td>\n",
       "      <td>0.0345249</td>\n",
       "      <td>0.00404652</td>\n",
       "      <td>3.39522</td>\n",
       "      <td>3.39499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481865</td>\n",
       "      <td>0.108265</td>\n",
       "      <td>0.176806</td>\n",
       "      <td>93</td>\n",
       "      <td>1210</td>\n",
       "      <td>100</td>\n",
       "      <td>766</td>\n",
       "      <td>0.0368472</td>\n",
       "      <td>0.054847</td>\n",
       "      <td>0.515965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.0153203</td>\n",
       "      <td>0.01545</td>\n",
       "      <td>0.0138099</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0.0120001</td>\n",
       "      <td>0.0144719</td>\n",
       "      <td>0.00140948</td>\n",
       "      <td>1.06823</td>\n",
       "      <td>1.06984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0651921</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>56</td>\n",
       "      <td>1270</td>\n",
       "      <td>40</td>\n",
       "      <td>803</td>\n",
       "      <td>0.0409134</td>\n",
       "      <td>0.0824133</td>\n",
       "      <td>0.517329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.318158</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>0.281373</td>\n",
       "      <td>0.218622</td>\n",
       "      <td>0.287975</td>\n",
       "      <td>0.0372854</td>\n",
       "      <td>32.6237</td>\n",
       "      <td>32.6236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501326</td>\n",
       "      <td>0.440047</td>\n",
       "      <td>0.468692</td>\n",
       "      <td>378</td>\n",
       "      <td>934</td>\n",
       "      <td>376</td>\n",
       "      <td>481</td>\n",
       "      <td>0.156313</td>\n",
       "      <td>0.157155</td>\n",
       "      <td>0.576512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>24.5705</td>\n",
       "      <td>24.3166</td>\n",
       "      <td>23.9987</td>\n",
       "      <td>24.1394</td>\n",
       "      <td>5.9137</td>\n",
       "      <td>20.5878</td>\n",
       "      <td>7.33952</td>\n",
       "      <td>0.256184</td>\n",
       "      <td>0.326657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.0523865</td>\n",
       "      <td>0.0971922</td>\n",
       "      <td>45</td>\n",
       "      <td>1288</td>\n",
       "      <td>22</td>\n",
       "      <td>814</td>\n",
       "      <td>0.0423072</td>\n",
       "      <td>0.100609</td>\n",
       "      <td>0.517796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>19.2525</td>\n",
       "      <td>19.3837</td>\n",
       "      <td>19.0851</td>\n",
       "      <td>19.5673</td>\n",
       "      <td>5.21927</td>\n",
       "      <td>16.5016</td>\n",
       "      <td>5.64337</td>\n",
       "      <td>0.182713</td>\n",
       "      <td>0.155648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525316</td>\n",
       "      <td>0.096624</td>\n",
       "      <td>0.163225</td>\n",
       "      <td>83</td>\n",
       "      <td>1235</td>\n",
       "      <td>75</td>\n",
       "      <td>776</td>\n",
       "      <td>0.0458068</td>\n",
       "      <td>0.0740945</td>\n",
       "      <td>0.519686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>14.5876</td>\n",
       "      <td>14.8477</td>\n",
       "      <td>14.8926</td>\n",
       "      <td>14.6564</td>\n",
       "      <td>3.97421</td>\n",
       "      <td>12.5917</td>\n",
       "      <td>4.31026</td>\n",
       "      <td>0.121564</td>\n",
       "      <td>0.0931549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474783</td>\n",
       "      <td>0.317811</td>\n",
       "      <td>0.380753</td>\n",
       "      <td>273</td>\n",
       "      <td>1008</td>\n",
       "      <td>302</td>\n",
       "      <td>586</td>\n",
       "      <td>0.0925439</td>\n",
       "      <td>0.0967062</td>\n",
       "      <td>0.543639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>46.2854</td>\n",
       "      <td>46.2874</td>\n",
       "      <td>45.987</td>\n",
       "      <td>46.5432</td>\n",
       "      <td>11.7699</td>\n",
       "      <td>39.3746</td>\n",
       "      <td>13.8035</td>\n",
       "      <td>0.860379</td>\n",
       "      <td>0.84595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.0826542</td>\n",
       "      <td>0.144603</td>\n",
       "      <td>71</td>\n",
       "      <td>1258</td>\n",
       "      <td>52</td>\n",
       "      <td>788</td>\n",
       "      <td>0.0503918</td>\n",
       "      <td>0.0908421</td>\n",
       "      <td>0.52148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.392649</td>\n",
       "      <td>0.367797</td>\n",
       "      <td>0.40612</td>\n",
       "      <td>0.417197</td>\n",
       "      <td>0.244035</td>\n",
       "      <td>0.36556</td>\n",
       "      <td>0.0629582</td>\n",
       "      <td>0.0397701</td>\n",
       "      <td>0.0411761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.0861467</td>\n",
       "      <td>0.15056</td>\n",
       "      <td>74</td>\n",
       "      <td>1260</td>\n",
       "      <td>50</td>\n",
       "      <td>785</td>\n",
       "      <td>0.0562661</td>\n",
       "      <td>0.10107</td>\n",
       "      <td>0.523989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.301941</td>\n",
       "      <td>0.226286</td>\n",
       "      <td>0.227901</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.149921</td>\n",
       "      <td>0.228286</td>\n",
       "      <td>0.0482092</td>\n",
       "      <td>0.0245261</td>\n",
       "      <td>0.0405619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573302</td>\n",
       "      <td>0.559953</td>\n",
       "      <td>0.566549</td>\n",
       "      <td>481</td>\n",
       "      <td>952</td>\n",
       "      <td>358</td>\n",
       "      <td>378</td>\n",
       "      <td>0.287825</td>\n",
       "      <td>0.287878</td>\n",
       "      <td>0.643335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.086664</td>\n",
       "      <td>0.0893309</td>\n",
       "      <td>0.095705</td>\n",
       "      <td>0.0820761</td>\n",
       "      <td>0.0923046</td>\n",
       "      <td>0.00889174</td>\n",
       "      <td>0.0361779</td>\n",
       "      <td>0.0402989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471769</td>\n",
       "      <td>0.437718</td>\n",
       "      <td>0.454106</td>\n",
       "      <td>376</td>\n",
       "      <td>889</td>\n",
       "      <td>421</td>\n",
       "      <td>483</td>\n",
       "      <td>0.117808</td>\n",
       "      <td>0.118024</td>\n",
       "      <td>0.558172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.10085</td>\n",
       "      <td>1.27564</td>\n",
       "      <td>1.40762</td>\n",
       "      <td>1.50375</td>\n",
       "      <td>0.676941</td>\n",
       "      <td>1.19296</td>\n",
       "      <td>0.291295</td>\n",
       "      <td>0.0405869</td>\n",
       "      <td>0.0418429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504418</td>\n",
       "      <td>0.731083</td>\n",
       "      <td>0.596958</td>\n",
       "      <td>628</td>\n",
       "      <td>693</td>\n",
       "      <td>617</td>\n",
       "      <td>231</td>\n",
       "      <td>0.241416</td>\n",
       "      <td>0.257238</td>\n",
       "      <td>0.630045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>9.30634</td>\n",
       "      <td>12.2932</td>\n",
       "      <td>12.8056</td>\n",
       "      <td>11.847</td>\n",
       "      <td>6.46535</td>\n",
       "      <td>10.5435</td>\n",
       "      <td>2.3681</td>\n",
       "      <td>0.0416608</td>\n",
       "      <td>0.0405419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.0473506</td>\n",
       "      <td>21</td>\n",
       "      <td>1303</td>\n",
       "      <td>7</td>\n",
       "      <td>838</td>\n",
       "      <td>0.0229204</td>\n",
       "      <td>0.0827672</td>\n",
       "      <td>0.509552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>8.39341</td>\n",
       "      <td>8.30647</td>\n",
       "      <td>16.3495</td>\n",
       "      <td>15.1863</td>\n",
       "      <td>8.90447</td>\n",
       "      <td>11.428</td>\n",
       "      <td>3.5684</td>\n",
       "      <td>0.0378633</td>\n",
       "      <td>0.038451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0314319</td>\n",
       "      <td>0.0603352</td>\n",
       "      <td>27</td>\n",
       "      <td>1301</td>\n",
       "      <td>9</td>\n",
       "      <td>832</td>\n",
       "      <td>0.0294125</td>\n",
       "      <td>0.094025</td>\n",
       "      <td>0.512281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>2.85385</td>\n",
       "      <td>2.85759</td>\n",
       "      <td>2.84363</td>\n",
       "      <td>2.81153</td>\n",
       "      <td>1.63842</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.481567</td>\n",
       "      <td>0.0538738</td>\n",
       "      <td>0.0527852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.00349243</td>\n",
       "      <td>0.00694444</td>\n",
       "      <td>3</td>\n",
       "      <td>1308</td>\n",
       "      <td>2</td>\n",
       "      <td>856</td>\n",
       "      <td>0.00237158</td>\n",
       "      <td>0.0200465</td>\n",
       "      <td>0.500983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>76.1301</td>\n",
       "      <td>114.411</td>\n",
       "      <td>90.5913</td>\n",
       "      <td>72.7467</td>\n",
       "      <td>45.0224</td>\n",
       "      <td>79.7802</td>\n",
       "      <td>22.7467</td>\n",
       "      <td>0.071748</td>\n",
       "      <td>0.0645199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.0337602</td>\n",
       "      <td>0.0641593</td>\n",
       "      <td>29</td>\n",
       "      <td>1294</td>\n",
       "      <td>16</td>\n",
       "      <td>830</td>\n",
       "      <td>0.0257462</td>\n",
       "      <td>0.0739306</td>\n",
       "      <td>0.510773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>16.2424</td>\n",
       "      <td>20.3116</td>\n",
       "      <td>15.4344</td>\n",
       "      <td>25.4352</td>\n",
       "      <td>10.4086</td>\n",
       "      <td>17.5664</td>\n",
       "      <td>5.03956</td>\n",
       "      <td>0.48258</td>\n",
       "      <td>0.410952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551118</td>\n",
       "      <td>0.40163</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>345</td>\n",
       "      <td>1029</td>\n",
       "      <td>281</td>\n",
       "      <td>514</td>\n",
       "      <td>0.196291</td>\n",
       "      <td>0.201974</td>\n",
       "      <td>0.593563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>20.3894</td>\n",
       "      <td>28.2197</td>\n",
       "      <td>18.9345</td>\n",
       "      <td>22.2932</td>\n",
       "      <td>11.8121</td>\n",
       "      <td>20.3298</td>\n",
       "      <td>5.30296</td>\n",
       "      <td>0.386774</td>\n",
       "      <td>0.196016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539088</td>\n",
       "      <td>0.385332</td>\n",
       "      <td>0.449423</td>\n",
       "      <td>331</td>\n",
       "      <td>1027</td>\n",
       "      <td>283</td>\n",
       "      <td>528</td>\n",
       "      <td>0.178042</td>\n",
       "      <td>0.183799</td>\n",
       "      <td>0.584651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>5.31536</td>\n",
       "      <td>5.37072</td>\n",
       "      <td>6.81196</td>\n",
       "      <td>5.14275</td>\n",
       "      <td>2.57379</td>\n",
       "      <td>5.04292</td>\n",
       "      <td>1.37243</td>\n",
       "      <td>0.154089</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452599</td>\n",
       "      <td>0.344587</td>\n",
       "      <td>0.391276</td>\n",
       "      <td>296</td>\n",
       "      <td>952</td>\n",
       "      <td>358</td>\n",
       "      <td>563</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>0.0759893</td>\n",
       "      <td>0.535652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>123.704</td>\n",
       "      <td>101.754</td>\n",
       "      <td>131.206</td>\n",
       "      <td>138.083</td>\n",
       "      <td>105.467</td>\n",
       "      <td>120.043</td>\n",
       "      <td>14.2156</td>\n",
       "      <td>1.67879</td>\n",
       "      <td>1.74705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.366705</td>\n",
       "      <td>0.436288</td>\n",
       "      <td>315</td>\n",
       "      <td>1040</td>\n",
       "      <td>270</td>\n",
       "      <td>544</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>0.176978</td>\n",
       "      <td>0.580299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.082927    0.0763922    0.0773358    0.0712838   \n",
       "0    NB_WordLevel_TF-IDF    0.0373771    0.0372999    0.0487292     0.039979   \n",
       "0       NB_N-Gram_TF-IDF    0.0160372    0.0153599    0.0191908    0.0156741   \n",
       "0    NB_CharLevel_TF-IDF     0.359745     0.315998     0.312276     0.302991   \n",
       "0       LR_Count_Vectors      26.1478      25.4018      26.7334       27.016   \n",
       "0    LR_WordLevel_TF-IDF     0.973822      0.75368     0.703657     0.815695   \n",
       "0       LR_N-Gram_TF-IDF     0.424682     0.443675     0.442271     0.435474   \n",
       "0    LR_CharLevel_TF-IDF      3.56081      3.63394      3.33128      3.94866   \n",
       "0      SVM_Count_Vectors      154.263      154.233      154.347      154.779   \n",
       "0   SVM_WordLevel_TF-IDF      139.987      139.777      139.179      139.687   \n",
       "0      SVM_N-Gram_TF-IDF      54.1892      54.1899      54.1159      54.2445   \n",
       "0   SVM_CharLevel_TF-IDF      534.433      535.874       536.68      538.468   \n",
       "0      kNN_Count_Vectors     0.057461    0.0572631    0.0568771    0.0570328   \n",
       "0   kNN_WordLevel_TF-IDF    0.0327489     0.041364    0.0321758      0.03654   \n",
       "0      kNN_N-Gram_TF-IDF    0.0153203      0.01545    0.0138099     0.015779   \n",
       "0   kNN_CharLevel_TF-IDF       0.3192     0.318158     0.302521     0.281373   \n",
       "0       RF_Count_Vectors      24.5705      24.3166      23.9987      24.1394   \n",
       "0    RF_WordLevel_TF-IDF      19.2525      19.3837      19.0851      19.5673   \n",
       "0       RF_N-Gram_TF-IDF      14.5876      14.8477      14.8926      14.6564   \n",
       "0    RF_CharLevel_TF-IDF      46.2854      46.2874       45.987      46.5432   \n",
       "0      SGD_Count_Vectors     0.392649     0.367797      0.40612     0.417197   \n",
       "0   SGD_WordLevel_TF-IDF     0.301941     0.226286     0.227901     0.235383   \n",
       "0     SGD_N-Gram_Vectors     0.107747     0.086664    0.0893309     0.095705   \n",
       "0  SGD_CharLevel_Vectors      1.10085      1.27564      1.40762      1.50375   \n",
       "0       GB_Count_Vectors      9.30634      12.2932      12.8056       11.847   \n",
       "0    GB_WordLevel_TF-IDF      8.39341      8.30647      16.3495      15.1863   \n",
       "0       GB_N-Gram_TF-IDF      2.85385      2.85759      2.84363      2.81153   \n",
       "0    GB_CharLevel_TF-IDF      76.1301      114.411      90.5913      72.7467   \n",
       "0      XGB_Count_Vectors      16.2424      20.3116      15.4344      25.4352   \n",
       "0   XGB_WordLevel_TF-IDF      20.3894      28.2197      18.9345      22.2932   \n",
       "0      XGB_N-Gram_TF-IDF      5.31536      5.37072      6.81196      5.14275   \n",
       "0   XGB_CharLevel_TF-IDF      123.704      101.754      131.206      138.083   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0557861      0.072745   0.00924974       0.044898      0.0429771  ...   \n",
       "0    0.0494978     0.0425766   0.00542925       0.035882      0.0368619  ...   \n",
       "0     0.010529     0.0153582   0.00277742      0.0303328      0.0319908  ...   \n",
       "0     0.208773      0.299957    0.0496133       0.135056      0.0519991  ...   \n",
       "0      9.79909       23.0196      6.63334      0.0384159       0.045301  ...   \n",
       "0     0.480079      0.745387     0.160826      0.0253992      0.0374689  ...   \n",
       "0     0.225618      0.394344    0.0846299      0.0359197       0.035552  ...   \n",
       "0      2.21036       3.33701      0.59695      0.0403571       0.041537  ...   \n",
       "0      89.7542       141.475      25.8613        40.0103        39.8527  ...   \n",
       "0      84.6814       128.662       21.992        37.3232        37.2859  ...   \n",
       "0      31.1127       49.5705      9.22896        13.9332        14.0377  ...   \n",
       "0      330.181       495.127      82.4832         139.72        139.997  ...   \n",
       "0      0.05635     0.0569968  0.000379583        3.44784        3.55118  ...   \n",
       "0    0.0297956     0.0345249   0.00404652        3.39522        3.39499  ...   \n",
       "0    0.0120001     0.0144719   0.00140948        1.06823        1.06984  ...   \n",
       "0     0.218622      0.287975    0.0372854        32.6237        32.6236  ...   \n",
       "0       5.9137       20.5878      7.33952       0.256184       0.326657  ...   \n",
       "0      5.21927       16.5016      5.64337       0.182713       0.155648  ...   \n",
       "0      3.97421       12.5917      4.31026       0.121564      0.0931549  ...   \n",
       "0      11.7699       39.3746      13.8035       0.860379        0.84595  ...   \n",
       "0     0.244035       0.36556    0.0629582      0.0397701      0.0411761  ...   \n",
       "0     0.149921      0.228286    0.0482092      0.0245261      0.0405619  ...   \n",
       "0    0.0820761     0.0923046   0.00889174      0.0361779      0.0402989  ...   \n",
       "0     0.676941       1.19296     0.291295      0.0405869      0.0418429  ...   \n",
       "0      6.46535       10.5435       2.3681      0.0416608      0.0405419  ...   \n",
       "0      8.90447        11.428       3.5684      0.0378633       0.038451  ...   \n",
       "0      1.63842         2.601     0.481567      0.0538738      0.0527852  ...   \n",
       "0      45.0224       79.7802      22.7467       0.071748      0.0645199  ...   \n",
       "0      10.4086       17.5664      5.03956        0.48258       0.410952  ...   \n",
       "0      11.8121       20.3298      5.30296       0.386774       0.196016  ...   \n",
       "0      2.57379       5.04292      1.37243       0.154089         0.1632  ...   \n",
       "0      105.467       120.043      14.2156        1.67879        1.74705  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.612013       0.438882         0.511186        377       1071   \n",
       "0     0.716981      0.0442375        0.0833333         38       1295   \n",
       "0      0.52439      0.0500582        0.0913921         43       1271   \n",
       "0            0              0                0          0       1310   \n",
       "0     0.564904       0.547148         0.555884        470        948   \n",
       "0     0.612632       0.338766         0.436282        291       1126   \n",
       "0     0.582759        0.19674         0.294169        169       1189   \n",
       "0     0.616279       0.185099         0.284691        159       1211   \n",
       "0        0.625       0.267753         0.374898        230       1172   \n",
       "0     0.639024       0.305006         0.412924        262       1162   \n",
       "0     0.640777      0.0768335         0.137214         66       1273   \n",
       "0        0.728       0.105937         0.184959         91       1276   \n",
       "0      0.52381       0.102445         0.171373         88       1230   \n",
       "0     0.481865       0.108265         0.176806         93       1210   \n",
       "0     0.583333      0.0651921         0.117277         56       1270   \n",
       "0     0.501326       0.440047         0.468692        378        934   \n",
       "0     0.671642      0.0523865        0.0971922         45       1288   \n",
       "0     0.525316       0.096624         0.163225         83       1235   \n",
       "0     0.474783       0.317811         0.380753        273       1008   \n",
       "0     0.577236      0.0826542         0.144603         71       1258   \n",
       "0     0.596774      0.0861467          0.15056         74       1260   \n",
       "0     0.573302       0.559953         0.566549        481        952   \n",
       "0     0.471769       0.437718         0.454106        376        889   \n",
       "0     0.504418       0.731083         0.596958        628        693   \n",
       "0         0.75       0.024447        0.0473506         21       1303   \n",
       "0         0.75      0.0314319        0.0603352         27       1301   \n",
       "0          0.6     0.00349243       0.00694444          3       1308   \n",
       "0     0.644444      0.0337602        0.0641593         29       1294   \n",
       "0     0.551118        0.40163         0.464646        345       1029   \n",
       "0     0.539088       0.385332         0.449423        331       1027   \n",
       "0     0.452599       0.344587         0.391276        296        952   \n",
       "0     0.538462       0.366705         0.436288        315       1040   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        239        482             0.269566                  0.278126   \n",
       "0         15        821            0.0391029                  0.103858   \n",
       "0         39        816            0.0240286                 0.0520219   \n",
       "0          0        859                    0                         0   \n",
       "0        362        389             0.272285                  0.272378   \n",
       "0        184        568              0.21484                  0.234514   \n",
       "0        121        690             0.117807                   0.14999   \n",
       "0         99        700             0.124524                  0.165467   \n",
       "0        138        629             0.180134                  0.211623   \n",
       "0        148        597             0.211015                  0.239869   \n",
       "0         37        793            0.0572647                  0.111735   \n",
       "0         34        768            0.0937763                  0.167855   \n",
       "0         80        771            0.0480263                 0.0757012   \n",
       "0        100        766            0.0368472                  0.054847   \n",
       "0         40        803            0.0409134                 0.0824133   \n",
       "0        376        481             0.156313                  0.157155   \n",
       "0         22        814            0.0423072                  0.100609   \n",
       "0         75        776            0.0458068                 0.0740945   \n",
       "0        302        586            0.0925439                 0.0967062   \n",
       "0         52        788            0.0503918                 0.0908421   \n",
       "0         50        785            0.0562661                   0.10107   \n",
       "0        358        378             0.287825                  0.287878   \n",
       "0        421        483             0.117808                  0.118024   \n",
       "0        617        231             0.241416                  0.257238   \n",
       "0          7        838            0.0229204                 0.0827672   \n",
       "0          9        832            0.0294125                  0.094025   \n",
       "0          2        856           0.00237158                 0.0200465   \n",
       "0         16        830            0.0257462                 0.0739306   \n",
       "0        281        514             0.196291                  0.201974   \n",
       "0        283        528             0.178042                  0.183799   \n",
       "0        358        563             0.074359                 0.0759893   \n",
       "0        270        544             0.169929                  0.176978   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0         0.62822  \n",
       "0        0.516394  \n",
       "0        0.510144  \n",
       "0             0.5  \n",
       "0        0.635406  \n",
       "0        0.599154  \n",
       "0        0.552187  \n",
       "0        0.554763  \n",
       "0        0.581205  \n",
       "0        0.596014  \n",
       "0        0.524295  \n",
       "0        0.539991  \n",
       "0        0.520688  \n",
       "0        0.515965  \n",
       "0        0.517329  \n",
       "0        0.576512  \n",
       "0        0.517796  \n",
       "0        0.519686  \n",
       "0        0.543639  \n",
       "0         0.52148  \n",
       "0        0.523989  \n",
       "0        0.643335  \n",
       "0        0.558172  \n",
       "0        0.630045  \n",
       "0        0.509552  \n",
       "0        0.512281  \n",
       "0        0.500983  \n",
       "0        0.510773  \n",
       "0        0.593563  \n",
       "0        0.584651  \n",
       "0        0.535652  \n",
       "0        0.580299  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Catboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if catboost_classifier:\n",
    "    # work in progress\n",
    "    if num_gpu>0:  # test gpu available\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "    else:\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Adaboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if adaboost_classifier:\n",
    "    # work in progress\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_count, y_train, x_test_count, y_test, name='Adaboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Adaboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Adaboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Adaboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lgbm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>LightGBM</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if lightgbm_classifier:\n",
    "    \n",
    "    # work in progress\n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.083657</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>0.0876408</td>\n",
       "      <td>0.0806589</td>\n",
       "      <td>0.0848186</td>\n",
       "      <td>0.00268425</td>\n",
       "      <td>0.0557067</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791759</td>\n",
       "      <td>0.714573</td>\n",
       "      <td>0.751189</td>\n",
       "      <td>711</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>284</td>\n",
       "      <td>0.559446</td>\n",
       "      <td>0.561763</td>\n",
       "      <td>0.777644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0570619</td>\n",
       "      <td>0.0624511</td>\n",
       "      <td>0.0512819</td>\n",
       "      <td>0.0743842</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.0559588</td>\n",
       "      <td>0.0131171</td>\n",
       "      <td>0.0463171</td>\n",
       "      <td>0.0433159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.58191</td>\n",
       "      <td>0.681176</td>\n",
       "      <td>579</td>\n",
       "      <td>1048</td>\n",
       "      <td>126</td>\n",
       "      <td>416</td>\n",
       "      <td>0.485369</td>\n",
       "      <td>0.504886</td>\n",
       "      <td>0.737292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.0146081</td>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.0386279</td>\n",
       "      <td>0.0167282</td>\n",
       "      <td>0.0169821</td>\n",
       "      <td>0.0236558</td>\n",
       "      <td>0.00956548</td>\n",
       "      <td>0.0413001</td>\n",
       "      <td>0.0396559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730563</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>0.626077</td>\n",
       "      <td>545</td>\n",
       "      <td>973</td>\n",
       "      <td>201</td>\n",
       "      <td>450</td>\n",
       "      <td>0.383852</td>\n",
       "      <td>0.394977</td>\n",
       "      <td>0.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.849732</td>\n",
       "      <td>0.797022</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.13063</td>\n",
       "      <td>0.286705</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.303969</td>\n",
       "      <td>0.125775</td>\n",
       "      <td>0.0687079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.188948</td>\n",
       "      <td>106</td>\n",
       "      <td>1153</td>\n",
       "      <td>21</td>\n",
       "      <td>889</td>\n",
       "      <td>0.0949604</td>\n",
       "      <td>0.188135</td>\n",
       "      <td>0.544323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>18.4121</td>\n",
       "      <td>19.1657</td>\n",
       "      <td>17.922</td>\n",
       "      <td>19.1001</td>\n",
       "      <td>9.19119</td>\n",
       "      <td>16.7582</td>\n",
       "      <td>3.81129</td>\n",
       "      <td>0.0704119</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738872</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.744766</td>\n",
       "      <td>747</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>248</td>\n",
       "      <td>0.525238</td>\n",
       "      <td>0.525295</td>\n",
       "      <td>0.762941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.667456</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.61599</td>\n",
       "      <td>0.690176</td>\n",
       "      <td>0.490711</td>\n",
       "      <td>0.626685</td>\n",
       "      <td>0.0722426</td>\n",
       "      <td>0.0417788</td>\n",
       "      <td>0.0403888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771372</td>\n",
       "      <td>0.779899</td>\n",
       "      <td>0.775612</td>\n",
       "      <td>776</td>\n",
       "      <td>944</td>\n",
       "      <td>230</td>\n",
       "      <td>219</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>0.583527</td>\n",
       "      <td>0.791994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.495503</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.355603</td>\n",
       "      <td>0.409944</td>\n",
       "      <td>0.34902</td>\n",
       "      <td>0.407818</td>\n",
       "      <td>0.0535282</td>\n",
       "      <td>0.0507712</td>\n",
       "      <td>0.0475352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701226</td>\n",
       "      <td>0.632161</td>\n",
       "      <td>0.664905</td>\n",
       "      <td>629</td>\n",
       "      <td>906</td>\n",
       "      <td>268</td>\n",
       "      <td>366</td>\n",
       "      <td>0.406937</td>\n",
       "      <td>0.408658</td>\n",
       "      <td>0.701941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>4.52604</td>\n",
       "      <td>5.30917</td>\n",
       "      <td>5.46984</td>\n",
       "      <td>5.11398</td>\n",
       "      <td>3.58113</td>\n",
       "      <td>4.80003</td>\n",
       "      <td>0.68807</td>\n",
       "      <td>0.07074</td>\n",
       "      <td>0.0679901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.730192</td>\n",
       "      <td>705</td>\n",
       "      <td>943</td>\n",
       "      <td>231</td>\n",
       "      <td>290</td>\n",
       "      <td>0.514104</td>\n",
       "      <td>0.514884</td>\n",
       "      <td>0.75589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>142.24</td>\n",
       "      <td>143.032</td>\n",
       "      <td>143.314</td>\n",
       "      <td>144.206</td>\n",
       "      <td>113.892</td>\n",
       "      <td>137.337</td>\n",
       "      <td>11.7393</td>\n",
       "      <td>45.6131</td>\n",
       "      <td>45.6117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750984</td>\n",
       "      <td>0.766834</td>\n",
       "      <td>0.758826</td>\n",
       "      <td>763</td>\n",
       "      <td>921</td>\n",
       "      <td>253</td>\n",
       "      <td>232</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.55055</td>\n",
       "      <td>0.775666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>171.507</td>\n",
       "      <td>164.749</td>\n",
       "      <td>171.223</td>\n",
       "      <td>171.68</td>\n",
       "      <td>118.665</td>\n",
       "      <td>159.565</td>\n",
       "      <td>20.6152</td>\n",
       "      <td>51.6323</td>\n",
       "      <td>49.8943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760194</td>\n",
       "      <td>0.786935</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>783</td>\n",
       "      <td>927</td>\n",
       "      <td>247</td>\n",
       "      <td>212</td>\n",
       "      <td>0.575001</td>\n",
       "      <td>0.575303</td>\n",
       "      <td>0.788271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>67.1528</td>\n",
       "      <td>65.9447</td>\n",
       "      <td>66.7256</td>\n",
       "      <td>65.973</td>\n",
       "      <td>32.62</td>\n",
       "      <td>59.6832</td>\n",
       "      <td>13.5394</td>\n",
       "      <td>24.1372</td>\n",
       "      <td>25.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.621106</td>\n",
       "      <td>0.658147</td>\n",
       "      <td>618</td>\n",
       "      <td>909</td>\n",
       "      <td>265</td>\n",
       "      <td>377</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.401015</td>\n",
       "      <td>0.697691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>567.083</td>\n",
       "      <td>577.67</td>\n",
       "      <td>578.479</td>\n",
       "      <td>579.833</td>\n",
       "      <td>336.997</td>\n",
       "      <td>528.013</td>\n",
       "      <td>95.6156</td>\n",
       "      <td>186.596</td>\n",
       "      <td>184.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>0.759799</td>\n",
       "      <td>0.758655</td>\n",
       "      <td>756</td>\n",
       "      <td>932</td>\n",
       "      <td>242</td>\n",
       "      <td>239</td>\n",
       "      <td>0.553539</td>\n",
       "      <td>0.553541</td>\n",
       "      <td>0.776833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.072355</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0731351</td>\n",
       "      <td>0.0939651</td>\n",
       "      <td>0.0768281</td>\n",
       "      <td>0.00857692</td>\n",
       "      <td>3.7683</td>\n",
       "      <td>3.65425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586263</td>\n",
       "      <td>0.720603</td>\n",
       "      <td>0.646528</td>\n",
       "      <td>717</td>\n",
       "      <td>668</td>\n",
       "      <td>506</td>\n",
       "      <td>278</td>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.290993</td>\n",
       "      <td>0.644799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.049027</td>\n",
       "      <td>0.0487049</td>\n",
       "      <td>0.051249</td>\n",
       "      <td>0.0619388</td>\n",
       "      <td>0.0523279</td>\n",
       "      <td>0.0526495</td>\n",
       "      <td>0.00483814</td>\n",
       "      <td>4.03613</td>\n",
       "      <td>3.9824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751938</td>\n",
       "      <td>0.0974874</td>\n",
       "      <td>0.172598</td>\n",
       "      <td>97</td>\n",
       "      <td>1142</td>\n",
       "      <td>32</td>\n",
       "      <td>898</td>\n",
       "      <td>0.0752212</td>\n",
       "      <td>0.147965</td>\n",
       "      <td>0.535115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.0136449</td>\n",
       "      <td>0.020715</td>\n",
       "      <td>0.0663297</td>\n",
       "      <td>0.0168662</td>\n",
       "      <td>0.0261462</td>\n",
       "      <td>0.0202725</td>\n",
       "      <td>1.33747</td>\n",
       "      <td>1.33642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584323</td>\n",
       "      <td>0.494472</td>\n",
       "      <td>0.535656</td>\n",
       "      <td>492</td>\n",
       "      <td>824</td>\n",
       "      <td>350</td>\n",
       "      <td>503</td>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.20076</td>\n",
       "      <td>0.598173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.655702</td>\n",
       "      <td>0.672649</td>\n",
       "      <td>0.65748</td>\n",
       "      <td>0.667741</td>\n",
       "      <td>0.195899</td>\n",
       "      <td>0.569894</td>\n",
       "      <td>0.187104</td>\n",
       "      <td>48.1581</td>\n",
       "      <td>48.1758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739785</td>\n",
       "      <td>0.345729</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>344</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>651</td>\n",
       "      <td>0.252933</td>\n",
       "      <td>0.294636</td>\n",
       "      <td>0.621331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.458804</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.420624</td>\n",
       "      <td>0.444952</td>\n",
       "      <td>0.266678</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.0751494</td>\n",
       "      <td>0.0419219</td>\n",
       "      <td>0.0525622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>0.872362</td>\n",
       "      <td>0.747631</td>\n",
       "      <td>868</td>\n",
       "      <td>715</td>\n",
       "      <td>459</td>\n",
       "      <td>127</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>0.492212</td>\n",
       "      <td>0.740695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>0.269268</td>\n",
       "      <td>0.277042</td>\n",
       "      <td>0.274197</td>\n",
       "      <td>0.175028</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.0459828</td>\n",
       "      <td>0.03966</td>\n",
       "      <td>0.039535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742604</td>\n",
       "      <td>0.756784</td>\n",
       "      <td>0.749627</td>\n",
       "      <td>753</td>\n",
       "      <td>913</td>\n",
       "      <td>261</td>\n",
       "      <td>242</td>\n",
       "      <td>0.53369</td>\n",
       "      <td>0.533773</td>\n",
       "      <td>0.767234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.118953</td>\n",
       "      <td>0.126627</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.105808</td>\n",
       "      <td>0.0728922</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>0.0183905</td>\n",
       "      <td>0.0420618</td>\n",
       "      <td>0.03441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.678392</td>\n",
       "      <td>0.65534</td>\n",
       "      <td>675</td>\n",
       "      <td>784</td>\n",
       "      <td>390</td>\n",
       "      <td>320</td>\n",
       "      <td>0.344348</td>\n",
       "      <td>0.345069</td>\n",
       "      <td>0.673097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.92091</td>\n",
       "      <td>1.93717</td>\n",
       "      <td>1.7542</td>\n",
       "      <td>1.82993</td>\n",
       "      <td>1.36733</td>\n",
       "      <td>1.76191</td>\n",
       "      <td>0.208027</td>\n",
       "      <td>0.0974619</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828369</td>\n",
       "      <td>0.586935</td>\n",
       "      <td>0.687059</td>\n",
       "      <td>584</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>411</td>\n",
       "      <td>0.494864</td>\n",
       "      <td>0.514763</td>\n",
       "      <td>0.741934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>62.9065</td>\n",
       "      <td>55.3991</td>\n",
       "      <td>56.9035</td>\n",
       "      <td>52.061</td>\n",
       "      <td>27.6382</td>\n",
       "      <td>50.9817</td>\n",
       "      <td>12.189</td>\n",
       "      <td>0.0316858</td>\n",
       "      <td>0.0516238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698055</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.691684</td>\n",
       "      <td>682</td>\n",
       "      <td>879</td>\n",
       "      <td>295</td>\n",
       "      <td>313</td>\n",
       "      <td>0.434749</td>\n",
       "      <td>0.43481</td>\n",
       "      <td>0.717075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>5.80552</td>\n",
       "      <td>7.31059</td>\n",
       "      <td>7.17226</td>\n",
       "      <td>6.93111</td>\n",
       "      <td>5.58752</td>\n",
       "      <td>6.5614</td>\n",
       "      <td>0.719849</td>\n",
       "      <td>0.0432222</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.566632</td>\n",
       "      <td>540</td>\n",
       "      <td>803</td>\n",
       "      <td>371</td>\n",
       "      <td>455</td>\n",
       "      <td>0.228168</td>\n",
       "      <td>0.228875</td>\n",
       "      <td>0.61335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>220.842</td>\n",
       "      <td>238.157</td>\n",
       "      <td>282.195</td>\n",
       "      <td>257.065</td>\n",
       "      <td>184.308</td>\n",
       "      <td>236.513</td>\n",
       "      <td>33.1146</td>\n",
       "      <td>0.0761411</td>\n",
       "      <td>0.0745621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719447</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.69938</td>\n",
       "      <td>677</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>318</td>\n",
       "      <td>0.457422</td>\n",
       "      <td>0.458003</td>\n",
       "      <td>0.727765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>92.4755</td>\n",
       "      <td>84.2349</td>\n",
       "      <td>73.6215</td>\n",
       "      <td>84.1027</td>\n",
       "      <td>48.617</td>\n",
       "      <td>76.6103</td>\n",
       "      <td>15.2219</td>\n",
       "      <td>0.0370519</td>\n",
       "      <td>0.0738072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>700</td>\n",
       "      <td>894</td>\n",
       "      <td>280</td>\n",
       "      <td>295</td>\n",
       "      <td>0.465552</td>\n",
       "      <td>0.465597</td>\n",
       "      <td>0.732508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>74.1953</td>\n",
       "      <td>65.7518</td>\n",
       "      <td>84.2132</td>\n",
       "      <td>65.1882</td>\n",
       "      <td>32.8804</td>\n",
       "      <td>64.4458</td>\n",
       "      <td>17.2304</td>\n",
       "      <td>0.516725</td>\n",
       "      <td>0.95696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.751759</td>\n",
       "      <td>0.748749</td>\n",
       "      <td>748</td>\n",
       "      <td>919</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>0.534225</td>\n",
       "      <td>0.53424</td>\n",
       "      <td>0.767276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>46.6946</td>\n",
       "      <td>49.373</td>\n",
       "      <td>52.0918</td>\n",
       "      <td>37.7714</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>42.1696</td>\n",
       "      <td>9.87732</td>\n",
       "      <td>0.425461</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727728</td>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.729188</td>\n",
       "      <td>727</td>\n",
       "      <td>902</td>\n",
       "      <td>272</td>\n",
       "      <td>268</td>\n",
       "      <td>0.498814</td>\n",
       "      <td>0.498817</td>\n",
       "      <td>0.749483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>14.4524</td>\n",
       "      <td>14.8459</td>\n",
       "      <td>10.1767</td>\n",
       "      <td>6.22884</td>\n",
       "      <td>11.4074</td>\n",
       "      <td>11.4223</td>\n",
       "      <td>3.14409</td>\n",
       "      <td>0.109201</td>\n",
       "      <td>0.0818152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640324</td>\n",
       "      <td>0.635176</td>\n",
       "      <td>0.63774</td>\n",
       "      <td>632</td>\n",
       "      <td>819</td>\n",
       "      <td>355</td>\n",
       "      <td>363</td>\n",
       "      <td>0.332995</td>\n",
       "      <td>0.333004</td>\n",
       "      <td>0.666395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>307.123</td>\n",
       "      <td>379.482</td>\n",
       "      <td>492.242</td>\n",
       "      <td>429.361</td>\n",
       "      <td>213.657</td>\n",
       "      <td>364.373</td>\n",
       "      <td>96.7421</td>\n",
       "      <td>3.22035</td>\n",
       "      <td>2.00201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744376</td>\n",
       "      <td>0.731658</td>\n",
       "      <td>0.737962</td>\n",
       "      <td>728</td>\n",
       "      <td>924</td>\n",
       "      <td>250</td>\n",
       "      <td>267</td>\n",
       "      <td>0.519388</td>\n",
       "      <td>0.519452</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>31.9763</td>\n",
       "      <td>31.7864</td>\n",
       "      <td>31.8414</td>\n",
       "      <td>31.192</td>\n",
       "      <td>8.30906</td>\n",
       "      <td>27.021</td>\n",
       "      <td>9.35985</td>\n",
       "      <td>0.502497</td>\n",
       "      <td>0.662884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.527638</td>\n",
       "      <td>0.629874</td>\n",
       "      <td>525</td>\n",
       "      <td>1027</td>\n",
       "      <td>147</td>\n",
       "      <td>470</td>\n",
       "      <td>0.412637</td>\n",
       "      <td>0.433646</td>\n",
       "      <td>0.701213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>24.037</td>\n",
       "      <td>24.4264</td>\n",
       "      <td>24.231</td>\n",
       "      <td>24.4041</td>\n",
       "      <td>6.19139</td>\n",
       "      <td>20.658</td>\n",
       "      <td>7.23465</td>\n",
       "      <td>0.252068</td>\n",
       "      <td>0.331617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751989</td>\n",
       "      <td>0.569849</td>\n",
       "      <td>0.64837</td>\n",
       "      <td>567</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>428</td>\n",
       "      <td>0.418288</td>\n",
       "      <td>0.429599</td>\n",
       "      <td>0.705282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>16.2884</td>\n",
       "      <td>16.1502</td>\n",
       "      <td>16.4692</td>\n",
       "      <td>16.4074</td>\n",
       "      <td>4.51221</td>\n",
       "      <td>13.9655</td>\n",
       "      <td>4.72788</td>\n",
       "      <td>0.128438</td>\n",
       "      <td>0.119955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615965</td>\n",
       "      <td>0.573869</td>\n",
       "      <td>0.594173</td>\n",
       "      <td>571</td>\n",
       "      <td>818</td>\n",
       "      <td>356</td>\n",
       "      <td>424</td>\n",
       "      <td>0.27205</td>\n",
       "      <td>0.272599</td>\n",
       "      <td>0.635316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>76.4607</td>\n",
       "      <td>76.9057</td>\n",
       "      <td>76.4275</td>\n",
       "      <td>76.0929</td>\n",
       "      <td>17.0665</td>\n",
       "      <td>64.5907</td>\n",
       "      <td>23.7635</td>\n",
       "      <td>1.07035</td>\n",
       "      <td>0.870651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729829</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.658577</td>\n",
       "      <td>597</td>\n",
       "      <td>953</td>\n",
       "      <td>221</td>\n",
       "      <td>398</td>\n",
       "      <td>0.417415</td>\n",
       "      <td>0.423331</td>\n",
       "      <td>0.705877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.083657     0.087841     0.084295    0.0876408   \n",
       "0    NB_WordLevel_TF-IDF    0.0570619    0.0624511    0.0512819    0.0743842   \n",
       "0       NB_N-Gram_TF-IDF    0.0146081     0.031333    0.0386279    0.0167282   \n",
       "0    NB_CharLevel_TF-IDF     0.849732     0.797022       1.1039      1.13063   \n",
       "0       LR_Count_Vectors      18.4121      19.1657       17.922      19.1001   \n",
       "0    LR_WordLevel_TF-IDF     0.667456     0.669091      0.61599     0.690176   \n",
       "0       LR_N-Gram_TF-IDF     0.495503     0.429021     0.355603     0.409944   \n",
       "0    LR_CharLevel_TF-IDF      4.52604      5.30917      5.46984      5.11398   \n",
       "0      SVM_Count_Vectors       142.24      143.032      143.314      144.206   \n",
       "0   SVM_WordLevel_TF-IDF      171.507      164.749      171.223       171.68   \n",
       "0      SVM_N-Gram_TF-IDF      67.1528      65.9447      66.7256       65.973   \n",
       "0   SVM_CharLevel_TF-IDF      567.083       577.67      578.479      579.833   \n",
       "0      kNN_Count_Vectors     0.071985     0.072355       0.0727    0.0731351   \n",
       "0   kNN_WordLevel_TF-IDF     0.049027    0.0487049     0.051249    0.0619388   \n",
       "0      kNN_N-Gram_TF-IDF     0.013175    0.0136449     0.020715    0.0663297   \n",
       "0   kNN_CharLevel_TF-IDF     0.655702     0.672649      0.65748     0.667741   \n",
       "0      SGD_Count_Vectors     0.458804     0.473416     0.420624     0.444952   \n",
       "0   SGD_WordLevel_TF-IDF       0.3128     0.269268     0.277042     0.274197   \n",
       "0     SGD_N-Gram_Vectors     0.118953     0.126627       0.1075     0.105808   \n",
       "0  SGD_CharLevel_Vectors      1.92091      1.93717       1.7542      1.82993   \n",
       "0       GB_Count_Vectors      62.9065      55.3991      56.9035       52.061   \n",
       "0       GB_N-Gram_TF-IDF      5.80552      7.31059      7.17226      6.93111   \n",
       "0    GB_CharLevel_TF-IDF      220.842      238.157      282.195      257.065   \n",
       "0    GB_WordLevel_TF-IDF      92.4755      84.2349      73.6215      84.1027   \n",
       "0      XGB_Count_Vectors      74.1953      65.7518      84.2132      65.1882   \n",
       "0   XGB_WordLevel_TF-IDF      46.6946       49.373      52.0918      37.7714   \n",
       "0      XGB_N-Gram_TF-IDF      14.4524      14.8459      10.1767      6.22884   \n",
       "0   XGB_CharLevel_TF-IDF      307.123      379.482      492.242      429.361   \n",
       "0       RF_Count_Vectors      31.9763      31.7864      31.8414       31.192   \n",
       "0    RF_WordLevel_TF-IDF       24.037      24.4264       24.231      24.4041   \n",
       "0       RF_N-Gram_TF-IDF      16.2884      16.1502      16.4692      16.4074   \n",
       "0    RF_CharLevel_TF-IDF      76.4607      76.9057      76.4275      76.0929   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0806589     0.0848186   0.00268425      0.0557067      0.0472348  ...   \n",
       "0     0.034615     0.0559588    0.0131171      0.0463171      0.0433159  ...   \n",
       "0    0.0169821     0.0236558   0.00956548      0.0413001      0.0396559  ...   \n",
       "0     0.286705      0.833599     0.303969       0.125775      0.0687079  ...   \n",
       "0      9.19119       16.7582      3.81129      0.0704119      0.0472348  ...   \n",
       "0     0.490711      0.626685    0.0722426      0.0417788      0.0403888  ...   \n",
       "0      0.34902      0.407818    0.0535282      0.0507712      0.0475352  ...   \n",
       "0      3.58113       4.80003      0.68807        0.07074      0.0679901  ...   \n",
       "0      113.892       137.337      11.7393        45.6131        45.6117  ...   \n",
       "0      118.665       159.565      20.6152        51.6323        49.8943  ...   \n",
       "0        32.62       59.6832      13.5394        24.1372        25.0592  ...   \n",
       "0      336.997       528.013      95.6156        186.596        184.288  ...   \n",
       "0    0.0939651     0.0768281   0.00857692         3.7683        3.65425  ...   \n",
       "0    0.0523279     0.0526495   0.00483814        4.03613         3.9824  ...   \n",
       "0    0.0168662     0.0261462    0.0202725        1.33747        1.33642  ...   \n",
       "0     0.195899      0.569894     0.187104        48.1581        48.1758  ...   \n",
       "0     0.266678      0.412895    0.0751494      0.0419219      0.0525622  ...   \n",
       "0     0.175028      0.261667    0.0459828        0.03966       0.039535  ...   \n",
       "0    0.0728922      0.106356    0.0183905      0.0420618        0.03441  ...   \n",
       "0      1.36733       1.76191     0.208027      0.0974619       0.103405  ...   \n",
       "0      27.6382       50.9817       12.189      0.0316858      0.0516238  ...   \n",
       "0      5.58752        6.5614     0.719849      0.0432222       0.025759  ...   \n",
       "0      184.308       236.513      33.1146      0.0761411      0.0745621  ...   \n",
       "0       48.617       76.6103      15.2219      0.0370519      0.0738072  ...   \n",
       "0      32.8804       64.4458      17.2304       0.516725        0.95696  ...   \n",
       "0      24.9171       42.1696      9.87732       0.425461       0.305956  ...   \n",
       "0      11.4074       11.4223      3.14409       0.109201      0.0818152  ...   \n",
       "0      213.657       364.373      96.7421        3.22035        2.00201  ...   \n",
       "0      8.30906        27.021      9.35985       0.502497       0.662884  ...   \n",
       "0      6.19139        20.658      7.23465       0.252068       0.331617  ...   \n",
       "0      4.51221       13.9655      4.72788       0.128438       0.119955  ...   \n",
       "0      17.0665       64.5907      23.7635        1.07035       0.870651  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.791759       0.714573         0.751189        711        987   \n",
       "0     0.821277        0.58191         0.681176        579       1048   \n",
       "0     0.730563       0.547739         0.626077        545        973   \n",
       "0     0.834646       0.106533         0.188948        106       1153   \n",
       "0     0.738872       0.750754         0.744766        747        910   \n",
       "0     0.771372       0.779899         0.775612        776        944   \n",
       "0     0.701226       0.632161         0.664905        629        906   \n",
       "0     0.753205       0.708543         0.730192        705        943   \n",
       "0     0.750984       0.766834         0.758826        763        921   \n",
       "0     0.760194       0.786935         0.773333        783        927   \n",
       "0     0.699887       0.621106         0.658147        618        909   \n",
       "0     0.757515       0.759799         0.758655        756        932   \n",
       "0     0.586263       0.720603         0.646528        717        668   \n",
       "0     0.751938      0.0974874         0.172598         97       1142   \n",
       "0     0.584323       0.494472         0.535656        492        824   \n",
       "0     0.739785       0.345729         0.471233        344       1053   \n",
       "0     0.654107       0.872362         0.747631        868        715   \n",
       "0     0.742604       0.756784         0.749627        753        913   \n",
       "0     0.633803       0.678392          0.65534        675        784   \n",
       "0     0.828369       0.586935         0.687059        584       1053   \n",
       "0     0.698055       0.685427         0.691684        682        879   \n",
       "0     0.592755       0.542714         0.566632        540        803   \n",
       "0     0.719447       0.680402          0.69938        677        910   \n",
       "0     0.714286       0.703518         0.708861        700        894   \n",
       "0     0.745763       0.751759         0.748749        748        919   \n",
       "0     0.727728       0.730653         0.729188        727        902   \n",
       "0     0.640324       0.635176          0.63774        632        819   \n",
       "0     0.744376       0.731658         0.737962        728        924   \n",
       "0      0.78125       0.527638         0.629874        525       1027   \n",
       "0     0.751989       0.569849          0.64837        567        987   \n",
       "0     0.615965       0.573869         0.594173        571        818   \n",
       "0     0.729829            0.6         0.658577        597        953   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        187        284             0.559446                  0.561763   \n",
       "0        126        416             0.485369                  0.504886   \n",
       "0        201        450             0.383852                  0.394977   \n",
       "0         21        889            0.0949604                  0.188135   \n",
       "0        264        248             0.525238                  0.525295   \n",
       "0        230        219             0.583496                  0.583527   \n",
       "0        268        366             0.406937                  0.408658   \n",
       "0        231        290             0.514104                  0.514884   \n",
       "0        253        232             0.550446                   0.55055   \n",
       "0        247        212             0.575001                  0.575303   \n",
       "0        265        377             0.398804                  0.401015   \n",
       "0        242        239             0.553539                  0.553541   \n",
       "0        506        278             0.284626                  0.290993   \n",
       "0         32        898            0.0752212                  0.147965   \n",
       "0        350        503             0.198675                   0.20076   \n",
       "0        121        651             0.252933                  0.294636   \n",
       "0        459        127             0.469449                  0.492212   \n",
       "0        261        242              0.53369                  0.533773   \n",
       "0        390        320             0.344348                  0.345069   \n",
       "0        121        411             0.494864                  0.514763   \n",
       "0        295        313             0.434749                   0.43481   \n",
       "0        371        455             0.228168                  0.228875   \n",
       "0        264        318             0.457422                  0.458003   \n",
       "0        280        295             0.465552                  0.465597   \n",
       "0        255        247             0.534225                   0.53424   \n",
       "0        272        268             0.498814                  0.498817   \n",
       "0        355        363             0.332995                  0.333004   \n",
       "0        250        267             0.519388                  0.519452   \n",
       "0        147        470             0.412637                  0.433646   \n",
       "0        187        428             0.418288                  0.429599   \n",
       "0        356        424              0.27205                  0.272599   \n",
       "0        221        398             0.417415                  0.423331   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0        0.777644  \n",
       "0        0.737292  \n",
       "0        0.688265  \n",
       "0        0.544323  \n",
       "0        0.762941  \n",
       "0        0.791994  \n",
       "0        0.701941  \n",
       "0         0.75589  \n",
       "0        0.775666  \n",
       "0        0.788271  \n",
       "0        0.697691  \n",
       "0        0.776833  \n",
       "0        0.644799  \n",
       "0        0.535115  \n",
       "0        0.598173  \n",
       "0        0.621331  \n",
       "0        0.740695  \n",
       "0        0.767234  \n",
       "0        0.673097  \n",
       "0        0.741934  \n",
       "0        0.717075  \n",
       "0         0.61335  \n",
       "0        0.727765  \n",
       "0        0.732508  \n",
       "0        0.767276  \n",
       "0        0.749483  \n",
       "0        0.666395  \n",
       "0        0.759356  \n",
       "0        0.701213  \n",
       "0        0.705282  \n",
       "0        0.635316  \n",
       "0        0.705877  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dl'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "pretrained = fasttext.FastText.load_model('/Users/diego/Documents/NLP/crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88180/88180 [00:04<00:00, 18509.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.54 s, sys: 1.81 s, total: 11.4 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a tokenizer \n",
    "token = Tokenizer(oov_token='<OOV>')\n",
    "token.fit_on_texts(df[TEXT])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=300)\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "words = []\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    words.append(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "if save_model:\n",
    "    filename = NAME_TOKEN_EMBEDDINGS\n",
    "    pickle.dump(token, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_w = {}\n",
    "for i in zip(range(len(class_weights)), class_weights):\n",
    "    class_w[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_NN(model, X, y, X_test, y_test,name=\"NN\", fit_params=None, scoring=None, n_splits=5, save=save_model, batch_size = 32,  use_multiprocessing=True):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param model: (model) neural network model\n",
    "    @param X: (list or matrix or tensor) training X data\n",
    "    @param y: (list) label data \n",
    "    @param X_test: (list or matrix or tensor) testing X data\n",
    "    @param y_test: (list) label test data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param scoring: (dict) dictionary of metrics and names\n",
    "    @param n_splits: (int) number of fold for cross-validation (default 5)\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    # ---- Parameters initialisation\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='auto', patience=3)\n",
    "    seed = 42\n",
    "    k = 1\n",
    "    np.random.seed(seed)\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Creation of list for each metric\n",
    "    if scoring==None:        # create a dictionary if none is passed\n",
    "        dic_scoring = {}\n",
    "    if scoring!=None:        # save the dict \n",
    "        dic_score = scoring.copy()\n",
    "    \n",
    "    dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "    dic_score[\"score_time\"] = None\n",
    "    scorer = {}\n",
    "    for i in dic_score.keys(): \n",
    "        scorer[i] = []\n",
    "    \n",
    "    index = [\"Model\"]\n",
    "    results = [name]\n",
    "    # ---- Loop on k-fold for cross-valisation\n",
    "    for train, test in kfold.split(X, y):   # training NN on each fold \n",
    "        # create model\n",
    "        print(f\"k-fold : {k}\")\n",
    "        fit_start = time.time()\n",
    "        _model = tf.keras.models.clone_model(model)\n",
    "        if len(np.unique(y))==2: # binary\n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        else:  # multiclass \n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "        fit_end = time.time() - fit_start\n",
    "\n",
    "        score_start = time.time()\n",
    "        y_pred = (_model.predict(X[test])>0.5).astype(int)\n",
    "        score_end = time.time() - score_start\n",
    "        #if len(set(y))>2:\n",
    "        #    y_pred =np.argmax(y_pred,axis=1)\n",
    "        #print(y_test[0], y_pred[0])\n",
    "        if len(set(y))==2:\n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], y_pred), 3)}% , Recall: {round(100*recall_score(y.iloc[test], y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        else: \n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        \n",
    "        \n",
    "        # ---- save each metric\n",
    "        for i in dic_score.keys():    # compute metrics \n",
    "            if i == \"fit_time\":\n",
    "                scorer[i].append(fit_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(fit_end)\n",
    "                continue\n",
    "            if i == \"score_time\":\n",
    "                scorer[i].append(score_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(score_end)\n",
    "                continue\n",
    "            \n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    scorer[i].append(dic_score[i](y.iloc[test], np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "                elif i==\"roc_auc\":\n",
    "                    scorer[i].append(dic_score[i](to_categorical(y.iloc[test]), y_pred, average = 'macro', multi_class=\"ovo\")) # make each function scorer\n",
    "                else:\n",
    "                    scorer[i].append(dic_score[i]( y.iloc[test], np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i]( y.iloc[test], y_pred)) # make each function scorer\n",
    "            #scorer[i].append(dic_score[i]( y.iloc[test], y_pred))\n",
    "            index.append(\"test_\"+i+'_cv'+str(k))\n",
    "            results.append(scorer[i][-1])\n",
    "        K.clear_session()\n",
    "        del _model\n",
    "        k+=1\n",
    "    \n",
    "    # Train test on the overall data\n",
    "    print(\"Overall train-test data\")\n",
    "    fit_start = time.time()\n",
    "    _model =  tf.keras.models.clone_model(model)\n",
    "    if len(np.unique(y))==2: # binary\n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    else:  # multiclass \n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False)\n",
    "    if save:\n",
    "        check_p = tf.keras.callbacks.ModelCheckpoint(os.path.join(root_dir, dir_name, name+\".h5\"), save_best_only=True)\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es, check_p], validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    else:\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es],  validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    #_acc = _model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    score_start = time.time()\n",
    "    y_pred = (_model.predict(X_test)>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    #if len(set(y))>2:\n",
    "    #    y_pred =np.argmax(y_pred,axis=1)\n",
    "    if len(set(y))==2:\n",
    "        print(f\"Precision: {round(100*precision_score(y_test, y_pred), 3)}% , Recall: {round(100*recall_score(y_test, y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "    else: \n",
    "        print(f\"Precision: {round(100*precision_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "\n",
    "    # Compute mean and std for each metric\n",
    "    for i in scorer: \n",
    "        \n",
    "        results.append(np.mean(scorer[i]))\n",
    "        results.append(np.std(scorer[i]))\n",
    "        if i == \"fit_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        \n",
    "        index.append(\"test_\"+i+\"_mean\")\n",
    "        index.append(\"test_\"+i+\"_std\")\n",
    "        \n",
    "    # add metrics averall dataset on the dictionary \n",
    "    for i in dic_score.keys():    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            scorer[i].append(fit_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            scorer[i].append(score_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(score_end)\n",
    "            continue\n",
    "        \n",
    "        if len(set(y))>2:\n",
    "            if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "            elif i==\"roc_auc\":\n",
    "                scorer[i].append(dic_score[i](to_categorical(y_test), y_pred, average = 'weighted', multi_class=\"ovo\")) # make each function scorer\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "        else:\n",
    "            scorer[i].append(dic_score[i](y.iloc[test], y_pred))                             \n",
    "            #scorer[i].append(dic_score[i](_model, X_test, y_test))\n",
    "        index.append(i+'_overall')\n",
    "        results.append(scorer[i][-1])\n",
    "    \n",
    "            \n",
    "    return pd.DataFrame(results, index=index).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='snn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Shallow Neural Network</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a shallow neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) shallow neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 16)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      \n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "        \n",
    "      #keras.layers.Dense(6, activation=\"relu\"),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = score_metrics\n",
    "\n",
    "# Creation of list for each metric\n",
    "if scoring==None:        # create a dictionary if none is passed\n",
    "    dic_scoring = {}\n",
    "if scoring!=None:        # save the dict \n",
    "    dic_score = scoring.copy()\n",
    "\n",
    "dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "dic_score[\"score_time\"] = None\n",
    "scorer = {}\n",
    "for i in dic_score.keys(): \n",
    "    scorer[i] = []\n",
    "\n",
    "index = [\"Model\"]\n",
    "results = ['Shallow_NN_WE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dic_score.keys():\n",
    "    scorer[i].append(dic_score[i](_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score['acc'](y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430    fanboy ism h k stuff scar h acr exactly aesthe...\n",
       "5089    curse aye awww good old bargaining alarm clock...\n",
       "111     usually psychic quality knowing come specific ...\n",
       "7536    thank advice appreciate just fantastic advice ...\n",
       "1113    trying understand difference sx sx just differ...\n",
       "                              ...                        \n",
       "2926    true comfortable contradiction hi following re...\n",
       "3821    pretty christian ish t believe jesus christ go...\n",
       "5805    bat sherlock holmes quite alright agnostic spi...\n",
       "3140    try lang studying japanese sent gt using tapat...\n",
       "2770    ugh honestly inclined think behavior abusive t...\n",
       "Name: text_clean_joined, Length: 2169, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score[i](_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 71.454% , Recall: 67.391%, Time \t 237.6945 ms\n",
      "k-fold : 2\n",
      "Precision: 72.438% , Recall: 68.677%, Time \t 365.2575 ms\n",
      "k-fold : 3\n",
      "Precision: 70.662% , Recall: 66.164%, Time \t 396.282 ms\n",
      "k-fold : 4\n",
      "Precision: 73.179% , Recall: 69.012%, Time \t 263.4461 ms\n",
      "k-fold : 5\n",
      "Precision: 71.357% , Recall: 71.357%, Time \t 327.027 ms\n",
      "Overall train-test data\n",
      "Precision: 70.657% , Recall: 67.035%, Time \t 338.0158 ms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1301, 2169]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-40eebde9b6a9>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mscorer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;31m#scorer[i].append(dic_score[i](_model, X_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_overall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1301, 2169]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if shallow_network:\n",
    "    df_results = df_results.append(cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Shallow_NN_WE\", scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dnn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Deep Neural Net</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_WE\",scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits , save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var1(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_var1_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Transformers</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](img/transformers_model_architecture.png)\n",
    "\n",
    "The Transformer – Model Architecture - [Source](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=vidya></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
