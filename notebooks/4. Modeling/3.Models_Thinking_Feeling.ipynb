{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project - Modeling (step 5)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Introduction</h3>\n",
    "    <p>This notebook contains the <b>Modeling</b> step which comes after the <b>Feature Engineering & Preprocessing</b> step. The main goal of this step involves selecting, training and deploying a model to make predictive insights.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-danger\">\n",
    "\n",
    "<h3>Disclaimer</h3>\n",
    "    <p>The purpose of this notebook is to go over certain aspects of Natural Language Processing. There might be some parts of the notebook that do not have particular use for the future of this project but they are useful for learning purposes so I left them inside. I also would like to mention that some of the code here is recycled from online articles and notebooks on GitHub, I will try to mention every source as best as possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Summarized goals](#goals)\n",
    "- [Importing Libraries](#importing)\n",
    "- [Review of our Dataset](#review)\n",
    "- [Models Introduction](#model)\n",
    "- [Parameters and Models](#parameters)\n",
    "- [Stopwords](#stopwords)\n",
    "- [Train Test Split](#train_test)\n",
    "- [CountVectorizer and tf-id](#cv)\n",
    "- [Report Function](#report)\n",
    "- [Let's Start Modeling](#modeling) Every model is created with CountVectorizer, TF-IDF words, TF-IDF n_grams, TF-IDF characters\n",
    "    - [MACHINE LEARNING](#ml)\n",
    "        - [Multinomial Naive Bayes Models](#nb)  \n",
    "        - [Logistic Regression](#lr)  \n",
    "        - [Support Vector Machines](#svm)  \n",
    "        - [K-Nearest Neightbors](#knn)  \n",
    "        - [Random Forest](#NB)      \n",
    "        - [Stocastic Gradient Descent](#sgd)\n",
    "        - [Boosting](#boost)\n",
    "            - [Gradient Boosting Classifier](#gbc)\n",
    "            - [XGBoost](#xgb)\n",
    "            - [Catboost](#cb) - Pending\n",
    "            - [Adaboost](#ab) - Pending        \n",
    "            - [LightGBM](#lgbm) - Pending       \n",
    "    - [DEEP LEARNING](#dl) - Pending all section\n",
    "        - [Shallow Neural Network](#snn)\n",
    "        - [Deep Neural Network](#dnn)\n",
    "        - [Transformers](#trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=goals></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarized Goals\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best model that classifies each post into the pair of attributes of the MBTI:\n",
    " - Introversion vs. Extraversion (I vs. E)\n",
    " - Intuition vs. Sensing (N vs. S)\n",
    " - Thinking vs. Feeling (T vs. F) --> This notebook focuses on this attribute\n",
    " - Judging vs. Perceiving (J vs. P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=importing></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# data wrangiling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.transforms\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set() #\n",
    "\n",
    "# natural language processing libraries\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "import textstat\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas(desc=\"Progress!\")\n",
    "import time\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/mbti_nlp.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Models Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the columns we need\n",
    "T = df[['T','text_clean_joined']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** I will be using `Christophe Pere's` notebook as the basis for this model. All credits go to him, [here is the original notebook](https://github.com/Christophe-pere/Model-Selection/blob/master/Text_Classification_Compare_Models.ipynb) and here is his [TowardsDataScience article](https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract the true, false positive and true false negative\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=parameters></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Models\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT           = \"text_clean_joined\"\n",
    "LABEL          = \"T\"\n",
    "NAME_SAVE_FILE = \"model_selection_results_TF\" # put just the name the .csv will be added at the end\n",
    "\n",
    "# global parameters\n",
    "num_gpu                = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "CV_splits              = 5        # Number of splits for cross-validation and k-folds\n",
    "save_results           = True     # if you want an output file containing all the results\n",
    "lang                   = False    # test if you want to use Google API detection (you will need to \"import from googletrans import Translator\")\n",
    "sample                 = True     # use just a sample of data\n",
    "nb_sample              = 6000     # default value of rows if sample selected\n",
    "save_model             = True     # concat all the data representation\n",
    "root_dir               = \"models/\"       # Place here the path where you want your models stored or use /path/to/your/folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names how the files will be saved as \n",
    "NAME_ENCODER                  = \"encoder.sav\"\n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_TOKEN_EMBEDDINGS         = \"token_embeddings.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "multinomial_naive_bayes= True\n",
    "logistic_regression    = True\n",
    "svm_model              = True\n",
    "k_nn_model             = True\n",
    "sgd                    = True\n",
    "random_forest          = True\n",
    "gradient_boosting      = True\n",
    "xgboost_classifier     = True\n",
    "adaboost_classifier    = True \n",
    "catboost_classifier    = True \n",
    "lightgbm_classifier    = True \n",
    "extratrees_classifier  = True\n",
    "shallow_network        = True\n",
    "deep_nn                = True\n",
    "rnn                    = True\n",
    "lstm                   = True\n",
    "cnn                    = True\n",
    "gru                    = True\n",
    "cnn_lstm               = True\n",
    "cnn_gru                = True\n",
    "bidirectional_rnn      = True\n",
    "bidirectional_lstm     = True\n",
    "bidirectional_gru      = True\n",
    "rcnn                   = True\n",
    "transformers           = False\n",
    "pre_trained            = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is created\n"
     ]
    }
   ],
   "source": [
    "if save_model:\n",
    "    # will create the folder to save all the models\n",
    "    try:\n",
    "        dir_name =  NAME_SAVE_FILE\n",
    "        os.makedirs(os.path.join(root_dir,dir_name))\n",
    "        print(\"The folder is created\")\n",
    "    except:\n",
    "        print(\"The folder can not be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can put all the metrics you want (included in sklearn.metrics).\n",
    "score_metrics = {'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Christophe Pere` has a set of functions to clean the text but we have already done that so I will not add them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do add a remove stop words function\n",
    "def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBTI types are rarely discussed in day to day converstaions, we will take them out since they would have low prediction power\n",
    "types = [x.lower() for x in df['type'].unique()] \n",
    "types_plural = [x+'s' for x in types]\n",
    "\n",
    "# some words that appear a lot but do not add value\n",
    "additional_stop_words = ['ll','type','fe','ni','na','wa','ve','don','nt','nf', 'ti','se','op','ne'] \n",
    "\n",
    "# We put these together and include the normal stopwords from the English language\n",
    "stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS.union(additional_stop_words + types + types_plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress!: 100%|██████████| 8675/8675 [00:03<00:00, 2607.81it/s]\n"
     ]
    }
   ],
   "source": [
    "T[TEXT] = T.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the code simple (for the time being), I will start by focusing on the `Thinking / Feeling` and later implement the same process for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = T.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[TEXT], df[LABEL], test_size=0.25, random_state=42, stratify=df[LABEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Personal note on stratify:** if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(y_train),\n",
    "                                                 y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 0.9241\tclass: 0\n",
      "Class weight: 1.0894\tclass: 1\n"
     ]
    }
   ],
   "source": [
    "print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset is balanced (ratio=0.848)\n"
     ]
    }
   ],
   "source": [
    "# Determined if the dataset is balanced or imbalanced \n",
    "ratio = np.min(df[LABEL].value_counts()) / np.max(df[LABEL].value_counts())\n",
    "if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "    balanced = True\n",
    "    print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "else:\n",
    "    balanced = False\n",
    "    print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "    #from imblearn.over_sampling import ADASYN\n",
    "    # put class for debalanced data \n",
    "    # in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer & TF-IDF\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section transforms our data into something interpretable by the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.44 s, sys: 259 ms, total: 9.7 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df[TEXT])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "x_train_count =  count_vect.transform(X_train)\n",
    "x_test_count =  count_vect.transform(X_test)\n",
    "\n",
    "if save_model:\n",
    "    # save the model to disk\n",
    "    filename = NAME_COUNT_VECT_MODEL\n",
    "    pickle.dump(count_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n",
      "CPU times: user 2min 17s, sys: 2.44 s, total: 2min 19s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "tfidf_vect.fit(df[TEXT])\n",
    "x_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "x_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "print(\"word level tf-idf done\")\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect_ngram.fit(df[TEXT])\n",
    "x_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "x_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "print(\"ngram level tf-idf done\")\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) \n",
    "tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "x_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "x_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) \n",
    "print(\"characters level tf-idf done\")\n",
    "\n",
    "if save_model:\n",
    "    # save the model tf-idf to disk\n",
    "    filename = NAME_TF_IDF_MODEL\n",
    "    pickle.dump(tfidf_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "\n",
    "    # save the model ngram to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # save the model ngram char to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_CHAR_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram_chars, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Function\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followign function will generate, for each model we create, a set of metrics that evaluate how well that model did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, x, y, X_test, y_test, name='classifier', cv=5, dict_scoring=None, fit_params=None, save=save_model):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param clf: (model) classifier\n",
    "    @param x: (list or matrix or tensor) training x data\n",
    "    @param y: (list) label data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param cv: (int) number of fold for cross-validation (default 5)\n",
    "    @param dict_scoring: (dict) dictionary of metrics and names\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param save: (bool) determine if the model need to be saved\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    \n",
    "    '''{'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}'''\n",
    "    \n",
    "    \n",
    "    if dict_scoring!=None:\n",
    "        score = dict_scoring.copy() # save the original dictionary\n",
    "        for i in score.keys():\n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted') # make each function scorer\n",
    "                elif i==\"roc_auc\":\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted', multi_class=\"ovo\",needs_proba=True) # make each function scorer\n",
    "                else:\n",
    "                    score[i] = make_scorer(score[i]) # make each function scorer\n",
    "                    \n",
    "            else:\n",
    "                score[i] = make_scorer(score[i]) # make each function scorer\n",
    "            \n",
    "    try:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False, n_jobs=-1,  fit_params=fit_params)\n",
    "    except:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False,  fit_params=fit_params)\n",
    "        \n",
    "    # Train test on the overall data\n",
    "    fit_start = time.time()\n",
    "    _model = clf\n",
    "    _model.fit(x, y)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    \n",
    "    score_start = time.time()\n",
    "    y_pred = _model.predict(X_test)#>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    \n",
    "    # this saves the model for reuse\n",
    "    if save:\n",
    "        filename= name+\".sav\"\n",
    "        pickle.dump(_model, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # initialisation \n",
    "    index = []\n",
    "    value = []\n",
    "    index.append(\"Model\")\n",
    "    value.append(name)\n",
    "    for i in scores:  # loop on each metric generate text and values\n",
    "        if i == \"estimator\":\n",
    "            continue\n",
    "        for j in enumerate(scores[i]):\n",
    "            index.append(i+\"_cv\"+str(j[0]+1))\n",
    "            value.append(j[1])\n",
    "        \n",
    "        \n",
    "        index.append(i+\"_mean\")\n",
    "        value.append(np.mean(scores[i]))\n",
    "        index.append(i+\"_std\")\n",
    "        value.append(np.std(scores[i]))\n",
    "    \n",
    "     # add metrics averall dataset on the dictionary \n",
    "    \n",
    "    for i in scores:    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,fit_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,score_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(score_end)\n",
    "            continue\n",
    "              \n",
    "        \n",
    "        scores[i] = np.append(scores[i] ,score[i.split(\"test_\")[-1]](_model, X_test, y_test))\n",
    "        index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "        value.append(scores[i][-1])\n",
    "    \n",
    "    return pd.DataFrame(data=value, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Modeling!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating the empty dataframe we will use to put the results of each model we create\n",
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Multinomial Naïve Bayes</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 803 ms, sys: 272 ms, total: 1.08 s\n",
      "Wall time: 5.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if multinomial_naive_bayes:\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_count, y_train, x_test_count, y_test, name='NB_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf, y_train, x_test_tfidf, y_test, name='NB_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='NB_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='NB_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Logistic Regression</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.2 s, sys: 4.09 s, total: 44.3 s\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if logistic_regression:\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_count, y_train, x_test_count, y_test, name='LR_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='LR_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='LR_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='LR_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Support Vector Machine</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 1min 54s, sys: 7.14 s, total: 1h 2min 1s\n",
      "Wall time: 1h 28min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if svm_model:\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_count, y_train, x_test_count, y_test, name='SVM_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SVM_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SVM_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SVM_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>K-Nearest Neighbors</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 40s, sys: 19.2 s, total: 16min 59s\n",
      "Wall time: 5min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if k_nn_model:\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_count, y_train, x_test_count, y_test, name='kNN_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf, y_train, x_test_tfidf, y_test, name='kNN_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='kNN_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='kNN_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Random Forest</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 1.49 s, total: 2min 43s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_count, y_train, x_test_count, y_test, name='RF_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf, y_train, x_test_tfidf, y_test, name='RF_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='RF_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='RF_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Stocastis Gradient Descent</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 212 ms, total: 1.94 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if sgd:\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_count, y_train, x_test_count, y_test, name='SGD_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SGD_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SGD_N-Gram_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SGD_CharLevel_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boost'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gbc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Gradient Boosting Classifier</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 220 ms, total: 31.5 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_count, y_train, x_test_count, y_test,\n",
    "                                          name='GB_Count_Vectors', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 s, sys: 170 ms, total: 39.1 s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf, y_train, x_test_tfidf, y_test,\n",
    "                                          name='GB_WordLevel_TF-IDF', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.01 s, sys: 54.3 ms, total: 5.06 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test,\n",
    "                                          name='GB_N-Gram_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 848 ms, total: 2min 47s\n",
      "Wall time: 9min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,\n",
    "                                          name='GB_CharLevel_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>XGBoost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 36s, sys: 2.61 s, total: 13min 39s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        # run on CPU\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 52s, sys: 1.23 s, total: 11min 53s\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist', n_estimators=1000, subsample=0.8), x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8),x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 35s, sys: 779 ms, total: 5min 36s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 24s, sys: 5.12 s, total: 44min 29s\n",
      "Wall time: 18min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.148857</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.193882</td>\n",
       "      <td>0.1851</td>\n",
       "      <td>0.0552094</td>\n",
       "      <td>0.149523</td>\n",
       "      <td>0.0497097</td>\n",
       "      <td>0.139927</td>\n",
       "      <td>0.179873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.714573</td>\n",
       "      <td>0.750396</td>\n",
       "      <td>711</td>\n",
       "      <td>985</td>\n",
       "      <td>189</td>\n",
       "      <td>284</td>\n",
       "      <td>0.557644</td>\n",
       "      <td>0.559858</td>\n",
       "      <td>0.776792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0427721</td>\n",
       "      <td>0.040756</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.027494</td>\n",
       "      <td>0.0396444</td>\n",
       "      <td>0.0065706</td>\n",
       "      <td>0.0290551</td>\n",
       "      <td>0.0328178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822157</td>\n",
       "      <td>0.566834</td>\n",
       "      <td>0.671029</td>\n",
       "      <td>564</td>\n",
       "      <td>1052</td>\n",
       "      <td>122</td>\n",
       "      <td>431</td>\n",
       "      <td>0.474141</td>\n",
       "      <td>0.496038</td>\n",
       "      <td>0.731458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.0164397</td>\n",
       "      <td>0.013272</td>\n",
       "      <td>0.0159199</td>\n",
       "      <td>0.00968599</td>\n",
       "      <td>0.0153917</td>\n",
       "      <td>0.00393648</td>\n",
       "      <td>0.0325251</td>\n",
       "      <td>0.0329893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728169</td>\n",
       "      <td>0.519598</td>\n",
       "      <td>0.606452</td>\n",
       "      <td>517</td>\n",
       "      <td>981</td>\n",
       "      <td>193</td>\n",
       "      <td>478</td>\n",
       "      <td>0.363132</td>\n",
       "      <td>0.377194</td>\n",
       "      <td>0.677601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.356506</td>\n",
       "      <td>0.354018</td>\n",
       "      <td>0.329918</td>\n",
       "      <td>0.334819</td>\n",
       "      <td>0.168894</td>\n",
       "      <td>0.308831</td>\n",
       "      <td>0.070735</td>\n",
       "      <td>0.078177</td>\n",
       "      <td>0.0425143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.104523</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>104</td>\n",
       "      <td>1153</td>\n",
       "      <td>21</td>\n",
       "      <td>891</td>\n",
       "      <td>0.0928224</td>\n",
       "      <td>0.185244</td>\n",
       "      <td>0.543318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>24.231</td>\n",
       "      <td>24.7313</td>\n",
       "      <td>24.0905</td>\n",
       "      <td>24.4714</td>\n",
       "      <td>8.17439</td>\n",
       "      <td>21.1397</td>\n",
       "      <td>6.48633</td>\n",
       "      <td>0.0406101</td>\n",
       "      <td>0.0223329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739604</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.745137</td>\n",
       "      <td>747</td>\n",
       "      <td>911</td>\n",
       "      <td>263</td>\n",
       "      <td>248</td>\n",
       "      <td>0.526129</td>\n",
       "      <td>0.52618</td>\n",
       "      <td>0.763367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.600737</td>\n",
       "      <td>0.623403</td>\n",
       "      <td>0.601045</td>\n",
       "      <td>0.430498</td>\n",
       "      <td>0.346689</td>\n",
       "      <td>0.520474</td>\n",
       "      <td>0.111198</td>\n",
       "      <td>0.0434902</td>\n",
       "      <td>0.0351679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769763</td>\n",
       "      <td>0.782915</td>\n",
       "      <td>0.776283</td>\n",
       "      <td>779</td>\n",
       "      <td>941</td>\n",
       "      <td>233</td>\n",
       "      <td>216</td>\n",
       "      <td>0.583688</td>\n",
       "      <td>0.58376</td>\n",
       "      <td>0.792224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.36437</td>\n",
       "      <td>0.514269</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.492275</td>\n",
       "      <td>0.293075</td>\n",
       "      <td>0.408759</td>\n",
       "      <td>0.0828236</td>\n",
       "      <td>0.0358307</td>\n",
       "      <td>0.0250649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698768</td>\n",
       "      <td>0.627136</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>624</td>\n",
       "      <td>905</td>\n",
       "      <td>269</td>\n",
       "      <td>371</td>\n",
       "      <td>0.401139</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>0.699002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>2.98576</td>\n",
       "      <td>3.37208</td>\n",
       "      <td>4.0606</td>\n",
       "      <td>4.068</td>\n",
       "      <td>2.36705</td>\n",
       "      <td>3.3707</td>\n",
       "      <td>0.650797</td>\n",
       "      <td>0.0413959</td>\n",
       "      <td>0.0436339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755342</td>\n",
       "      <td>0.710553</td>\n",
       "      <td>0.732263</td>\n",
       "      <td>707</td>\n",
       "      <td>945</td>\n",
       "      <td>229</td>\n",
       "      <td>288</td>\n",
       "      <td>0.517834</td>\n",
       "      <td>0.51862</td>\n",
       "      <td>0.757747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>123.176</td>\n",
       "      <td>123.122</td>\n",
       "      <td>123.839</td>\n",
       "      <td>124.329</td>\n",
       "      <td>71.9894</td>\n",
       "      <td>113.291</td>\n",
       "      <td>20.6557</td>\n",
       "      <td>38.2718</td>\n",
       "      <td>38.1412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751961</td>\n",
       "      <td>0.770854</td>\n",
       "      <td>0.76129</td>\n",
       "      <td>767</td>\n",
       "      <td>921</td>\n",
       "      <td>253</td>\n",
       "      <td>228</td>\n",
       "      <td>0.55429</td>\n",
       "      <td>0.554439</td>\n",
       "      <td>0.777676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>124.359</td>\n",
       "      <td>125.554</td>\n",
       "      <td>125.471</td>\n",
       "      <td>125.911</td>\n",
       "      <td>75.9015</td>\n",
       "      <td>115.439</td>\n",
       "      <td>19.7757</td>\n",
       "      <td>33.7508</td>\n",
       "      <td>33.8758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.78794</td>\n",
       "      <td>0.774704</td>\n",
       "      <td>784</td>\n",
       "      <td>929</td>\n",
       "      <td>245</td>\n",
       "      <td>211</td>\n",
       "      <td>0.577746</td>\n",
       "      <td>0.578033</td>\n",
       "      <td>0.789626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>50.908</td>\n",
       "      <td>50.6797</td>\n",
       "      <td>50.6103</td>\n",
       "      <td>50.7885</td>\n",
       "      <td>31.0052</td>\n",
       "      <td>46.7983</td>\n",
       "      <td>7.89721</td>\n",
       "      <td>14.5457</td>\n",
       "      <td>14.561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683502</td>\n",
       "      <td>0.61206</td>\n",
       "      <td>0.645811</td>\n",
       "      <td>609</td>\n",
       "      <td>892</td>\n",
       "      <td>282</td>\n",
       "      <td>386</td>\n",
       "      <td>0.374843</td>\n",
       "      <td>0.376631</td>\n",
       "      <td>0.685928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>462.087</td>\n",
       "      <td>461.296</td>\n",
       "      <td>464.893</td>\n",
       "      <td>465.791</td>\n",
       "      <td>276.889</td>\n",
       "      <td>426.191</td>\n",
       "      <td>74.6702</td>\n",
       "      <td>119.158</td>\n",
       "      <td>119.202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761284</td>\n",
       "      <td>0.762814</td>\n",
       "      <td>0.762048</td>\n",
       "      <td>759</td>\n",
       "      <td>936</td>\n",
       "      <td>238</td>\n",
       "      <td>236</td>\n",
       "      <td>0.560003</td>\n",
       "      <td>0.560003</td>\n",
       "      <td>0.780044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.0736361</td>\n",
       "      <td>0.075166</td>\n",
       "      <td>0.069994</td>\n",
       "      <td>0.0703878</td>\n",
       "      <td>0.0307064</td>\n",
       "      <td>0.0639781</td>\n",
       "      <td>0.0167495</td>\n",
       "      <td>3.60133</td>\n",
       "      <td>3.53605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594229</td>\n",
       "      <td>0.662312</td>\n",
       "      <td>0.626426</td>\n",
       "      <td>659</td>\n",
       "      <td>724</td>\n",
       "      <td>450</td>\n",
       "      <td>336</td>\n",
       "      <td>0.276591</td>\n",
       "      <td>0.278126</td>\n",
       "      <td>0.639503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.0274119</td>\n",
       "      <td>0.030735</td>\n",
       "      <td>0.033596</td>\n",
       "      <td>0.0391703</td>\n",
       "      <td>0.0286672</td>\n",
       "      <td>0.0319161</td>\n",
       "      <td>0.00418767</td>\n",
       "      <td>3.46543</td>\n",
       "      <td>3.45874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.080402</td>\n",
       "      <td>0.14733</td>\n",
       "      <td>80</td>\n",
       "      <td>1163</td>\n",
       "      <td>11</td>\n",
       "      <td>915</td>\n",
       "      <td>0.0763184</td>\n",
       "      <td>0.176546</td>\n",
       "      <td>0.535516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.0128708</td>\n",
       "      <td>0.0140781</td>\n",
       "      <td>0.0176237</td>\n",
       "      <td>0.01595</td>\n",
       "      <td>0.0143311</td>\n",
       "      <td>0.00227536</td>\n",
       "      <td>1.10888</td>\n",
       "      <td>1.16158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.563725</td>\n",
       "      <td>575</td>\n",
       "      <td>704</td>\n",
       "      <td>470</td>\n",
       "      <td>420</td>\n",
       "      <td>0.176871</td>\n",
       "      <td>0.177061</td>\n",
       "      <td>0.588774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.29471</td>\n",
       "      <td>0.293625</td>\n",
       "      <td>0.278665</td>\n",
       "      <td>0.273606</td>\n",
       "      <td>0.157102</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>0.0518767</td>\n",
       "      <td>33.1459</td>\n",
       "      <td>33.2813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741803</td>\n",
       "      <td>0.363819</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>362</td>\n",
       "      <td>1048</td>\n",
       "      <td>126</td>\n",
       "      <td>633</td>\n",
       "      <td>0.26686</td>\n",
       "      <td>0.306075</td>\n",
       "      <td>0.628247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>21.7515</td>\n",
       "      <td>22.2565</td>\n",
       "      <td>21.9428</td>\n",
       "      <td>22.1224</td>\n",
       "      <td>5.51077</td>\n",
       "      <td>18.7168</td>\n",
       "      <td>6.6052</td>\n",
       "      <td>0.327522</td>\n",
       "      <td>0.284147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790952</td>\n",
       "      <td>0.509548</td>\n",
       "      <td>0.619804</td>\n",
       "      <td>507</td>\n",
       "      <td>1040</td>\n",
       "      <td>134</td>\n",
       "      <td>488</td>\n",
       "      <td>0.406432</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.697704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>17.0595</td>\n",
       "      <td>17.5062</td>\n",
       "      <td>17.3772</td>\n",
       "      <td>17.6241</td>\n",
       "      <td>4.61598</td>\n",
       "      <td>14.8366</td>\n",
       "      <td>5.11378</td>\n",
       "      <td>0.12819</td>\n",
       "      <td>0.141058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768435</td>\n",
       "      <td>0.596985</td>\n",
       "      <td>0.671946</td>\n",
       "      <td>594</td>\n",
       "      <td>995</td>\n",
       "      <td>179</td>\n",
       "      <td>401</td>\n",
       "      <td>0.452206</td>\n",
       "      <td>0.462487</td>\n",
       "      <td>0.722257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>12.0346</td>\n",
       "      <td>12.0484</td>\n",
       "      <td>11.949</td>\n",
       "      <td>12.1523</td>\n",
       "      <td>2.92217</td>\n",
       "      <td>10.2213</td>\n",
       "      <td>3.65014</td>\n",
       "      <td>0.125397</td>\n",
       "      <td>0.176219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.538693</td>\n",
       "      <td>0.588042</td>\n",
       "      <td>536</td>\n",
       "      <td>882</td>\n",
       "      <td>292</td>\n",
       "      <td>459</td>\n",
       "      <td>0.293729</td>\n",
       "      <td>0.29742</td>\n",
       "      <td>0.644986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>43.698</td>\n",
       "      <td>44.3365</td>\n",
       "      <td>43.9283</td>\n",
       "      <td>44.3347</td>\n",
       "      <td>11.261</td>\n",
       "      <td>37.5117</td>\n",
       "      <td>13.1276</td>\n",
       "      <td>0.633405</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720418</td>\n",
       "      <td>0.624121</td>\n",
       "      <td>0.668821</td>\n",
       "      <td>621</td>\n",
       "      <td>933</td>\n",
       "      <td>241</td>\n",
       "      <td>374</td>\n",
       "      <td>0.423152</td>\n",
       "      <td>0.426483</td>\n",
       "      <td>0.70942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.38897</td>\n",
       "      <td>0.401629</td>\n",
       "      <td>0.392322</td>\n",
       "      <td>0.441335</td>\n",
       "      <td>0.229587</td>\n",
       "      <td>0.370768</td>\n",
       "      <td>0.0730206</td>\n",
       "      <td>0.0455689</td>\n",
       "      <td>0.0481122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710501</td>\n",
       "      <td>0.754774</td>\n",
       "      <td>0.731969</td>\n",
       "      <td>751</td>\n",
       "      <td>868</td>\n",
       "      <td>306</td>\n",
       "      <td>244</td>\n",
       "      <td>0.49179</td>\n",
       "      <td>0.492599</td>\n",
       "      <td>0.747063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.226678</td>\n",
       "      <td>0.185636</td>\n",
       "      <td>0.253725</td>\n",
       "      <td>0.202837</td>\n",
       "      <td>0.176574</td>\n",
       "      <td>0.20909</td>\n",
       "      <td>0.0281001</td>\n",
       "      <td>0.0369208</td>\n",
       "      <td>0.0361989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718864</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.752276</td>\n",
       "      <td>785</td>\n",
       "      <td>867</td>\n",
       "      <td>307</td>\n",
       "      <td>210</td>\n",
       "      <td>0.523555</td>\n",
       "      <td>0.525659</td>\n",
       "      <td>0.763723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.118484</td>\n",
       "      <td>0.099462</td>\n",
       "      <td>0.125143</td>\n",
       "      <td>0.114368</td>\n",
       "      <td>0.0859709</td>\n",
       "      <td>0.108686</td>\n",
       "      <td>0.0141434</td>\n",
       "      <td>0.0399122</td>\n",
       "      <td>0.0480962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630413</td>\n",
       "      <td>0.629146</td>\n",
       "      <td>0.629779</td>\n",
       "      <td>626</td>\n",
       "      <td>807</td>\n",
       "      <td>367</td>\n",
       "      <td>369</td>\n",
       "      <td>0.316588</td>\n",
       "      <td>0.316588</td>\n",
       "      <td>0.65827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.07339</td>\n",
       "      <td>1.09698</td>\n",
       "      <td>0.995801</td>\n",
       "      <td>1.04763</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>0.957619</td>\n",
       "      <td>0.194585</td>\n",
       "      <td>0.0397961</td>\n",
       "      <td>0.041182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768025</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>735</td>\n",
       "      <td>952</td>\n",
       "      <td>222</td>\n",
       "      <td>260</td>\n",
       "      <td>0.551201</td>\n",
       "      <td>0.551547</td>\n",
       "      <td>0.774798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>62.8157</td>\n",
       "      <td>47.7324</td>\n",
       "      <td>57.8882</td>\n",
       "      <td>45.7788</td>\n",
       "      <td>27.6093</td>\n",
       "      <td>48.3649</td>\n",
       "      <td>12.1441</td>\n",
       "      <td>0.030396</td>\n",
       "      <td>0.0486929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699588</td>\n",
       "      <td>0.683417</td>\n",
       "      <td>0.691408</td>\n",
       "      <td>680</td>\n",
       "      <td>882</td>\n",
       "      <td>292</td>\n",
       "      <td>315</td>\n",
       "      <td>0.435462</td>\n",
       "      <td>0.435562</td>\n",
       "      <td>0.717347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>56.4446</td>\n",
       "      <td>58.6827</td>\n",
       "      <td>55.7383</td>\n",
       "      <td>52.9696</td>\n",
       "      <td>32.0016</td>\n",
       "      <td>51.1674</td>\n",
       "      <td>9.75499</td>\n",
       "      <td>0.0371299</td>\n",
       "      <td>0.0266747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699399</td>\n",
       "      <td>0.701508</td>\n",
       "      <td>0.700452</td>\n",
       "      <td>698</td>\n",
       "      <td>874</td>\n",
       "      <td>300</td>\n",
       "      <td>297</td>\n",
       "      <td>0.445868</td>\n",
       "      <td>0.44587</td>\n",
       "      <td>0.722985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>7.88613</td>\n",
       "      <td>7.28621</td>\n",
       "      <td>8.7487</td>\n",
       "      <td>8.96308</td>\n",
       "      <td>4.3683</td>\n",
       "      <td>7.45049</td>\n",
       "      <td>1.65469</td>\n",
       "      <td>0.0449111</td>\n",
       "      <td>0.0386229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.529648</td>\n",
       "      <td>0.561534</td>\n",
       "      <td>527</td>\n",
       "      <td>819</td>\n",
       "      <td>355</td>\n",
       "      <td>468</td>\n",
       "      <td>0.229248</td>\n",
       "      <td>0.230543</td>\n",
       "      <td>0.613632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>290.358</td>\n",
       "      <td>264.686</td>\n",
       "      <td>269.63</td>\n",
       "      <td>246.244</td>\n",
       "      <td>150.698</td>\n",
       "      <td>244.323</td>\n",
       "      <td>48.8748</td>\n",
       "      <td>0.048856</td>\n",
       "      <td>0.101486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730313</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.704475</td>\n",
       "      <td>677</td>\n",
       "      <td>924</td>\n",
       "      <td>250</td>\n",
       "      <td>318</td>\n",
       "      <td>0.469903</td>\n",
       "      <td>0.470852</td>\n",
       "      <td>0.733727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>46.2352</td>\n",
       "      <td>39.3515</td>\n",
       "      <td>44.8149</td>\n",
       "      <td>25.6293</td>\n",
       "      <td>21.204</td>\n",
       "      <td>35.447</td>\n",
       "      <td>10.1847</td>\n",
       "      <td>0.368251</td>\n",
       "      <td>0.531139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740519</td>\n",
       "      <td>0.745729</td>\n",
       "      <td>0.743115</td>\n",
       "      <td>742</td>\n",
       "      <td>914</td>\n",
       "      <td>260</td>\n",
       "      <td>253</td>\n",
       "      <td>0.523983</td>\n",
       "      <td>0.523994</td>\n",
       "      <td>0.762132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>46.2841</td>\n",
       "      <td>33.094</td>\n",
       "      <td>58.4231</td>\n",
       "      <td>58.8243</td>\n",
       "      <td>38.8453</td>\n",
       "      <td>47.0942</td>\n",
       "      <td>10.3019</td>\n",
       "      <td>0.375354</td>\n",
       "      <td>0.398289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754065</td>\n",
       "      <td>0.745729</td>\n",
       "      <td>0.749874</td>\n",
       "      <td>742</td>\n",
       "      <td>932</td>\n",
       "      <td>242</td>\n",
       "      <td>253</td>\n",
       "      <td>0.540051</td>\n",
       "      <td>0.540079</td>\n",
       "      <td>0.769798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>18.5131</td>\n",
       "      <td>13.2173</td>\n",
       "      <td>11.4383</td>\n",
       "      <td>11.3983</td>\n",
       "      <td>12.9196</td>\n",
       "      <td>13.4973</td>\n",
       "      <td>2.61595</td>\n",
       "      <td>0.0879488</td>\n",
       "      <td>0.137267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631635</td>\n",
       "      <td>0.59799</td>\n",
       "      <td>0.614352</td>\n",
       "      <td>595</td>\n",
       "      <td>827</td>\n",
       "      <td>347</td>\n",
       "      <td>400</td>\n",
       "      <td>0.303652</td>\n",
       "      <td>0.304024</td>\n",
       "      <td>0.65121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>299.463</td>\n",
       "      <td>166.304</td>\n",
       "      <td>286.804</td>\n",
       "      <td>204.923</td>\n",
       "      <td>232.582</td>\n",
       "      <td>238.015</td>\n",
       "      <td>49.8462</td>\n",
       "      <td>0.883031</td>\n",
       "      <td>1.74164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747673</td>\n",
       "      <td>0.726633</td>\n",
       "      <td>0.737003</td>\n",
       "      <td>723</td>\n",
       "      <td>930</td>\n",
       "      <td>244</td>\n",
       "      <td>272</td>\n",
       "      <td>0.519912</td>\n",
       "      <td>0.520089</td>\n",
       "      <td>0.759398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.148857     0.164567     0.193882       0.1851   \n",
       "0    NB_WordLevel_TF-IDF    0.0427721     0.040756       0.0472         0.04   \n",
       "0       NB_N-Gram_TF-IDF     0.021641    0.0164397     0.013272    0.0159199   \n",
       "0    NB_CharLevel_TF-IDF     0.356506     0.354018     0.329918     0.334819   \n",
       "0       LR_Count_Vectors       24.231      24.7313      24.0905      24.4714   \n",
       "0    LR_WordLevel_TF-IDF     0.600737     0.623403     0.601045     0.430498   \n",
       "0       LR_N-Gram_TF-IDF      0.36437     0.514269     0.379806     0.492275   \n",
       "0    LR_CharLevel_TF-IDF      2.98576      3.37208       4.0606        4.068   \n",
       "0      SVM_Count_Vectors      123.176      123.122      123.839      124.329   \n",
       "0   SVM_WordLevel_TF-IDF      124.359      125.554      125.471      125.911   \n",
       "0      SVM_N-Gram_TF-IDF       50.908      50.6797      50.6103      50.7885   \n",
       "0   SVM_CharLevel_TF-IDF      462.087      461.296      464.893      465.791   \n",
       "0      kNN_Count_Vectors    0.0736361     0.075166     0.069994    0.0703878   \n",
       "0   kNN_WordLevel_TF-IDF    0.0274119     0.030735     0.033596    0.0391703   \n",
       "0      kNN_N-Gram_TF-IDF     0.011133    0.0128708    0.0140781    0.0176237   \n",
       "0   kNN_CharLevel_TF-IDF      0.29471     0.293625     0.278665     0.273606   \n",
       "0       RF_Count_Vectors      21.7515      22.2565      21.9428      22.1224   \n",
       "0    RF_WordLevel_TF-IDF      17.0595      17.5062      17.3772      17.6241   \n",
       "0       RF_N-Gram_TF-IDF      12.0346      12.0484       11.949      12.1523   \n",
       "0    RF_CharLevel_TF-IDF       43.698      44.3365      43.9283      44.3347   \n",
       "0      SGD_Count_Vectors      0.38897     0.401629     0.392322     0.441335   \n",
       "0   SGD_WordLevel_TF-IDF     0.226678     0.185636     0.253725     0.202837   \n",
       "0     SGD_N-Gram_Vectors     0.118484     0.099462     0.125143     0.114368   \n",
       "0  SGD_CharLevel_Vectors      1.07339      1.09698     0.995801      1.04763   \n",
       "0       GB_Count_Vectors      62.8157      47.7324      57.8882      45.7788   \n",
       "0    GB_WordLevel_TF-IDF      56.4446      58.6827      55.7383      52.9696   \n",
       "0       GB_N-Gram_TF-IDF      7.88613      7.28621       8.7487      8.96308   \n",
       "0    GB_CharLevel_TF-IDF      290.358      264.686       269.63      246.244   \n",
       "0      XGB_Count_Vectors      46.2352      39.3515      44.8149      25.6293   \n",
       "0   XGB_WordLevel_TF-IDF      46.2841       33.094      58.4231      58.8243   \n",
       "0      XGB_N-Gram_TF-IDF      18.5131      13.2173      11.4383      11.3983   \n",
       "0   XGB_CharLevel_TF-IDF      299.463      166.304      286.804      204.923   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0552094      0.149523    0.0497097       0.139927       0.179873  ...   \n",
       "0     0.027494     0.0396444    0.0065706      0.0290551      0.0328178  ...   \n",
       "0   0.00968599     0.0153917   0.00393648      0.0325251      0.0329893  ...   \n",
       "0     0.168894      0.308831     0.070735       0.078177      0.0425143  ...   \n",
       "0      8.17439       21.1397      6.48633      0.0406101      0.0223329  ...   \n",
       "0     0.346689      0.520474     0.111198      0.0434902      0.0351679  ...   \n",
       "0     0.293075      0.408759    0.0828236      0.0358307      0.0250649  ...   \n",
       "0      2.36705        3.3707     0.650797      0.0413959      0.0436339  ...   \n",
       "0      71.9894       113.291      20.6557        38.2718        38.1412  ...   \n",
       "0      75.9015       115.439      19.7757        33.7508        33.8758  ...   \n",
       "0      31.0052       46.7983      7.89721        14.5457         14.561  ...   \n",
       "0      276.889       426.191      74.6702        119.158        119.202  ...   \n",
       "0    0.0307064     0.0639781    0.0167495        3.60133        3.53605  ...   \n",
       "0    0.0286672     0.0319161   0.00418767        3.46543        3.45874  ...   \n",
       "0      0.01595     0.0143311   0.00227536        1.10888        1.16158  ...   \n",
       "0     0.157102      0.259542    0.0518767        33.1459        33.2813  ...   \n",
       "0      5.51077       18.7168       6.6052       0.327522       0.284147  ...   \n",
       "0      4.61598       14.8366      5.11378        0.12819       0.141058  ...   \n",
       "0      2.92217       10.2213      3.65014       0.125397       0.176219  ...   \n",
       "0       11.261       37.5117      13.1276       0.633405       0.627753  ...   \n",
       "0     0.229587      0.370768    0.0730206      0.0455689      0.0481122  ...   \n",
       "0     0.176574       0.20909    0.0281001      0.0369208      0.0361989  ...   \n",
       "0    0.0859709      0.108686    0.0141434      0.0399122      0.0480962  ...   \n",
       "0       0.5743      0.957619     0.194585      0.0397961       0.041182  ...   \n",
       "0      27.6093       48.3649      12.1441       0.030396      0.0486929  ...   \n",
       "0      32.0016       51.1674      9.75499      0.0371299      0.0266747  ...   \n",
       "0       4.3683       7.45049      1.65469      0.0449111      0.0386229  ...   \n",
       "0      150.698       244.323      48.8748       0.048856       0.101486  ...   \n",
       "0       21.204        35.447      10.1847       0.368251       0.531139  ...   \n",
       "0      38.8453       47.0942      10.3019       0.375354       0.398289  ...   \n",
       "0      12.9196       13.4973      2.61595      0.0879488       0.137267  ...   \n",
       "0      232.582       238.015      49.8462       0.883031        1.74164  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0         0.79       0.714573         0.750396        711        985   \n",
       "0     0.822157       0.566834         0.671029        564       1052   \n",
       "0     0.728169       0.519598         0.606452        517        981   \n",
       "0        0.832       0.104523         0.185714        104       1153   \n",
       "0     0.739604       0.750754         0.745137        747        911   \n",
       "0     0.769763       0.782915         0.776283        779        941   \n",
       "0     0.698768       0.627136         0.661017        624        905   \n",
       "0     0.755342       0.710553         0.732263        707        945   \n",
       "0     0.751961       0.770854          0.76129        767        921   \n",
       "0     0.761905        0.78794         0.774704        784        929   \n",
       "0     0.683502        0.61206         0.645811        609        892   \n",
       "0     0.761284       0.762814         0.762048        759        936   \n",
       "0     0.594229       0.662312         0.626426        659        724   \n",
       "0     0.879121       0.080402          0.14733         80       1163   \n",
       "0     0.550239       0.577889         0.563725        575        704   \n",
       "0     0.741803       0.363819           0.4882        362       1048   \n",
       "0     0.790952       0.509548         0.619804        507       1040   \n",
       "0     0.768435       0.596985         0.671946        594        995   \n",
       "0     0.647343       0.538693         0.588042        536        882   \n",
       "0     0.720418       0.624121         0.668821        621        933   \n",
       "0     0.710501       0.754774         0.731969        751        868   \n",
       "0     0.718864       0.788945         0.752276        785        867   \n",
       "0     0.630413       0.629146         0.629779        626        807   \n",
       "0     0.768025       0.738693         0.753074        735        952   \n",
       "0     0.699588       0.683417         0.691408        680        882   \n",
       "0     0.699399       0.701508         0.700452        698        874   \n",
       "0     0.597506       0.529648         0.561534        527        819   \n",
       "0     0.730313       0.680402         0.704475        677        924   \n",
       "0     0.740519       0.745729         0.743115        742        914   \n",
       "0     0.754065       0.745729         0.749874        742        932   \n",
       "0     0.631635        0.59799         0.614352        595        827   \n",
       "0     0.747673       0.726633         0.737003        723        930   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        189        284             0.557644                  0.559858   \n",
       "0        122        431             0.474141                  0.496038   \n",
       "0        193        478             0.363132                  0.377194   \n",
       "0         21        891            0.0928224                  0.185244   \n",
       "0        263        248             0.526129                   0.52618   \n",
       "0        233        216             0.583688                   0.58376   \n",
       "0        269        371             0.401139                  0.402979   \n",
       "0        229        288             0.517834                   0.51862   \n",
       "0        253        228              0.55429                  0.554439   \n",
       "0        245        211             0.577746                  0.578033   \n",
       "0        282        386             0.374843                  0.376631   \n",
       "0        238        236             0.560003                  0.560003   \n",
       "0        450        336             0.276591                  0.278126   \n",
       "0         11        915            0.0763184                  0.176546   \n",
       "0        470        420             0.176871                  0.177061   \n",
       "0        126        633              0.26686                  0.306075   \n",
       "0        134        488             0.406432                  0.431818   \n",
       "0        179        401             0.452206                  0.462487   \n",
       "0        292        459             0.293729                   0.29742   \n",
       "0        241        374             0.423152                  0.426483   \n",
       "0        306        244              0.49179                  0.492599   \n",
       "0        307        210             0.523555                  0.525659   \n",
       "0        367        369             0.316588                  0.316588   \n",
       "0        222        260             0.551201                  0.551547   \n",
       "0        292        315             0.435462                  0.435562   \n",
       "0        300        297             0.445868                   0.44587   \n",
       "0        355        468             0.229248                  0.230543   \n",
       "0        250        318             0.469903                  0.470852   \n",
       "0        260        253             0.523983                  0.523994   \n",
       "0        242        253             0.540051                  0.540079   \n",
       "0        347        400             0.303652                  0.304024   \n",
       "0        244        272             0.519912                  0.520089   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0        0.776792  \n",
       "0        0.731458  \n",
       "0        0.677601  \n",
       "0        0.543318  \n",
       "0        0.763367  \n",
       "0        0.792224  \n",
       "0        0.699002  \n",
       "0        0.757747  \n",
       "0        0.777676  \n",
       "0        0.789626  \n",
       "0        0.685928  \n",
       "0        0.780044  \n",
       "0        0.639503  \n",
       "0        0.535516  \n",
       "0        0.588774  \n",
       "0        0.628247  \n",
       "0        0.697704  \n",
       "0        0.722257  \n",
       "0        0.644986  \n",
       "0         0.70942  \n",
       "0        0.747063  \n",
       "0        0.763723  \n",
       "0         0.65827  \n",
       "0        0.774798  \n",
       "0        0.717347  \n",
       "0        0.722985  \n",
       "0        0.613632  \n",
       "0        0.733727  \n",
       "0        0.762132  \n",
       "0        0.769798  \n",
       "0         0.65121  \n",
       "0        0.759398  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Catboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if catboost_classifier:\n",
    "    # work in progress\n",
    "    if num_gpu>0:  # test gpu available\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "    else:\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Adaboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if adaboost_classifier:\n",
    "    # work in progress\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_count, y_train, x_test_count, y_test, name='Adaboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Adaboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Adaboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Adaboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lgbm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>LightGBM</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if lightgbm_classifier:\n",
    "    \n",
    "    # work in progress\n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.083657</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>0.0876408</td>\n",
       "      <td>0.0806589</td>\n",
       "      <td>0.0848186</td>\n",
       "      <td>0.00268425</td>\n",
       "      <td>0.0557067</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791759</td>\n",
       "      <td>0.714573</td>\n",
       "      <td>0.751189</td>\n",
       "      <td>711</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>284</td>\n",
       "      <td>0.559446</td>\n",
       "      <td>0.561763</td>\n",
       "      <td>0.777644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0570619</td>\n",
       "      <td>0.0624511</td>\n",
       "      <td>0.0512819</td>\n",
       "      <td>0.0743842</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.0559588</td>\n",
       "      <td>0.0131171</td>\n",
       "      <td>0.0463171</td>\n",
       "      <td>0.0433159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.58191</td>\n",
       "      <td>0.681176</td>\n",
       "      <td>579</td>\n",
       "      <td>1048</td>\n",
       "      <td>126</td>\n",
       "      <td>416</td>\n",
       "      <td>0.485369</td>\n",
       "      <td>0.504886</td>\n",
       "      <td>0.737292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.0146081</td>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.0386279</td>\n",
       "      <td>0.0167282</td>\n",
       "      <td>0.0169821</td>\n",
       "      <td>0.0236558</td>\n",
       "      <td>0.00956548</td>\n",
       "      <td>0.0413001</td>\n",
       "      <td>0.0396559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730563</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>0.626077</td>\n",
       "      <td>545</td>\n",
       "      <td>973</td>\n",
       "      <td>201</td>\n",
       "      <td>450</td>\n",
       "      <td>0.383852</td>\n",
       "      <td>0.394977</td>\n",
       "      <td>0.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.849732</td>\n",
       "      <td>0.797022</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.13063</td>\n",
       "      <td>0.286705</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.303969</td>\n",
       "      <td>0.125775</td>\n",
       "      <td>0.0687079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.188948</td>\n",
       "      <td>106</td>\n",
       "      <td>1153</td>\n",
       "      <td>21</td>\n",
       "      <td>889</td>\n",
       "      <td>0.0949604</td>\n",
       "      <td>0.188135</td>\n",
       "      <td>0.544323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>18.4121</td>\n",
       "      <td>19.1657</td>\n",
       "      <td>17.922</td>\n",
       "      <td>19.1001</td>\n",
       "      <td>9.19119</td>\n",
       "      <td>16.7582</td>\n",
       "      <td>3.81129</td>\n",
       "      <td>0.0704119</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738872</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.744766</td>\n",
       "      <td>747</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>248</td>\n",
       "      <td>0.525238</td>\n",
       "      <td>0.525295</td>\n",
       "      <td>0.762941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.667456</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.61599</td>\n",
       "      <td>0.690176</td>\n",
       "      <td>0.490711</td>\n",
       "      <td>0.626685</td>\n",
       "      <td>0.0722426</td>\n",
       "      <td>0.0417788</td>\n",
       "      <td>0.0403888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771372</td>\n",
       "      <td>0.779899</td>\n",
       "      <td>0.775612</td>\n",
       "      <td>776</td>\n",
       "      <td>944</td>\n",
       "      <td>230</td>\n",
       "      <td>219</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>0.583527</td>\n",
       "      <td>0.791994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.495503</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.355603</td>\n",
       "      <td>0.409944</td>\n",
       "      <td>0.34902</td>\n",
       "      <td>0.407818</td>\n",
       "      <td>0.0535282</td>\n",
       "      <td>0.0507712</td>\n",
       "      <td>0.0475352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701226</td>\n",
       "      <td>0.632161</td>\n",
       "      <td>0.664905</td>\n",
       "      <td>629</td>\n",
       "      <td>906</td>\n",
       "      <td>268</td>\n",
       "      <td>366</td>\n",
       "      <td>0.406937</td>\n",
       "      <td>0.408658</td>\n",
       "      <td>0.701941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>4.52604</td>\n",
       "      <td>5.30917</td>\n",
       "      <td>5.46984</td>\n",
       "      <td>5.11398</td>\n",
       "      <td>3.58113</td>\n",
       "      <td>4.80003</td>\n",
       "      <td>0.68807</td>\n",
       "      <td>0.07074</td>\n",
       "      <td>0.0679901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.730192</td>\n",
       "      <td>705</td>\n",
       "      <td>943</td>\n",
       "      <td>231</td>\n",
       "      <td>290</td>\n",
       "      <td>0.514104</td>\n",
       "      <td>0.514884</td>\n",
       "      <td>0.75589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>142.24</td>\n",
       "      <td>143.032</td>\n",
       "      <td>143.314</td>\n",
       "      <td>144.206</td>\n",
       "      <td>113.892</td>\n",
       "      <td>137.337</td>\n",
       "      <td>11.7393</td>\n",
       "      <td>45.6131</td>\n",
       "      <td>45.6117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750984</td>\n",
       "      <td>0.766834</td>\n",
       "      <td>0.758826</td>\n",
       "      <td>763</td>\n",
       "      <td>921</td>\n",
       "      <td>253</td>\n",
       "      <td>232</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.55055</td>\n",
       "      <td>0.775666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>171.507</td>\n",
       "      <td>164.749</td>\n",
       "      <td>171.223</td>\n",
       "      <td>171.68</td>\n",
       "      <td>118.665</td>\n",
       "      <td>159.565</td>\n",
       "      <td>20.6152</td>\n",
       "      <td>51.6323</td>\n",
       "      <td>49.8943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760194</td>\n",
       "      <td>0.786935</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>783</td>\n",
       "      <td>927</td>\n",
       "      <td>247</td>\n",
       "      <td>212</td>\n",
       "      <td>0.575001</td>\n",
       "      <td>0.575303</td>\n",
       "      <td>0.788271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>67.1528</td>\n",
       "      <td>65.9447</td>\n",
       "      <td>66.7256</td>\n",
       "      <td>65.973</td>\n",
       "      <td>32.62</td>\n",
       "      <td>59.6832</td>\n",
       "      <td>13.5394</td>\n",
       "      <td>24.1372</td>\n",
       "      <td>25.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.621106</td>\n",
       "      <td>0.658147</td>\n",
       "      <td>618</td>\n",
       "      <td>909</td>\n",
       "      <td>265</td>\n",
       "      <td>377</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.401015</td>\n",
       "      <td>0.697691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>567.083</td>\n",
       "      <td>577.67</td>\n",
       "      <td>578.479</td>\n",
       "      <td>579.833</td>\n",
       "      <td>336.997</td>\n",
       "      <td>528.013</td>\n",
       "      <td>95.6156</td>\n",
       "      <td>186.596</td>\n",
       "      <td>184.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>0.759799</td>\n",
       "      <td>0.758655</td>\n",
       "      <td>756</td>\n",
       "      <td>932</td>\n",
       "      <td>242</td>\n",
       "      <td>239</td>\n",
       "      <td>0.553539</td>\n",
       "      <td>0.553541</td>\n",
       "      <td>0.776833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.072355</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0731351</td>\n",
       "      <td>0.0939651</td>\n",
       "      <td>0.0768281</td>\n",
       "      <td>0.00857692</td>\n",
       "      <td>3.7683</td>\n",
       "      <td>3.65425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586263</td>\n",
       "      <td>0.720603</td>\n",
       "      <td>0.646528</td>\n",
       "      <td>717</td>\n",
       "      <td>668</td>\n",
       "      <td>506</td>\n",
       "      <td>278</td>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.290993</td>\n",
       "      <td>0.644799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.049027</td>\n",
       "      <td>0.0487049</td>\n",
       "      <td>0.051249</td>\n",
       "      <td>0.0619388</td>\n",
       "      <td>0.0523279</td>\n",
       "      <td>0.0526495</td>\n",
       "      <td>0.00483814</td>\n",
       "      <td>4.03613</td>\n",
       "      <td>3.9824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751938</td>\n",
       "      <td>0.0974874</td>\n",
       "      <td>0.172598</td>\n",
       "      <td>97</td>\n",
       "      <td>1142</td>\n",
       "      <td>32</td>\n",
       "      <td>898</td>\n",
       "      <td>0.0752212</td>\n",
       "      <td>0.147965</td>\n",
       "      <td>0.535115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.0136449</td>\n",
       "      <td>0.020715</td>\n",
       "      <td>0.0663297</td>\n",
       "      <td>0.0168662</td>\n",
       "      <td>0.0261462</td>\n",
       "      <td>0.0202725</td>\n",
       "      <td>1.33747</td>\n",
       "      <td>1.33642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584323</td>\n",
       "      <td>0.494472</td>\n",
       "      <td>0.535656</td>\n",
       "      <td>492</td>\n",
       "      <td>824</td>\n",
       "      <td>350</td>\n",
       "      <td>503</td>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.20076</td>\n",
       "      <td>0.598173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.655702</td>\n",
       "      <td>0.672649</td>\n",
       "      <td>0.65748</td>\n",
       "      <td>0.667741</td>\n",
       "      <td>0.195899</td>\n",
       "      <td>0.569894</td>\n",
       "      <td>0.187104</td>\n",
       "      <td>48.1581</td>\n",
       "      <td>48.1758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739785</td>\n",
       "      <td>0.345729</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>344</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>651</td>\n",
       "      <td>0.252933</td>\n",
       "      <td>0.294636</td>\n",
       "      <td>0.621331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.458804</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.420624</td>\n",
       "      <td>0.444952</td>\n",
       "      <td>0.266678</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.0751494</td>\n",
       "      <td>0.0419219</td>\n",
       "      <td>0.0525622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>0.872362</td>\n",
       "      <td>0.747631</td>\n",
       "      <td>868</td>\n",
       "      <td>715</td>\n",
       "      <td>459</td>\n",
       "      <td>127</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>0.492212</td>\n",
       "      <td>0.740695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>0.269268</td>\n",
       "      <td>0.277042</td>\n",
       "      <td>0.274197</td>\n",
       "      <td>0.175028</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.0459828</td>\n",
       "      <td>0.03966</td>\n",
       "      <td>0.039535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742604</td>\n",
       "      <td>0.756784</td>\n",
       "      <td>0.749627</td>\n",
       "      <td>753</td>\n",
       "      <td>913</td>\n",
       "      <td>261</td>\n",
       "      <td>242</td>\n",
       "      <td>0.53369</td>\n",
       "      <td>0.533773</td>\n",
       "      <td>0.767234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.118953</td>\n",
       "      <td>0.126627</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.105808</td>\n",
       "      <td>0.0728922</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>0.0183905</td>\n",
       "      <td>0.0420618</td>\n",
       "      <td>0.03441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.678392</td>\n",
       "      <td>0.65534</td>\n",
       "      <td>675</td>\n",
       "      <td>784</td>\n",
       "      <td>390</td>\n",
       "      <td>320</td>\n",
       "      <td>0.344348</td>\n",
       "      <td>0.345069</td>\n",
       "      <td>0.673097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.92091</td>\n",
       "      <td>1.93717</td>\n",
       "      <td>1.7542</td>\n",
       "      <td>1.82993</td>\n",
       "      <td>1.36733</td>\n",
       "      <td>1.76191</td>\n",
       "      <td>0.208027</td>\n",
       "      <td>0.0974619</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828369</td>\n",
       "      <td>0.586935</td>\n",
       "      <td>0.687059</td>\n",
       "      <td>584</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>411</td>\n",
       "      <td>0.494864</td>\n",
       "      <td>0.514763</td>\n",
       "      <td>0.741934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>62.9065</td>\n",
       "      <td>55.3991</td>\n",
       "      <td>56.9035</td>\n",
       "      <td>52.061</td>\n",
       "      <td>27.6382</td>\n",
       "      <td>50.9817</td>\n",
       "      <td>12.189</td>\n",
       "      <td>0.0316858</td>\n",
       "      <td>0.0516238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698055</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.691684</td>\n",
       "      <td>682</td>\n",
       "      <td>879</td>\n",
       "      <td>295</td>\n",
       "      <td>313</td>\n",
       "      <td>0.434749</td>\n",
       "      <td>0.43481</td>\n",
       "      <td>0.717075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>5.80552</td>\n",
       "      <td>7.31059</td>\n",
       "      <td>7.17226</td>\n",
       "      <td>6.93111</td>\n",
       "      <td>5.58752</td>\n",
       "      <td>6.5614</td>\n",
       "      <td>0.719849</td>\n",
       "      <td>0.0432222</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.566632</td>\n",
       "      <td>540</td>\n",
       "      <td>803</td>\n",
       "      <td>371</td>\n",
       "      <td>455</td>\n",
       "      <td>0.228168</td>\n",
       "      <td>0.228875</td>\n",
       "      <td>0.61335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>220.842</td>\n",
       "      <td>238.157</td>\n",
       "      <td>282.195</td>\n",
       "      <td>257.065</td>\n",
       "      <td>184.308</td>\n",
       "      <td>236.513</td>\n",
       "      <td>33.1146</td>\n",
       "      <td>0.0761411</td>\n",
       "      <td>0.0745621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719447</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.69938</td>\n",
       "      <td>677</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>318</td>\n",
       "      <td>0.457422</td>\n",
       "      <td>0.458003</td>\n",
       "      <td>0.727765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>92.4755</td>\n",
       "      <td>84.2349</td>\n",
       "      <td>73.6215</td>\n",
       "      <td>84.1027</td>\n",
       "      <td>48.617</td>\n",
       "      <td>76.6103</td>\n",
       "      <td>15.2219</td>\n",
       "      <td>0.0370519</td>\n",
       "      <td>0.0738072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>700</td>\n",
       "      <td>894</td>\n",
       "      <td>280</td>\n",
       "      <td>295</td>\n",
       "      <td>0.465552</td>\n",
       "      <td>0.465597</td>\n",
       "      <td>0.732508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>74.1953</td>\n",
       "      <td>65.7518</td>\n",
       "      <td>84.2132</td>\n",
       "      <td>65.1882</td>\n",
       "      <td>32.8804</td>\n",
       "      <td>64.4458</td>\n",
       "      <td>17.2304</td>\n",
       "      <td>0.516725</td>\n",
       "      <td>0.95696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.751759</td>\n",
       "      <td>0.748749</td>\n",
       "      <td>748</td>\n",
       "      <td>919</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>0.534225</td>\n",
       "      <td>0.53424</td>\n",
       "      <td>0.767276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>46.6946</td>\n",
       "      <td>49.373</td>\n",
       "      <td>52.0918</td>\n",
       "      <td>37.7714</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>42.1696</td>\n",
       "      <td>9.87732</td>\n",
       "      <td>0.425461</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727728</td>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.729188</td>\n",
       "      <td>727</td>\n",
       "      <td>902</td>\n",
       "      <td>272</td>\n",
       "      <td>268</td>\n",
       "      <td>0.498814</td>\n",
       "      <td>0.498817</td>\n",
       "      <td>0.749483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>14.4524</td>\n",
       "      <td>14.8459</td>\n",
       "      <td>10.1767</td>\n",
       "      <td>6.22884</td>\n",
       "      <td>11.4074</td>\n",
       "      <td>11.4223</td>\n",
       "      <td>3.14409</td>\n",
       "      <td>0.109201</td>\n",
       "      <td>0.0818152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640324</td>\n",
       "      <td>0.635176</td>\n",
       "      <td>0.63774</td>\n",
       "      <td>632</td>\n",
       "      <td>819</td>\n",
       "      <td>355</td>\n",
       "      <td>363</td>\n",
       "      <td>0.332995</td>\n",
       "      <td>0.333004</td>\n",
       "      <td>0.666395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>307.123</td>\n",
       "      <td>379.482</td>\n",
       "      <td>492.242</td>\n",
       "      <td>429.361</td>\n",
       "      <td>213.657</td>\n",
       "      <td>364.373</td>\n",
       "      <td>96.7421</td>\n",
       "      <td>3.22035</td>\n",
       "      <td>2.00201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744376</td>\n",
       "      <td>0.731658</td>\n",
       "      <td>0.737962</td>\n",
       "      <td>728</td>\n",
       "      <td>924</td>\n",
       "      <td>250</td>\n",
       "      <td>267</td>\n",
       "      <td>0.519388</td>\n",
       "      <td>0.519452</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>31.9763</td>\n",
       "      <td>31.7864</td>\n",
       "      <td>31.8414</td>\n",
       "      <td>31.192</td>\n",
       "      <td>8.30906</td>\n",
       "      <td>27.021</td>\n",
       "      <td>9.35985</td>\n",
       "      <td>0.502497</td>\n",
       "      <td>0.662884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.527638</td>\n",
       "      <td>0.629874</td>\n",
       "      <td>525</td>\n",
       "      <td>1027</td>\n",
       "      <td>147</td>\n",
       "      <td>470</td>\n",
       "      <td>0.412637</td>\n",
       "      <td>0.433646</td>\n",
       "      <td>0.701213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>24.037</td>\n",
       "      <td>24.4264</td>\n",
       "      <td>24.231</td>\n",
       "      <td>24.4041</td>\n",
       "      <td>6.19139</td>\n",
       "      <td>20.658</td>\n",
       "      <td>7.23465</td>\n",
       "      <td>0.252068</td>\n",
       "      <td>0.331617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751989</td>\n",
       "      <td>0.569849</td>\n",
       "      <td>0.64837</td>\n",
       "      <td>567</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>428</td>\n",
       "      <td>0.418288</td>\n",
       "      <td>0.429599</td>\n",
       "      <td>0.705282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>16.2884</td>\n",
       "      <td>16.1502</td>\n",
       "      <td>16.4692</td>\n",
       "      <td>16.4074</td>\n",
       "      <td>4.51221</td>\n",
       "      <td>13.9655</td>\n",
       "      <td>4.72788</td>\n",
       "      <td>0.128438</td>\n",
       "      <td>0.119955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615965</td>\n",
       "      <td>0.573869</td>\n",
       "      <td>0.594173</td>\n",
       "      <td>571</td>\n",
       "      <td>818</td>\n",
       "      <td>356</td>\n",
       "      <td>424</td>\n",
       "      <td>0.27205</td>\n",
       "      <td>0.272599</td>\n",
       "      <td>0.635316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>76.4607</td>\n",
       "      <td>76.9057</td>\n",
       "      <td>76.4275</td>\n",
       "      <td>76.0929</td>\n",
       "      <td>17.0665</td>\n",
       "      <td>64.5907</td>\n",
       "      <td>23.7635</td>\n",
       "      <td>1.07035</td>\n",
       "      <td>0.870651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729829</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.658577</td>\n",
       "      <td>597</td>\n",
       "      <td>953</td>\n",
       "      <td>221</td>\n",
       "      <td>398</td>\n",
       "      <td>0.417415</td>\n",
       "      <td>0.423331</td>\n",
       "      <td>0.705877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.083657     0.087841     0.084295    0.0876408   \n",
       "0    NB_WordLevel_TF-IDF    0.0570619    0.0624511    0.0512819    0.0743842   \n",
       "0       NB_N-Gram_TF-IDF    0.0146081     0.031333    0.0386279    0.0167282   \n",
       "0    NB_CharLevel_TF-IDF     0.849732     0.797022       1.1039      1.13063   \n",
       "0       LR_Count_Vectors      18.4121      19.1657       17.922      19.1001   \n",
       "0    LR_WordLevel_TF-IDF     0.667456     0.669091      0.61599     0.690176   \n",
       "0       LR_N-Gram_TF-IDF     0.495503     0.429021     0.355603     0.409944   \n",
       "0    LR_CharLevel_TF-IDF      4.52604      5.30917      5.46984      5.11398   \n",
       "0      SVM_Count_Vectors       142.24      143.032      143.314      144.206   \n",
       "0   SVM_WordLevel_TF-IDF      171.507      164.749      171.223       171.68   \n",
       "0      SVM_N-Gram_TF-IDF      67.1528      65.9447      66.7256       65.973   \n",
       "0   SVM_CharLevel_TF-IDF      567.083       577.67      578.479      579.833   \n",
       "0      kNN_Count_Vectors     0.071985     0.072355       0.0727    0.0731351   \n",
       "0   kNN_WordLevel_TF-IDF     0.049027    0.0487049     0.051249    0.0619388   \n",
       "0      kNN_N-Gram_TF-IDF     0.013175    0.0136449     0.020715    0.0663297   \n",
       "0   kNN_CharLevel_TF-IDF     0.655702     0.672649      0.65748     0.667741   \n",
       "0      SGD_Count_Vectors     0.458804     0.473416     0.420624     0.444952   \n",
       "0   SGD_WordLevel_TF-IDF       0.3128     0.269268     0.277042     0.274197   \n",
       "0     SGD_N-Gram_Vectors     0.118953     0.126627       0.1075     0.105808   \n",
       "0  SGD_CharLevel_Vectors      1.92091      1.93717       1.7542      1.82993   \n",
       "0       GB_Count_Vectors      62.9065      55.3991      56.9035       52.061   \n",
       "0       GB_N-Gram_TF-IDF      5.80552      7.31059      7.17226      6.93111   \n",
       "0    GB_CharLevel_TF-IDF      220.842      238.157      282.195      257.065   \n",
       "0    GB_WordLevel_TF-IDF      92.4755      84.2349      73.6215      84.1027   \n",
       "0      XGB_Count_Vectors      74.1953      65.7518      84.2132      65.1882   \n",
       "0   XGB_WordLevel_TF-IDF      46.6946       49.373      52.0918      37.7714   \n",
       "0      XGB_N-Gram_TF-IDF      14.4524      14.8459      10.1767      6.22884   \n",
       "0   XGB_CharLevel_TF-IDF      307.123      379.482      492.242      429.361   \n",
       "0       RF_Count_Vectors      31.9763      31.7864      31.8414       31.192   \n",
       "0    RF_WordLevel_TF-IDF       24.037      24.4264       24.231      24.4041   \n",
       "0       RF_N-Gram_TF-IDF      16.2884      16.1502      16.4692      16.4074   \n",
       "0    RF_CharLevel_TF-IDF      76.4607      76.9057      76.4275      76.0929   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0806589     0.0848186   0.00268425      0.0557067      0.0472348  ...   \n",
       "0     0.034615     0.0559588    0.0131171      0.0463171      0.0433159  ...   \n",
       "0    0.0169821     0.0236558   0.00956548      0.0413001      0.0396559  ...   \n",
       "0     0.286705      0.833599     0.303969       0.125775      0.0687079  ...   \n",
       "0      9.19119       16.7582      3.81129      0.0704119      0.0472348  ...   \n",
       "0     0.490711      0.626685    0.0722426      0.0417788      0.0403888  ...   \n",
       "0      0.34902      0.407818    0.0535282      0.0507712      0.0475352  ...   \n",
       "0      3.58113       4.80003      0.68807        0.07074      0.0679901  ...   \n",
       "0      113.892       137.337      11.7393        45.6131        45.6117  ...   \n",
       "0      118.665       159.565      20.6152        51.6323        49.8943  ...   \n",
       "0        32.62       59.6832      13.5394        24.1372        25.0592  ...   \n",
       "0      336.997       528.013      95.6156        186.596        184.288  ...   \n",
       "0    0.0939651     0.0768281   0.00857692         3.7683        3.65425  ...   \n",
       "0    0.0523279     0.0526495   0.00483814        4.03613         3.9824  ...   \n",
       "0    0.0168662     0.0261462    0.0202725        1.33747        1.33642  ...   \n",
       "0     0.195899      0.569894     0.187104        48.1581        48.1758  ...   \n",
       "0     0.266678      0.412895    0.0751494      0.0419219      0.0525622  ...   \n",
       "0     0.175028      0.261667    0.0459828        0.03966       0.039535  ...   \n",
       "0    0.0728922      0.106356    0.0183905      0.0420618        0.03441  ...   \n",
       "0      1.36733       1.76191     0.208027      0.0974619       0.103405  ...   \n",
       "0      27.6382       50.9817       12.189      0.0316858      0.0516238  ...   \n",
       "0      5.58752        6.5614     0.719849      0.0432222       0.025759  ...   \n",
       "0      184.308       236.513      33.1146      0.0761411      0.0745621  ...   \n",
       "0       48.617       76.6103      15.2219      0.0370519      0.0738072  ...   \n",
       "0      32.8804       64.4458      17.2304       0.516725        0.95696  ...   \n",
       "0      24.9171       42.1696      9.87732       0.425461       0.305956  ...   \n",
       "0      11.4074       11.4223      3.14409       0.109201      0.0818152  ...   \n",
       "0      213.657       364.373      96.7421        3.22035        2.00201  ...   \n",
       "0      8.30906        27.021      9.35985       0.502497       0.662884  ...   \n",
       "0      6.19139        20.658      7.23465       0.252068       0.331617  ...   \n",
       "0      4.51221       13.9655      4.72788       0.128438       0.119955  ...   \n",
       "0      17.0665       64.5907      23.7635        1.07035       0.870651  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.791759       0.714573         0.751189        711        987   \n",
       "0     0.821277        0.58191         0.681176        579       1048   \n",
       "0     0.730563       0.547739         0.626077        545        973   \n",
       "0     0.834646       0.106533         0.188948        106       1153   \n",
       "0     0.738872       0.750754         0.744766        747        910   \n",
       "0     0.771372       0.779899         0.775612        776        944   \n",
       "0     0.701226       0.632161         0.664905        629        906   \n",
       "0     0.753205       0.708543         0.730192        705        943   \n",
       "0     0.750984       0.766834         0.758826        763        921   \n",
       "0     0.760194       0.786935         0.773333        783        927   \n",
       "0     0.699887       0.621106         0.658147        618        909   \n",
       "0     0.757515       0.759799         0.758655        756        932   \n",
       "0     0.586263       0.720603         0.646528        717        668   \n",
       "0     0.751938      0.0974874         0.172598         97       1142   \n",
       "0     0.584323       0.494472         0.535656        492        824   \n",
       "0     0.739785       0.345729         0.471233        344       1053   \n",
       "0     0.654107       0.872362         0.747631        868        715   \n",
       "0     0.742604       0.756784         0.749627        753        913   \n",
       "0     0.633803       0.678392          0.65534        675        784   \n",
       "0     0.828369       0.586935         0.687059        584       1053   \n",
       "0     0.698055       0.685427         0.691684        682        879   \n",
       "0     0.592755       0.542714         0.566632        540        803   \n",
       "0     0.719447       0.680402          0.69938        677        910   \n",
       "0     0.714286       0.703518         0.708861        700        894   \n",
       "0     0.745763       0.751759         0.748749        748        919   \n",
       "0     0.727728       0.730653         0.729188        727        902   \n",
       "0     0.640324       0.635176          0.63774        632        819   \n",
       "0     0.744376       0.731658         0.737962        728        924   \n",
       "0      0.78125       0.527638         0.629874        525       1027   \n",
       "0     0.751989       0.569849          0.64837        567        987   \n",
       "0     0.615965       0.573869         0.594173        571        818   \n",
       "0     0.729829            0.6         0.658577        597        953   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        187        284             0.559446                  0.561763   \n",
       "0        126        416             0.485369                  0.504886   \n",
       "0        201        450             0.383852                  0.394977   \n",
       "0         21        889            0.0949604                  0.188135   \n",
       "0        264        248             0.525238                  0.525295   \n",
       "0        230        219             0.583496                  0.583527   \n",
       "0        268        366             0.406937                  0.408658   \n",
       "0        231        290             0.514104                  0.514884   \n",
       "0        253        232             0.550446                   0.55055   \n",
       "0        247        212             0.575001                  0.575303   \n",
       "0        265        377             0.398804                  0.401015   \n",
       "0        242        239             0.553539                  0.553541   \n",
       "0        506        278             0.284626                  0.290993   \n",
       "0         32        898            0.0752212                  0.147965   \n",
       "0        350        503             0.198675                   0.20076   \n",
       "0        121        651             0.252933                  0.294636   \n",
       "0        459        127             0.469449                  0.492212   \n",
       "0        261        242              0.53369                  0.533773   \n",
       "0        390        320             0.344348                  0.345069   \n",
       "0        121        411             0.494864                  0.514763   \n",
       "0        295        313             0.434749                   0.43481   \n",
       "0        371        455             0.228168                  0.228875   \n",
       "0        264        318             0.457422                  0.458003   \n",
       "0        280        295             0.465552                  0.465597   \n",
       "0        255        247             0.534225                   0.53424   \n",
       "0        272        268             0.498814                  0.498817   \n",
       "0        355        363             0.332995                  0.333004   \n",
       "0        250        267             0.519388                  0.519452   \n",
       "0        147        470             0.412637                  0.433646   \n",
       "0        187        428             0.418288                  0.429599   \n",
       "0        356        424              0.27205                  0.272599   \n",
       "0        221        398             0.417415                  0.423331   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0        0.777644  \n",
       "0        0.737292  \n",
       "0        0.688265  \n",
       "0        0.544323  \n",
       "0        0.762941  \n",
       "0        0.791994  \n",
       "0        0.701941  \n",
       "0         0.75589  \n",
       "0        0.775666  \n",
       "0        0.788271  \n",
       "0        0.697691  \n",
       "0        0.776833  \n",
       "0        0.644799  \n",
       "0        0.535115  \n",
       "0        0.598173  \n",
       "0        0.621331  \n",
       "0        0.740695  \n",
       "0        0.767234  \n",
       "0        0.673097  \n",
       "0        0.741934  \n",
       "0        0.717075  \n",
       "0         0.61335  \n",
       "0        0.727765  \n",
       "0        0.732508  \n",
       "0        0.767276  \n",
       "0        0.749483  \n",
       "0        0.666395  \n",
       "0        0.759356  \n",
       "0        0.701213  \n",
       "0        0.705282  \n",
       "0        0.635316  \n",
       "0        0.705877  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dl'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "pretrained = fasttext.FastText.load_model('/Users/diego/Documents/NLP/crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88180/88180 [00:04<00:00, 18509.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.54 s, sys: 1.81 s, total: 11.4 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a tokenizer \n",
    "token = Tokenizer(oov_token='<OOV>')\n",
    "token.fit_on_texts(df[TEXT])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=300)\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "words = []\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    words.append(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "if save_model:\n",
    "    filename = NAME_TOKEN_EMBEDDINGS\n",
    "    pickle.dump(token, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_w = {}\n",
    "for i in zip(range(len(class_weights)), class_weights):\n",
    "    class_w[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_NN(model, X, y, X_test, y_test,name=\"NN\", fit_params=None, scoring=None, n_splits=5, save=save_model, batch_size = 32,  use_multiprocessing=True):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param model: (model) neural network model\n",
    "    @param X: (list or matrix or tensor) training X data\n",
    "    @param y: (list) label data \n",
    "    @param X_test: (list or matrix or tensor) testing X data\n",
    "    @param y_test: (list) label test data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param scoring: (dict) dictionary of metrics and names\n",
    "    @param n_splits: (int) number of fold for cross-validation (default 5)\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    # ---- Parameters initialisation\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='auto', patience=3)\n",
    "    seed = 42\n",
    "    k = 1\n",
    "    np.random.seed(seed)\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Creation of list for each metric\n",
    "    if scoring==None:        # create a dictionary if none is passed\n",
    "        dic_scoring = {}\n",
    "    if scoring!=None:        # save the dict \n",
    "        dic_score = scoring.copy()\n",
    "    \n",
    "    dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "    dic_score[\"score_time\"] = None\n",
    "    scorer = {}\n",
    "    for i in dic_score.keys(): \n",
    "        scorer[i] = []\n",
    "    \n",
    "    index = [\"Model\"]\n",
    "    results = [name]\n",
    "    # ---- Loop on k-fold for cross-valisation\n",
    "    for train, test in kfold.split(X, y):   # training NN on each fold \n",
    "        # create model\n",
    "        print(f\"k-fold : {k}\")\n",
    "        fit_start = time.time()\n",
    "        _model = tf.keras.models.clone_model(model)\n",
    "        if len(np.unique(y))==2: # binary\n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        else:  # multiclass \n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "        fit_end = time.time() - fit_start\n",
    "\n",
    "        score_start = time.time()\n",
    "        y_pred = (_model.predict(X[test])>0.5).astype(int)\n",
    "        score_end = time.time() - score_start\n",
    "        #if len(set(y))>2:\n",
    "        #    y_pred =np.argmax(y_pred,axis=1)\n",
    "        #print(y_test[0], y_pred[0])\n",
    "        if len(set(y))==2:\n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], y_pred), 3)}% , Recall: {round(100*recall_score(y.iloc[test], y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        else: \n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        \n",
    "        \n",
    "        # ---- save each metric\n",
    "        for i in dic_score.keys():    # compute metrics \n",
    "            if i == \"fit_time\":\n",
    "                scorer[i].append(fit_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(fit_end)\n",
    "                continue\n",
    "            if i == \"score_time\":\n",
    "                scorer[i].append(score_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(score_end)\n",
    "                continue\n",
    "            \n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    scorer[i].append(dic_score[i](y.iloc[test], np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "                elif i==\"roc_auc\":\n",
    "                    scorer[i].append(dic_score[i](to_categorical(y.iloc[test]), y_pred, average = 'macro', multi_class=\"ovo\")) # make each function scorer\n",
    "                else:\n",
    "                    scorer[i].append(dic_score[i]( y.iloc[test], np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i]( y.iloc[test], y_pred)) # make each function scorer\n",
    "            #scorer[i].append(dic_score[i]( y.iloc[test], y_pred))\n",
    "            index.append(\"test_\"+i+'_cv'+str(k))\n",
    "            results.append(scorer[i][-1])\n",
    "        K.clear_session()\n",
    "        del _model\n",
    "        k+=1\n",
    "    \n",
    "    # Train test on the overall data\n",
    "    print(\"Overall train-test data\")\n",
    "    fit_start = time.time()\n",
    "    _model =  tf.keras.models.clone_model(model)\n",
    "    if len(np.unique(y))==2: # binary\n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    else:  # multiclass \n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False)\n",
    "    if save:\n",
    "        check_p = tf.keras.callbacks.ModelCheckpoint(os.path.join(root_dir, dir_name, name+\".h5\"), save_best_only=True)\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es, check_p], validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    else:\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es],  validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    #_acc = _model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    score_start = time.time()\n",
    "    y_pred = (_model.predict(X_test)>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    #if len(set(y))>2:\n",
    "    #    y_pred =np.argmax(y_pred,axis=1)\n",
    "    if len(set(y))==2:\n",
    "        print(f\"Precision: {round(100*precision_score(y_test, y_pred), 3)}% , Recall: {round(100*recall_score(y_test, y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "    else: \n",
    "        print(f\"Precision: {round(100*precision_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "\n",
    "    # Compute mean and std for each metric\n",
    "    for i in scorer: \n",
    "        \n",
    "        results.append(np.mean(scorer[i]))\n",
    "        results.append(np.std(scorer[i]))\n",
    "        if i == \"fit_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        \n",
    "        index.append(\"test_\"+i+\"_mean\")\n",
    "        index.append(\"test_\"+i+\"_std\")\n",
    "        \n",
    "    # add metrics averall dataset on the dictionary \n",
    "    for i in dic_score.keys():    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            scorer[i].append(fit_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            scorer[i].append(score_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(score_end)\n",
    "            continue\n",
    "        \n",
    "        if len(set(y))>2:\n",
    "            if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "            elif i==\"roc_auc\":\n",
    "                scorer[i].append(dic_score[i](to_categorical(y_test), y_pred, average = 'weighted', multi_class=\"ovo\")) # make each function scorer\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "        else:\n",
    "            scorer[i].append(dic_score[i](y.iloc[test], y_pred))                             \n",
    "            #scorer[i].append(dic_score[i](_model, X_test, y_test))\n",
    "        index.append(i+'_overall')\n",
    "        results.append(scorer[i][-1])\n",
    "    \n",
    "            \n",
    "    return pd.DataFrame(results, index=index).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='snn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Shallow Neural Network</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a shallow neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) shallow neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 16)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      \n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "        \n",
    "      #keras.layers.Dense(6, activation=\"relu\"),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = score_metrics\n",
    "\n",
    "# Creation of list for each metric\n",
    "if scoring==None:        # create a dictionary if none is passed\n",
    "    dic_scoring = {}\n",
    "if scoring!=None:        # save the dict \n",
    "    dic_score = scoring.copy()\n",
    "\n",
    "dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "dic_score[\"score_time\"] = None\n",
    "scorer = {}\n",
    "for i in dic_score.keys(): \n",
    "    scorer[i] = []\n",
    "\n",
    "index = [\"Model\"]\n",
    "results = ['Shallow_NN_WE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dic_score.keys():\n",
    "    scorer[i].append(dic_score[i](_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score['acc'](y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430    fanboy ism h k stuff scar h acr exactly aesthe...\n",
       "5089    curse aye awww good old bargaining alarm clock...\n",
       "111     usually psychic quality knowing come specific ...\n",
       "7536    thank advice appreciate just fantastic advice ...\n",
       "1113    trying understand difference sx sx just differ...\n",
       "                              ...                        \n",
       "2926    true comfortable contradiction hi following re...\n",
       "3821    pretty christian ish t believe jesus christ go...\n",
       "5805    bat sherlock holmes quite alright agnostic spi...\n",
       "3140    try lang studying japanese sent gt using tapat...\n",
       "2770    ugh honestly inclined think behavior abusive t...\n",
       "Name: text_clean_joined, Length: 2169, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score[i](_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 71.454% , Recall: 67.391%, Time \t 237.6945 ms\n",
      "k-fold : 2\n",
      "Precision: 72.438% , Recall: 68.677%, Time \t 365.2575 ms\n",
      "k-fold : 3\n",
      "Precision: 70.662% , Recall: 66.164%, Time \t 396.282 ms\n",
      "k-fold : 4\n",
      "Precision: 73.179% , Recall: 69.012%, Time \t 263.4461 ms\n",
      "k-fold : 5\n",
      "Precision: 71.357% , Recall: 71.357%, Time \t 327.027 ms\n",
      "Overall train-test data\n",
      "Precision: 70.657% , Recall: 67.035%, Time \t 338.0158 ms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1301, 2169]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-40eebde9b6a9>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mscorer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;31m#scorer[i].append(dic_score[i](_model, X_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_overall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1301, 2169]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if shallow_network:\n",
    "    df_results = df_results.append(cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Shallow_NN_WE\", scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dnn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Deep Neural Net</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_WE\",scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits , save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var1(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_var1_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Transformers</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](img/transformers_model_architecture.png)\n",
    "\n",
    "The Transformer – Model Architecture - [Source](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=vidya></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
