{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project - Modeling (step 5)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Introduction</h3>\n",
    "    <p>This notebook contains the <b>Modeling</b> step which comes after the <b>Feature Engineering & Preprocessing</b> step. The main goal of this step involves selecting, training and deploying a model to make predictive insights.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-danger\">\n",
    "\n",
    "<h3>Disclaimer</h3>\n",
    "    <p>The purpose of this notebook is to go over certain aspects of Natural Language Processing. There might be some parts of the notebook that do not have particular use for the future of this project but they are useful for learning purposes so I left them inside. I also would like to mention that some of the code here is recycled from online articles and notebooks on GitHub, I will try to mention every source as best as possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Summarized goals](#goals)\n",
    "- [Importing Libraries](#importing)\n",
    "- [Review of our Dataset](#review)\n",
    "- [Models Introduction](#model)\n",
    "- [Parameters and Models](#parameters)\n",
    "- [Stopwords](#stopwords)\n",
    "- [Train Test Split](#train_test)\n",
    "- [CountVectorizer and tf-id](#cv)\n",
    "- [Report Function](#report)\n",
    "- [Let's Start Modeling](#modeling) Every model is created with CountVectorizer, TF-IDF words, TF-IDF n_grams, TF-IDF characters\n",
    "    - [MACHINE LEARNING](#ml)\n",
    "        - [Multinomial Naive Bayes Models](#nb)  \n",
    "        - [Logistic Regression](#lr)  \n",
    "        - [Support Vector Machines](#svm)  \n",
    "        - [K-Nearest Neightbors](#knn)  \n",
    "        - [Random Forest](#NB)      \n",
    "        - [Stocastic Gradient Descent](#sgd)\n",
    "        - [Boosting](#boost)\n",
    "            - [Gradient Boosting Classifier](#gbc)\n",
    "            - [XGBoost](#xgb)\n",
    "            - [Catboost](#cb) - Pending\n",
    "            - [Adaboost](#ab) - Pending        \n",
    "            - [LightGBM](#lgbm) - Pending       \n",
    "    - [DEEP LEARNING](#dl) - Pending all section\n",
    "        - [Shallow Neural Network](#snn)\n",
    "        - [Deep Neural Network](#dnn)\n",
    "        - [Transformers](#trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=goals></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarized Goals\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best model that classifies each post into the pair of attributes of the MBTI:\n",
    " - Introversion vs. Extraversion (I vs. E)\n",
    " - Intuition vs. Sensing (N vs. S) --> This notebook focuses on this category\n",
    " - Thinking vs. Feeling (T vs. F)\n",
    " - Judging vs. Perceiving (J vs. P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=importing></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# data wrangiling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.transforms\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set() #\n",
    "\n",
    "# natural language processing libraries\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "import textstat\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas(desc=\"Progress!\")\n",
    "import time\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/mbti_nlp.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Models Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece generates four dataframes one for each pair of attributes\n",
    "N = df[['N','text_clean_joined']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** I will be using `Christophe Pere's` notebook as the basis for this model. All credits go to him, [here is the original notebook](https://github.com/Christophe-pere/Model-Selection/blob/master/Text_Classification_Compare_Models.ipynb) and here is his [TowardsDataScience article](https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract the true, false positive and true false negative\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=parameters></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Models\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT           = \"text_clean_joined\"\n",
    "LABEL          = \"N\"\n",
    "NAME_SAVE_FILE = \"model_selection_results_NS\" # put just the name the .csv will be added at the end\n",
    "\n",
    "# global parameters\n",
    "num_gpu                = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "CV_splits              = 5        # Number of splits for cross-validation and k-folds\n",
    "save_results           = True     # if you want an output file containing all the results\n",
    "lang                   = False    # test if you want to use Google API detection (you will need to \"import from googletrans import Translator\")\n",
    "sample                 = True     # use just a sample of data\n",
    "nb_sample              = 6000     # default value of rows if sample selected\n",
    "save_model             = True     # concat all the data representation\n",
    "root_dir               = \"models/\"       # Place here the path where you want your models stored or use /path/to/your/folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names how the files will be saved as \n",
    "NAME_ENCODER                  = \"encoder.sav\"\n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_TOKEN_EMBEDDINGS         = \"token_embeddings.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "multinomial_naive_bayes= True\n",
    "logistic_regression    = True\n",
    "svm_model              = True\n",
    "k_nn_model             = True\n",
    "sgd                    = True\n",
    "random_forest          = True\n",
    "gradient_boosting      = True\n",
    "xgboost_classifier     = True\n",
    "adaboost_classifier    = True \n",
    "catboost_classifier    = True \n",
    "lightgbm_classifier    = True \n",
    "extratrees_classifier  = True\n",
    "shallow_network        = True\n",
    "deep_nn                = True\n",
    "rnn                    = True\n",
    "lstm                   = True\n",
    "cnn                    = True\n",
    "gru                    = True\n",
    "cnn_lstm               = True\n",
    "cnn_gru                = True\n",
    "bidirectional_rnn      = True\n",
    "bidirectional_lstm     = True\n",
    "bidirectional_gru      = True\n",
    "rcnn                   = True\n",
    "transformers           = False\n",
    "pre_trained            = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is created\n"
     ]
    }
   ],
   "source": [
    "if save_model:\n",
    "    # will create the folder to save all the models\n",
    "    try:\n",
    "        dir_name =  NAME_SAVE_FILE\n",
    "        os.makedirs(os.path.join(root_dir,dir_name))\n",
    "        print(\"The folder is created\")\n",
    "    except:\n",
    "        print(\"The folder can not be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can put all the metrics you want (included in sklearn.metrics).\n",
    "score_metrics = {'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Christophe Pere` has a set of functions to clean the text but we have already done that so I will not add them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do add a remove stop words function\n",
    "def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBTI types are rarely discussed in day to day converstaions, we will take them out since they would have low prediction power\n",
    "types = [x.lower() for x in df['type'].unique()] \n",
    "types_plural = [x+'s' for x in types]\n",
    "\n",
    "# some words that appear a lot but do not add value\n",
    "additional_stop_words = ['ll','type','fe','ni','na','wa','ve','don','nt','nf', 'ti','se','op','ne'] \n",
    "\n",
    "# We put these together and include the normal stopwords from the English language\n",
    "stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS.union(additional_stop_words + types + types_plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress!: 100%|██████████| 8675/8675 [00:02<00:00, 2913.97it/s]\n"
     ]
    }
   ],
   "source": [
    "N[TEXT] = N.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the code simple (for the time being), I will start by focusing on the `Thinking / Feeling` and later implement the same process for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = N.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[TEXT], df[LABEL], test_size=0.25, random_state=42, stratify=df[LABEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Personal note on stratify:** if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(y_train),\n",
    "                                                 y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 3.6225\tclass: 0\n",
      "Class weight: 0.5801\tclass: 1\n"
     ]
    }
   ],
   "source": [
    "print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset is balanced (ratio=0.16)\n"
     ]
    }
   ],
   "source": [
    "# Determined if the dataset is balanced or imbalanced \n",
    "ratio = np.min(df[LABEL].value_counts()) / np.max(df[LABEL].value_counts())\n",
    "if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "    balanced = True\n",
    "    print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "else:\n",
    "    balanced = False\n",
    "    print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "    #from imblearn.over_sampling import ADASYN\n",
    "    # put class for debalanced data \n",
    "    # in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer & TF-IDF\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section transforms our data into something interpretable by the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.08 s, sys: 188 ms, total: 8.27 s\n",
      "Wall time: 8.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df[TEXT])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "x_train_count =  count_vect.transform(X_train)\n",
    "x_test_count =  count_vect.transform(X_test)\n",
    "\n",
    "if save_model:\n",
    "    # save the model to disk\n",
    "    filename = NAME_COUNT_VECT_MODEL\n",
    "    pickle.dump(count_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n",
      "CPU times: user 2min 29s, sys: 3.09 s, total: 2min 32s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "tfidf_vect.fit(df[TEXT])\n",
    "x_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "x_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "print(\"word level tf-idf done\")\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect_ngram.fit(df[TEXT])\n",
    "x_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "x_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "print(\"ngram level tf-idf done\")\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) \n",
    "tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "x_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "x_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) \n",
    "print(\"characters level tf-idf done\")\n",
    "\n",
    "if save_model:\n",
    "    # save the model tf-idf to disk\n",
    "    filename = NAME_TF_IDF_MODEL\n",
    "    pickle.dump(tfidf_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "\n",
    "    # save the model ngram to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # save the model ngram char to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_CHAR_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram_chars, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Function\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followign function will generate, for each model we create, a set of metrics that evaluate how well that model did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, x, y, X_test, y_test, name='classifier', cv=5, dict_scoring=None, fit_params=None, save=save_model):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param clf: (model) classifier\n",
    "    @param x: (list or matrix or tensor) training x data\n",
    "    @param y: (list) label data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param cv: (int) number of fold for cross-validation (default 5)\n",
    "    @param dict_scoring: (dict) dictionary of metrics and names\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param save: (bool) determine if the model need to be saved\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    \n",
    "    '''{'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}'''\n",
    "    \n",
    "    \n",
    "    if dict_scoring!=None:\n",
    "        score = dict_scoring.copy() # save the original dictionary\n",
    "        for i in score.keys():\n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted') # make each function scorer\n",
    "                elif i==\"roc_auc\":\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted', multi_class=\"ovo\",needs_proba=True) # make each function scorer\n",
    "                else:\n",
    "                    score[i] = make_scorer(score[i]) # make each function scorer\n",
    "                    \n",
    "            else:\n",
    "                score[i] = make_scorer(score[i]) # make each function scorer\n",
    "            \n",
    "    try:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False, n_jobs=-1,  fit_params=fit_params)\n",
    "    except:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False,  fit_params=fit_params)\n",
    "        \n",
    "    # Train test on the overall data\n",
    "    fit_start = time.time()\n",
    "    _model = clf\n",
    "    _model.fit(x, y)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    \n",
    "    score_start = time.time()\n",
    "    y_pred = _model.predict(X_test)#>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    \n",
    "    # this saves the model for reuse\n",
    "    if save:\n",
    "        filename= name+\".sav\"\n",
    "        pickle.dump(_model, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # initialisation \n",
    "    index = []\n",
    "    value = []\n",
    "    index.append(\"Model\")\n",
    "    value.append(name)\n",
    "    for i in scores:  # loop on each metric generate text and values\n",
    "        if i == \"estimator\":\n",
    "            continue\n",
    "        for j in enumerate(scores[i]):\n",
    "            index.append(i+\"_cv\"+str(j[0]+1))\n",
    "            value.append(j[1])\n",
    "        \n",
    "        \n",
    "        index.append(i+\"_mean\")\n",
    "        value.append(np.mean(scores[i]))\n",
    "        index.append(i+\"_std\")\n",
    "        value.append(np.std(scores[i]))\n",
    "    \n",
    "     # add metrics averall dataset on the dictionary \n",
    "    \n",
    "    for i in scores:    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,fit_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,score_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(score_end)\n",
    "            continue\n",
    "              \n",
    "        \n",
    "        scores[i] = np.append(scores[i] ,score[i.split(\"test_\")[-1]](_model, X_test, y_test))\n",
    "        index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "        value.append(scores[i][-1])\n",
    "    \n",
    "    return pd.DataFrame(data=value, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Modeling!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating the empty dataframe we will use to put the results of each model we create\n",
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Multinomial Naïve Bayes</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `RuntimeWarning: invalid value encountered in double_scalars` is due to the model predicting 0 true negatives of 0 false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 765 ms, sys: 277 ms, total: 1.04 s\n",
      "Wall time: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if multinomial_naive_bayes:\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_count, y_train, x_test_count, y_test, name='NB_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf, y_train, x_test_tfidf, y_test, name='NB_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='NB_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(),x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='NB_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Logistic Regression</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.2 s, sys: 4.16 s, total: 47.4 s\n",
      "Wall time: 54.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if logistic_regression:\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_count, y_train, x_test_count, y_test, name='LR_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='LR_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='LR_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='LR_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Support Vector Machine</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 44s, sys: 5.83 s, total: 48min 50s\n",
      "Wall time: 1h 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if svm_model:\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_count, y_train, x_test_count, y_test, name='SVM_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SVM_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SVM_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SVM_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>K-Nearest Neighbors</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 58s, sys: 20.7 s, total: 18min 19s\n",
      "Wall time: 6min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if k_nn_model:\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_count, y_train, x_test_count, y_test, name='kNN_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf, y_train, x_test_tfidf, y_test, name='kNN_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='kNN_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='kNN_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Random Forest</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 2s, sys: 1.48 s, total: 4min 3s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_count, y_train, x_test_count, y_test, name='RF_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf, y_train, x_test_tfidf, y_test, name='RF_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='RF_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,  name='RF_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Stocastis Gradient Descent</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 s, sys: 222 ms, total: 1.94 s\n",
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if sgd:\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_count, y_train, x_test_count, y_test, name='SGD_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf, y_train, x_test_tfidf, y_test, name='SGD_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='SGD_N-Gram_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='SGD_CharLevel_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boost'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gbc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Gradient Boosting Classifier</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.15 s, sys: 130 ms, total: 7.28 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_count, y_train, x_test_count, y_test,\n",
    "                                          name='GB_Count_Vectors', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 s, sys: 83.4 ms, total: 8.42 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf, y_train, x_test_tfidf, y_test,\n",
    "                                          name='GB_WordLevel_TF-IDF', \n",
    "                                          cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, \n",
    "                                          save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 s, sys: 25.4 ms, total: 1.91 s\n",
      "Wall time: 6.32 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test,\n",
    "                                          name='GB_N-Gram_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.4 s, sys: 425 ms, total: 53.8 s\n",
      "Wall time: 2min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), \n",
    "                                          x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test,\n",
    "                                          name='GB_CharLevel_TF-IDF', cv=CV_splits, \n",
    "                                          dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>XGBoost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 41s, sys: 1.92 s, total: 12min 43s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        # run on CPU\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_count, y_train, x_test_count, y_test, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 51s, sys: 1.14 s, total: 9min 52s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist', n_estimators=1000, subsample=0.8), x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8),x_train_tfidf, y_train, x_test_tfidf, y_test, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 57s, sys: 578 ms, total: 4min 58s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    \n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 2s, sys: 3.21 s, total: 36min 6s\n",
      "Wall time: 11min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10, 'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    \n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.0797718</td>\n",
       "      <td>0.071507</td>\n",
       "      <td>0.0771351</td>\n",
       "      <td>0.0738273</td>\n",
       "      <td>0.0672839</td>\n",
       "      <td>0.073905</td>\n",
       "      <td>0.00434622</td>\n",
       "      <td>0.0425792</td>\n",
       "      <td>0.0462492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867487</td>\n",
       "      <td>0.980214</td>\n",
       "      <td>0.920412</td>\n",
       "      <td>1833</td>\n",
       "      <td>19</td>\n",
       "      <td>280</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0664406</td>\n",
       "      <td>0.0951219</td>\n",
       "      <td>0.52188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0481851</td>\n",
       "      <td>0.0466261</td>\n",
       "      <td>0.0766451</td>\n",
       "      <td>0.0649893</td>\n",
       "      <td>0.0271151</td>\n",
       "      <td>0.0527122</td>\n",
       "      <td>0.0169485</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0535359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.014116</td>\n",
       "      <td>0.01668</td>\n",
       "      <td>0.016032</td>\n",
       "      <td>0.0157461</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>0.0151942</td>\n",
       "      <td>0.00123341</td>\n",
       "      <td>0.0271609</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.287065</td>\n",
       "      <td>0.285828</td>\n",
       "      <td>0.284507</td>\n",
       "      <td>0.296301</td>\n",
       "      <td>0.162329</td>\n",
       "      <td>0.263206</td>\n",
       "      <td>0.0506087</td>\n",
       "      <td>0.0458188</td>\n",
       "      <td>0.04583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862085</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.925706</td>\n",
       "      <td>1869</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000919856</td>\n",
       "      <td>-0.00858786</td>\n",
       "      <td>0.499733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>21.9553</td>\n",
       "      <td>21.4892</td>\n",
       "      <td>21.74</td>\n",
       "      <td>21.59</td>\n",
       "      <td>8.46463</td>\n",
       "      <td>19.0478</td>\n",
       "      <td>5.29392</td>\n",
       "      <td>0.0266199</td>\n",
       "      <td>0.048497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886261</td>\n",
       "      <td>0.941711</td>\n",
       "      <td>0.913145</td>\n",
       "      <td>1761</td>\n",
       "      <td>73</td>\n",
       "      <td>226</td>\n",
       "      <td>109</td>\n",
       "      <td>0.222417</td>\n",
       "      <td>0.231102</td>\n",
       "      <td>0.592929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.788837</td>\n",
       "      <td>0.714346</td>\n",
       "      <td>0.687638</td>\n",
       "      <td>0.755387</td>\n",
       "      <td>0.356363</td>\n",
       "      <td>0.660514</td>\n",
       "      <td>0.155955</td>\n",
       "      <td>0.0317733</td>\n",
       "      <td>0.0414839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862546</td>\n",
       "      <td>1</td>\n",
       "      <td>0.926201</td>\n",
       "      <td>1870</td>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00575294</td>\n",
       "      <td>0.0537101</td>\n",
       "      <td>0.501672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.286369</td>\n",
       "      <td>0.227528</td>\n",
       "      <td>0.290044</td>\n",
       "      <td>0.26089</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.247726</td>\n",
       "      <td>0.0431982</td>\n",
       "      <td>0.0387211</td>\n",
       "      <td>0.0386691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>3.30882</td>\n",
       "      <td>2.90528</td>\n",
       "      <td>3.38328</td>\n",
       "      <td>3.49237</td>\n",
       "      <td>1.72421</td>\n",
       "      <td>2.96279</td>\n",
       "      <td>0.650291</td>\n",
       "      <td>0.05165</td>\n",
       "      <td>0.061012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>131.387</td>\n",
       "      <td>112.139</td>\n",
       "      <td>112.127</td>\n",
       "      <td>131.035</td>\n",
       "      <td>68.7219</td>\n",
       "      <td>111.082</td>\n",
       "      <td>22.8343</td>\n",
       "      <td>26.4534</td>\n",
       "      <td>29.5846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>133.493</td>\n",
       "      <td>133.016</td>\n",
       "      <td>132.823</td>\n",
       "      <td>133.299</td>\n",
       "      <td>76.1669</td>\n",
       "      <td>121.759</td>\n",
       "      <td>22.7974</td>\n",
       "      <td>31.5992</td>\n",
       "      <td>31.3205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>1</td>\n",
       "      <td>0.926431</td>\n",
       "      <td>1870</td>\n",
       "      <td>2</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0114781</td>\n",
       "      <td>0.075975</td>\n",
       "      <td>0.503344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>47.7785</td>\n",
       "      <td>47.4211</td>\n",
       "      <td>47.8603</td>\n",
       "      <td>47.842</td>\n",
       "      <td>29.1891</td>\n",
       "      <td>44.0182</td>\n",
       "      <td>7.41624</td>\n",
       "      <td>13.9483</td>\n",
       "      <td>13.848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>338.5</td>\n",
       "      <td>335.992</td>\n",
       "      <td>334.67</td>\n",
       "      <td>332.739</td>\n",
       "      <td>196.133</td>\n",
       "      <td>307.607</td>\n",
       "      <td>55.7686</td>\n",
       "      <td>83.4399</td>\n",
       "      <td>83.2907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.0667431</td>\n",
       "      <td>0.0663488</td>\n",
       "      <td>0.0670531</td>\n",
       "      <td>0.0661709</td>\n",
       "      <td>0.0530539</td>\n",
       "      <td>0.063874</td>\n",
       "      <td>0.00541875</td>\n",
       "      <td>3.76564</td>\n",
       "      <td>3.7982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862818</td>\n",
       "      <td>0.99893</td>\n",
       "      <td>0.925898</td>\n",
       "      <td>1868</td>\n",
       "      <td>2</td>\n",
       "      <td>297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0095966</td>\n",
       "      <td>0.0451534</td>\n",
       "      <td>0.50281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.0299883</td>\n",
       "      <td>0.0400939</td>\n",
       "      <td>0.038903</td>\n",
       "      <td>0.0345459</td>\n",
       "      <td>0.034281</td>\n",
       "      <td>0.0355624</td>\n",
       "      <td>0.00361773</td>\n",
       "      <td>3.52425</td>\n",
       "      <td>3.57414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863279</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.926394</td>\n",
       "      <td>1869</td>\n",
       "      <td>3</td>\n",
       "      <td>296</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0162214</td>\n",
       "      <td>0.076324</td>\n",
       "      <td>0.504749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.0121822</td>\n",
       "      <td>0.0121479</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.0324237</td>\n",
       "      <td>0.0171649</td>\n",
       "      <td>0.0177564</td>\n",
       "      <td>0.00756807</td>\n",
       "      <td>1.13322</td>\n",
       "      <td>1.11459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.306187</td>\n",
       "      <td>0.311969</td>\n",
       "      <td>0.298373</td>\n",
       "      <td>0.299444</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.278114</td>\n",
       "      <td>0.0519911</td>\n",
       "      <td>33.8946</td>\n",
       "      <td>33.921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>35.4174</td>\n",
       "      <td>34.4362</td>\n",
       "      <td>35.394</td>\n",
       "      <td>36.0363</td>\n",
       "      <td>9.56988</td>\n",
       "      <td>30.1708</td>\n",
       "      <td>10.3131</td>\n",
       "      <td>0.522788</td>\n",
       "      <td>0.440718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>29.6302</td>\n",
       "      <td>29.046</td>\n",
       "      <td>29.2784</td>\n",
       "      <td>28.713</td>\n",
       "      <td>7.92183</td>\n",
       "      <td>24.9179</td>\n",
       "      <td>8.5033</td>\n",
       "      <td>0.164787</td>\n",
       "      <td>0.153455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92666</td>\n",
       "      <td>1870</td>\n",
       "      <td>3</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0171758</td>\n",
       "      <td>0.0930715</td>\n",
       "      <td>0.505017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>16.1311</td>\n",
       "      <td>15.9055</td>\n",
       "      <td>15.809</td>\n",
       "      <td>15.8788</td>\n",
       "      <td>3.96744</td>\n",
       "      <td>13.5384</td>\n",
       "      <td>4.78668</td>\n",
       "      <td>0.099746</td>\n",
       "      <td>0.189458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>72.818</td>\n",
       "      <td>74.9766</td>\n",
       "      <td>74.1606</td>\n",
       "      <td>72.0259</td>\n",
       "      <td>20.3815</td>\n",
       "      <td>62.8725</td>\n",
       "      <td>21.2702</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.44614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.466264</td>\n",
       "      <td>0.439922</td>\n",
       "      <td>0.372193</td>\n",
       "      <td>0.351783</td>\n",
       "      <td>0.247021</td>\n",
       "      <td>0.375437</td>\n",
       "      <td>0.0767706</td>\n",
       "      <td>0.0806799</td>\n",
       "      <td>0.0717239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894488</td>\n",
       "      <td>0.91123</td>\n",
       "      <td>0.902781</td>\n",
       "      <td>1704</td>\n",
       "      <td>98</td>\n",
       "      <td>201</td>\n",
       "      <td>166</td>\n",
       "      <td>0.251348</td>\n",
       "      <td>0.251991</td>\n",
       "      <td>0.619495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.213058</td>\n",
       "      <td>0.216579</td>\n",
       "      <td>0.219832</td>\n",
       "      <td>0.218021</td>\n",
       "      <td>0.151218</td>\n",
       "      <td>0.203742</td>\n",
       "      <td>0.0263557</td>\n",
       "      <td>0.0352151</td>\n",
       "      <td>0.0362551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886476</td>\n",
       "      <td>0.960428</td>\n",
       "      <td>0.921971</td>\n",
       "      <td>1796</td>\n",
       "      <td>69</td>\n",
       "      <td>230</td>\n",
       "      <td>74</td>\n",
       "      <td>0.24486</td>\n",
       "      <td>0.265613</td>\n",
       "      <td>0.595599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.100723</td>\n",
       "      <td>0.10501</td>\n",
       "      <td>0.104479</td>\n",
       "      <td>0.098423</td>\n",
       "      <td>0.107885</td>\n",
       "      <td>0.103304</td>\n",
       "      <td>0.00333954</td>\n",
       "      <td>0.0329521</td>\n",
       "      <td>0.0335541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869417</td>\n",
       "      <td>0.957754</td>\n",
       "      <td>0.91145</td>\n",
       "      <td>1791</td>\n",
       "      <td>30</td>\n",
       "      <td>269</td>\n",
       "      <td>79</td>\n",
       "      <td>0.0792393</td>\n",
       "      <td>0.0916641</td>\n",
       "      <td>0.529044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.05028</td>\n",
       "      <td>0.982358</td>\n",
       "      <td>0.989798</td>\n",
       "      <td>0.989138</td>\n",
       "      <td>0.966923</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.0285068</td>\n",
       "      <td>0.0510993</td>\n",
       "      <td>0.0418549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862085</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.925706</td>\n",
       "      <td>1869</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000919856</td>\n",
       "      <td>-0.00858786</td>\n",
       "      <td>0.499733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>8.7917</td>\n",
       "      <td>10.8815</td>\n",
       "      <td>12.6547</td>\n",
       "      <td>9.57907</td>\n",
       "      <td>5.49654</td>\n",
       "      <td>9.48071</td>\n",
       "      <td>2.38305</td>\n",
       "      <td>0.0406899</td>\n",
       "      <td>0.0356059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.997861</td>\n",
       "      <td>0.926055</td>\n",
       "      <td>1866</td>\n",
       "      <td>5</td>\n",
       "      <td>294</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0246095</td>\n",
       "      <td>0.0782107</td>\n",
       "      <td>0.507292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>8.38092</td>\n",
       "      <td>8.40496</td>\n",
       "      <td>8.37228</td>\n",
       "      <td>8.33199</td>\n",
       "      <td>5.34439</td>\n",
       "      <td>7.76691</td>\n",
       "      <td>1.21149</td>\n",
       "      <td>0.038368</td>\n",
       "      <td>0.0394897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863678</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.926624</td>\n",
       "      <td>1869</td>\n",
       "      <td>4</td>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0218804</td>\n",
       "      <td>0.0923241</td>\n",
       "      <td>0.506422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>2.81184</td>\n",
       "      <td>2.68285</td>\n",
       "      <td>2.73053</td>\n",
       "      <td>2.66889</td>\n",
       "      <td>1.4829</td>\n",
       "      <td>2.4754</td>\n",
       "      <td>0.498761</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.053539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>69.101</td>\n",
       "      <td>69.5549</td>\n",
       "      <td>59.0963</td>\n",
       "      <td>47.4193</td>\n",
       "      <td>53.0389</td>\n",
       "      <td>59.6421</td>\n",
       "      <td>8.72964</td>\n",
       "      <td>0.053515</td>\n",
       "      <td>0.0418267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862148</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>19.377</td>\n",
       "      <td>14.4478</td>\n",
       "      <td>14.4455</td>\n",
       "      <td>15.2261</td>\n",
       "      <td>12.7457</td>\n",
       "      <td>15.2484</td>\n",
       "      <td>2.21789</td>\n",
       "      <td>0.266235</td>\n",
       "      <td>0.508641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869137</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.926037</td>\n",
       "      <td>1853</td>\n",
       "      <td>20</td>\n",
       "      <td>279</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0914643</td>\n",
       "      <td>0.153879</td>\n",
       "      <td>0.528899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>23.9198</td>\n",
       "      <td>19.9961</td>\n",
       "      <td>14.7141</td>\n",
       "      <td>22.6357</td>\n",
       "      <td>11.6995</td>\n",
       "      <td>18.593</td>\n",
       "      <td>4.67447</td>\n",
       "      <td>0.195653</td>\n",
       "      <td>0.400202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.923039</td>\n",
       "      <td>1853</td>\n",
       "      <td>7</td>\n",
       "      <td>292</td>\n",
       "      <td>17</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.0471948</td>\n",
       "      <td>0.50716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>5.79742</td>\n",
       "      <td>7.03303</td>\n",
       "      <td>5.90267</td>\n",
       "      <td>8.18669</td>\n",
       "      <td>3.42863</td>\n",
       "      <td>6.06969</td>\n",
       "      <td>1.58031</td>\n",
       "      <td>0.168404</td>\n",
       "      <td>0.133843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863488</td>\n",
       "      <td>0.987701</td>\n",
       "      <td>0.921427</td>\n",
       "      <td>1847</td>\n",
       "      <td>7</td>\n",
       "      <td>292</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0178621</td>\n",
       "      <td>0.0328004</td>\n",
       "      <td>0.505556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>71.1413</td>\n",
       "      <td>79.9296</td>\n",
       "      <td>87.3211</td>\n",
       "      <td>87.5242</td>\n",
       "      <td>41.3582</td>\n",
       "      <td>73.4549</td>\n",
       "      <td>17.133</td>\n",
       "      <td>1.80491</td>\n",
       "      <td>1.77573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865304</td>\n",
       "      <td>0.996257</td>\n",
       "      <td>0.926174</td>\n",
       "      <td>1863</td>\n",
       "      <td>9</td>\n",
       "      <td>290</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0437516</td>\n",
       "      <td>0.106187</td>\n",
       "      <td>0.513179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors    0.0797718     0.071507    0.0771351    0.0738273   \n",
       "0    NB_WordLevel_TF-IDF    0.0481851    0.0466261    0.0766451    0.0649893   \n",
       "0       NB_N-Gram_TF-IDF     0.014116      0.01668     0.016032    0.0157461   \n",
       "0    NB_CharLevel_TF-IDF     0.287065     0.285828     0.284507     0.296301   \n",
       "0       LR_Count_Vectors      21.9553      21.4892        21.74        21.59   \n",
       "0    LR_WordLevel_TF-IDF     0.788837     0.714346     0.687638     0.755387   \n",
       "0       LR_N-Gram_TF-IDF     0.286369     0.227528     0.290044      0.26089   \n",
       "0    LR_CharLevel_TF-IDF      3.30882      2.90528      3.38328      3.49237   \n",
       "0      SVM_Count_Vectors      131.387      112.139      112.127      131.035   \n",
       "0   SVM_WordLevel_TF-IDF      133.493      133.016      132.823      133.299   \n",
       "0      SVM_N-Gram_TF-IDF      47.7785      47.4211      47.8603       47.842   \n",
       "0   SVM_CharLevel_TF-IDF        338.5      335.992       334.67      332.739   \n",
       "0      kNN_Count_Vectors    0.0667431    0.0663488    0.0670531    0.0661709   \n",
       "0   kNN_WordLevel_TF-IDF    0.0299883    0.0400939     0.038903    0.0345459   \n",
       "0      kNN_N-Gram_TF-IDF    0.0121822    0.0121479     0.014863    0.0324237   \n",
       "0   kNN_CharLevel_TF-IDF     0.306187     0.311969     0.298373     0.299444   \n",
       "0       RF_Count_Vectors      35.4174      34.4362       35.394      36.0363   \n",
       "0    RF_WordLevel_TF-IDF      29.6302       29.046      29.2784       28.713   \n",
       "0       RF_N-Gram_TF-IDF      16.1311      15.9055       15.809      15.8788   \n",
       "0    RF_CharLevel_TF-IDF       72.818      74.9766      74.1606      72.0259   \n",
       "0      SGD_Count_Vectors     0.466264     0.439922     0.372193     0.351783   \n",
       "0   SGD_WordLevel_TF-IDF     0.213058     0.216579     0.219832     0.218021   \n",
       "0     SGD_N-Gram_Vectors     0.100723      0.10501     0.104479     0.098423   \n",
       "0  SGD_CharLevel_Vectors      1.05028     0.982358     0.989798     0.989138   \n",
       "0       GB_Count_Vectors       8.7917      10.8815      12.6547      9.57907   \n",
       "0    GB_WordLevel_TF-IDF      8.38092      8.40496      8.37228      8.33199   \n",
       "0       GB_N-Gram_TF-IDF      2.81184      2.68285      2.73053      2.66889   \n",
       "0    GB_CharLevel_TF-IDF       69.101      69.5549      59.0963      47.4193   \n",
       "0      XGB_Count_Vectors       19.377      14.4478      14.4455      15.2261   \n",
       "0   XGB_WordLevel_TF-IDF      23.9198      19.9961      14.7141      22.6357   \n",
       "0      XGB_N-Gram_TF-IDF      5.79742      7.03303      5.90267      8.18669   \n",
       "0   XGB_CharLevel_TF-IDF      71.1413      79.9296      87.3211      87.5242   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0672839      0.073905   0.00434622      0.0425792      0.0462492  ...   \n",
       "0    0.0271151     0.0527122    0.0169485       0.052336      0.0535359  ...   \n",
       "0     0.013397     0.0151942   0.00123341      0.0271609       0.030457  ...   \n",
       "0     0.162329      0.263206    0.0506087      0.0458188        0.04583  ...   \n",
       "0      8.46463       19.0478      5.29392      0.0266199       0.048497  ...   \n",
       "0     0.356363      0.660514     0.155955      0.0317733      0.0414839  ...   \n",
       "0       0.1738      0.247726    0.0431982      0.0387211      0.0386691  ...   \n",
       "0      1.72421       2.96279     0.650291        0.05165       0.061012  ...   \n",
       "0      68.7219       111.082      22.8343        26.4534        29.5846  ...   \n",
       "0      76.1669       121.759      22.7974        31.5992        31.3205  ...   \n",
       "0      29.1891       44.0182      7.41624        13.9483         13.848  ...   \n",
       "0      196.133       307.607      55.7686        83.4399        83.2907  ...   \n",
       "0    0.0530539      0.063874   0.00541875        3.76564         3.7982  ...   \n",
       "0     0.034281     0.0355624   0.00361773        3.52425        3.57414  ...   \n",
       "0    0.0171649     0.0177564   0.00756807        1.13322        1.11459  ...   \n",
       "0     0.174597      0.278114    0.0519911        33.8946         33.921  ...   \n",
       "0      9.56988       30.1708      10.3131       0.522788       0.440718  ...   \n",
       "0      7.92183       24.9179       8.5033       0.164787       0.153455  ...   \n",
       "0      3.96744       13.5384      4.78668       0.099746       0.189458  ...   \n",
       "0      20.3815       62.8725      21.2702       0.634454        0.44614  ...   \n",
       "0     0.247021      0.375437    0.0767706      0.0806799      0.0717239  ...   \n",
       "0     0.151218      0.203742    0.0263557      0.0352151      0.0362551  ...   \n",
       "0     0.107885      0.103304   0.00333954      0.0329521      0.0335541  ...   \n",
       "0     0.966923        0.9957    0.0285068      0.0510993      0.0418549  ...   \n",
       "0      5.49654       9.48071      2.38305      0.0406899      0.0356059  ...   \n",
       "0      5.34439       7.76691      1.21149       0.038368      0.0394897  ...   \n",
       "0       1.4829        2.4754     0.498761       0.036294       0.053539  ...   \n",
       "0      53.0389       59.6421      8.72964       0.053515      0.0418267  ...   \n",
       "0      12.7457       15.2484      2.21789       0.266235       0.508641  ...   \n",
       "0      11.6995        18.593      4.67447       0.195653       0.400202  ...   \n",
       "0      3.42863       6.06969      1.58031       0.168404       0.133843  ...   \n",
       "0      41.3582       73.4549       17.133        1.80491        1.77573  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.867487       0.980214         0.920412       1833         19   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862085       0.999465         0.925706       1869          0   \n",
       "0     0.886261       0.941711         0.913145       1761         73   \n",
       "0     0.862546              1         0.926201       1870          1   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862944              1         0.926431       1870          2   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862818        0.99893         0.925898       1868          2   \n",
       "0     0.863279       0.999465         0.926394       1869          3   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.863343              1          0.92666       1870          3   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.894488        0.91123         0.902781       1704         98   \n",
       "0     0.886476       0.960428         0.921971       1796         69   \n",
       "0     0.869417       0.957754          0.91145       1791         30   \n",
       "0     0.862085       0.999465         0.925706       1869          0   \n",
       "0     0.863889       0.997861         0.926055       1866          5   \n",
       "0     0.863678       0.999465         0.926624       1869          4   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.862148              1         0.925972       1870          0   \n",
       "0     0.869137       0.990909         0.926037       1853         20   \n",
       "0     0.863869       0.990909         0.923039       1853          7   \n",
       "0     0.863488       0.987701         0.921427       1847          7   \n",
       "0     0.865304       0.996257         0.926174       1863          9   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        280         37            0.0664406                 0.0951219   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        299          1         -0.000919856               -0.00858786   \n",
       "0        226        109             0.222417                  0.231102   \n",
       "0        298          0           0.00575294                 0.0537101   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        297          0            0.0114781                  0.075975   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        297          2            0.0095966                 0.0451534   \n",
       "0        296          1            0.0162214                  0.076324   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        296          0            0.0171758                 0.0930715   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        201        166             0.251348                  0.251991   \n",
       "0        230         74              0.24486                  0.265613   \n",
       "0        269         79            0.0792393                 0.0916641   \n",
       "0        299          1         -0.000919856               -0.00858786   \n",
       "0        294          4            0.0246095                 0.0782107   \n",
       "0        295          1            0.0218804                 0.0923241   \n",
       "0        299          0                    0                         0   \n",
       "0        299          0                    0                         0   \n",
       "0        279         17            0.0914643                  0.153879   \n",
       "0        292         17             0.023336                 0.0471948   \n",
       "0        292         23            0.0178621                 0.0328004   \n",
       "0        290          7            0.0437516                  0.106187   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0         0.52188  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0        0.499733  \n",
       "0        0.592929  \n",
       "0        0.501672  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0        0.503344  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0         0.50281  \n",
       "0        0.504749  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0        0.505017  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0        0.619495  \n",
       "0        0.595599  \n",
       "0        0.529044  \n",
       "0        0.499733  \n",
       "0        0.507292  \n",
       "0        0.506422  \n",
       "0             0.5  \n",
       "0             0.5  \n",
       "0        0.528899  \n",
       "0         0.50716  \n",
       "0        0.505556  \n",
       "0        0.513179  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Catboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if catboost_classifier:\n",
    "    # work in progress\n",
    "    if num_gpu>0:  # test gpu available\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "    else:\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_count, y_train, x_test_count, y_test, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Catboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Catboost_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Adaboost</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if adaboost_classifier:\n",
    "    # work in progress\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_count, y_train, x_test_count, y_test, name='Adaboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf, y_train, x_test_tfidf, y_test, name='Adaboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram,y_train, x_test_tfidf_ngram, y_test, name='Adaboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), x_train_tfidf_ngram_chars,y_train, x_test_tfidf_ngram_chars, y_test, name='Adaboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lgbm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>LightGBM</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if lightgbm_classifier:\n",
    "    \n",
    "    # work in progress\n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_count, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(x_test_tfidf_ngram_chars, y_test)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>fit_time_cv1</th>\n",
       "      <th>fit_time_cv2</th>\n",
       "      <th>fit_time_cv3</th>\n",
       "      <th>fit_time_cv4</th>\n",
       "      <th>fit_time_cv5</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_cv1</th>\n",
       "      <th>score_time_cv2</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>tp_overall</th>\n",
       "      <th>tn_overall</th>\n",
       "      <th>fp_overall</th>\n",
       "      <th>fn_overall</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.083657</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>0.0876408</td>\n",
       "      <td>0.0806589</td>\n",
       "      <td>0.0848186</td>\n",
       "      <td>0.00268425</td>\n",
       "      <td>0.0557067</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791759</td>\n",
       "      <td>0.714573</td>\n",
       "      <td>0.751189</td>\n",
       "      <td>711</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>284</td>\n",
       "      <td>0.559446</td>\n",
       "      <td>0.561763</td>\n",
       "      <td>0.777644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.0570619</td>\n",
       "      <td>0.0624511</td>\n",
       "      <td>0.0512819</td>\n",
       "      <td>0.0743842</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.0559588</td>\n",
       "      <td>0.0131171</td>\n",
       "      <td>0.0463171</td>\n",
       "      <td>0.0433159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.58191</td>\n",
       "      <td>0.681176</td>\n",
       "      <td>579</td>\n",
       "      <td>1048</td>\n",
       "      <td>126</td>\n",
       "      <td>416</td>\n",
       "      <td>0.485369</td>\n",
       "      <td>0.504886</td>\n",
       "      <td>0.737292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_N-Gram_TF-IDF</td>\n",
       "      <td>0.0146081</td>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.0386279</td>\n",
       "      <td>0.0167282</td>\n",
       "      <td>0.0169821</td>\n",
       "      <td>0.0236558</td>\n",
       "      <td>0.00956548</td>\n",
       "      <td>0.0413001</td>\n",
       "      <td>0.0396559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730563</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>0.626077</td>\n",
       "      <td>545</td>\n",
       "      <td>973</td>\n",
       "      <td>201</td>\n",
       "      <td>450</td>\n",
       "      <td>0.383852</td>\n",
       "      <td>0.394977</td>\n",
       "      <td>0.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CharLevel_TF-IDF</td>\n",
       "      <td>0.849732</td>\n",
       "      <td>0.797022</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.13063</td>\n",
       "      <td>0.286705</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.303969</td>\n",
       "      <td>0.125775</td>\n",
       "      <td>0.0687079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.188948</td>\n",
       "      <td>106</td>\n",
       "      <td>1153</td>\n",
       "      <td>21</td>\n",
       "      <td>889</td>\n",
       "      <td>0.0949604</td>\n",
       "      <td>0.188135</td>\n",
       "      <td>0.544323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>18.4121</td>\n",
       "      <td>19.1657</td>\n",
       "      <td>17.922</td>\n",
       "      <td>19.1001</td>\n",
       "      <td>9.19119</td>\n",
       "      <td>16.7582</td>\n",
       "      <td>3.81129</td>\n",
       "      <td>0.0704119</td>\n",
       "      <td>0.0472348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738872</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.744766</td>\n",
       "      <td>747</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>248</td>\n",
       "      <td>0.525238</td>\n",
       "      <td>0.525295</td>\n",
       "      <td>0.762941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.667456</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.61599</td>\n",
       "      <td>0.690176</td>\n",
       "      <td>0.490711</td>\n",
       "      <td>0.626685</td>\n",
       "      <td>0.0722426</td>\n",
       "      <td>0.0417788</td>\n",
       "      <td>0.0403888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771372</td>\n",
       "      <td>0.779899</td>\n",
       "      <td>0.775612</td>\n",
       "      <td>776</td>\n",
       "      <td>944</td>\n",
       "      <td>230</td>\n",
       "      <td>219</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>0.583527</td>\n",
       "      <td>0.791994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.495503</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.355603</td>\n",
       "      <td>0.409944</td>\n",
       "      <td>0.34902</td>\n",
       "      <td>0.407818</td>\n",
       "      <td>0.0535282</td>\n",
       "      <td>0.0507712</td>\n",
       "      <td>0.0475352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701226</td>\n",
       "      <td>0.632161</td>\n",
       "      <td>0.664905</td>\n",
       "      <td>629</td>\n",
       "      <td>906</td>\n",
       "      <td>268</td>\n",
       "      <td>366</td>\n",
       "      <td>0.406937</td>\n",
       "      <td>0.408658</td>\n",
       "      <td>0.701941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>4.52604</td>\n",
       "      <td>5.30917</td>\n",
       "      <td>5.46984</td>\n",
       "      <td>5.11398</td>\n",
       "      <td>3.58113</td>\n",
       "      <td>4.80003</td>\n",
       "      <td>0.68807</td>\n",
       "      <td>0.07074</td>\n",
       "      <td>0.0679901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.730192</td>\n",
       "      <td>705</td>\n",
       "      <td>943</td>\n",
       "      <td>231</td>\n",
       "      <td>290</td>\n",
       "      <td>0.514104</td>\n",
       "      <td>0.514884</td>\n",
       "      <td>0.75589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Count_Vectors</td>\n",
       "      <td>142.24</td>\n",
       "      <td>143.032</td>\n",
       "      <td>143.314</td>\n",
       "      <td>144.206</td>\n",
       "      <td>113.892</td>\n",
       "      <td>137.337</td>\n",
       "      <td>11.7393</td>\n",
       "      <td>45.6131</td>\n",
       "      <td>45.6117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750984</td>\n",
       "      <td>0.766834</td>\n",
       "      <td>0.758826</td>\n",
       "      <td>763</td>\n",
       "      <td>921</td>\n",
       "      <td>253</td>\n",
       "      <td>232</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.55055</td>\n",
       "      <td>0.775666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_WordLevel_TF-IDF</td>\n",
       "      <td>171.507</td>\n",
       "      <td>164.749</td>\n",
       "      <td>171.223</td>\n",
       "      <td>171.68</td>\n",
       "      <td>118.665</td>\n",
       "      <td>159.565</td>\n",
       "      <td>20.6152</td>\n",
       "      <td>51.6323</td>\n",
       "      <td>49.8943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760194</td>\n",
       "      <td>0.786935</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>783</td>\n",
       "      <td>927</td>\n",
       "      <td>247</td>\n",
       "      <td>212</td>\n",
       "      <td>0.575001</td>\n",
       "      <td>0.575303</td>\n",
       "      <td>0.788271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_N-Gram_TF-IDF</td>\n",
       "      <td>67.1528</td>\n",
       "      <td>65.9447</td>\n",
       "      <td>66.7256</td>\n",
       "      <td>65.973</td>\n",
       "      <td>32.62</td>\n",
       "      <td>59.6832</td>\n",
       "      <td>13.5394</td>\n",
       "      <td>24.1372</td>\n",
       "      <td>25.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.621106</td>\n",
       "      <td>0.658147</td>\n",
       "      <td>618</td>\n",
       "      <td>909</td>\n",
       "      <td>265</td>\n",
       "      <td>377</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.401015</td>\n",
       "      <td>0.697691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_CharLevel_TF-IDF</td>\n",
       "      <td>567.083</td>\n",
       "      <td>577.67</td>\n",
       "      <td>578.479</td>\n",
       "      <td>579.833</td>\n",
       "      <td>336.997</td>\n",
       "      <td>528.013</td>\n",
       "      <td>95.6156</td>\n",
       "      <td>186.596</td>\n",
       "      <td>184.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>0.759799</td>\n",
       "      <td>0.758655</td>\n",
       "      <td>756</td>\n",
       "      <td>932</td>\n",
       "      <td>242</td>\n",
       "      <td>239</td>\n",
       "      <td>0.553539</td>\n",
       "      <td>0.553541</td>\n",
       "      <td>0.776833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_Count_Vectors</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.072355</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0731351</td>\n",
       "      <td>0.0939651</td>\n",
       "      <td>0.0768281</td>\n",
       "      <td>0.00857692</td>\n",
       "      <td>3.7683</td>\n",
       "      <td>3.65425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586263</td>\n",
       "      <td>0.720603</td>\n",
       "      <td>0.646528</td>\n",
       "      <td>717</td>\n",
       "      <td>668</td>\n",
       "      <td>506</td>\n",
       "      <td>278</td>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.290993</td>\n",
       "      <td>0.644799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_WordLevel_TF-IDF</td>\n",
       "      <td>0.049027</td>\n",
       "      <td>0.0487049</td>\n",
       "      <td>0.051249</td>\n",
       "      <td>0.0619388</td>\n",
       "      <td>0.0523279</td>\n",
       "      <td>0.0526495</td>\n",
       "      <td>0.00483814</td>\n",
       "      <td>4.03613</td>\n",
       "      <td>3.9824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751938</td>\n",
       "      <td>0.0974874</td>\n",
       "      <td>0.172598</td>\n",
       "      <td>97</td>\n",
       "      <td>1142</td>\n",
       "      <td>32</td>\n",
       "      <td>898</td>\n",
       "      <td>0.0752212</td>\n",
       "      <td>0.147965</td>\n",
       "      <td>0.535115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_N-Gram_TF-IDF</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.0136449</td>\n",
       "      <td>0.020715</td>\n",
       "      <td>0.0663297</td>\n",
       "      <td>0.0168662</td>\n",
       "      <td>0.0261462</td>\n",
       "      <td>0.0202725</td>\n",
       "      <td>1.33747</td>\n",
       "      <td>1.33642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584323</td>\n",
       "      <td>0.494472</td>\n",
       "      <td>0.535656</td>\n",
       "      <td>492</td>\n",
       "      <td>824</td>\n",
       "      <td>350</td>\n",
       "      <td>503</td>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.20076</td>\n",
       "      <td>0.598173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN_CharLevel_TF-IDF</td>\n",
       "      <td>0.655702</td>\n",
       "      <td>0.672649</td>\n",
       "      <td>0.65748</td>\n",
       "      <td>0.667741</td>\n",
       "      <td>0.195899</td>\n",
       "      <td>0.569894</td>\n",
       "      <td>0.187104</td>\n",
       "      <td>48.1581</td>\n",
       "      <td>48.1758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739785</td>\n",
       "      <td>0.345729</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>344</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>651</td>\n",
       "      <td>0.252933</td>\n",
       "      <td>0.294636</td>\n",
       "      <td>0.621331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.458804</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.420624</td>\n",
       "      <td>0.444952</td>\n",
       "      <td>0.266678</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.0751494</td>\n",
       "      <td>0.0419219</td>\n",
       "      <td>0.0525622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>0.872362</td>\n",
       "      <td>0.747631</td>\n",
       "      <td>868</td>\n",
       "      <td>715</td>\n",
       "      <td>459</td>\n",
       "      <td>127</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>0.492212</td>\n",
       "      <td>0.740695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>0.269268</td>\n",
       "      <td>0.277042</td>\n",
       "      <td>0.274197</td>\n",
       "      <td>0.175028</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.0459828</td>\n",
       "      <td>0.03966</td>\n",
       "      <td>0.039535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742604</td>\n",
       "      <td>0.756784</td>\n",
       "      <td>0.749627</td>\n",
       "      <td>753</td>\n",
       "      <td>913</td>\n",
       "      <td>261</td>\n",
       "      <td>242</td>\n",
       "      <td>0.53369</td>\n",
       "      <td>0.533773</td>\n",
       "      <td>0.767234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.118953</td>\n",
       "      <td>0.126627</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.105808</td>\n",
       "      <td>0.0728922</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>0.0183905</td>\n",
       "      <td>0.0420618</td>\n",
       "      <td>0.03441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.678392</td>\n",
       "      <td>0.65534</td>\n",
       "      <td>675</td>\n",
       "      <td>784</td>\n",
       "      <td>390</td>\n",
       "      <td>320</td>\n",
       "      <td>0.344348</td>\n",
       "      <td>0.345069</td>\n",
       "      <td>0.673097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>1.92091</td>\n",
       "      <td>1.93717</td>\n",
       "      <td>1.7542</td>\n",
       "      <td>1.82993</td>\n",
       "      <td>1.36733</td>\n",
       "      <td>1.76191</td>\n",
       "      <td>0.208027</td>\n",
       "      <td>0.0974619</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828369</td>\n",
       "      <td>0.586935</td>\n",
       "      <td>0.687059</td>\n",
       "      <td>584</td>\n",
       "      <td>1053</td>\n",
       "      <td>121</td>\n",
       "      <td>411</td>\n",
       "      <td>0.494864</td>\n",
       "      <td>0.514763</td>\n",
       "      <td>0.741934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>62.9065</td>\n",
       "      <td>55.3991</td>\n",
       "      <td>56.9035</td>\n",
       "      <td>52.061</td>\n",
       "      <td>27.6382</td>\n",
       "      <td>50.9817</td>\n",
       "      <td>12.189</td>\n",
       "      <td>0.0316858</td>\n",
       "      <td>0.0516238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698055</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.691684</td>\n",
       "      <td>682</td>\n",
       "      <td>879</td>\n",
       "      <td>295</td>\n",
       "      <td>313</td>\n",
       "      <td>0.434749</td>\n",
       "      <td>0.43481</td>\n",
       "      <td>0.717075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>5.80552</td>\n",
       "      <td>7.31059</td>\n",
       "      <td>7.17226</td>\n",
       "      <td>6.93111</td>\n",
       "      <td>5.58752</td>\n",
       "      <td>6.5614</td>\n",
       "      <td>0.719849</td>\n",
       "      <td>0.0432222</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.566632</td>\n",
       "      <td>540</td>\n",
       "      <td>803</td>\n",
       "      <td>371</td>\n",
       "      <td>455</td>\n",
       "      <td>0.228168</td>\n",
       "      <td>0.228875</td>\n",
       "      <td>0.61335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>220.842</td>\n",
       "      <td>238.157</td>\n",
       "      <td>282.195</td>\n",
       "      <td>257.065</td>\n",
       "      <td>184.308</td>\n",
       "      <td>236.513</td>\n",
       "      <td>33.1146</td>\n",
       "      <td>0.0761411</td>\n",
       "      <td>0.0745621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719447</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.69938</td>\n",
       "      <td>677</td>\n",
       "      <td>910</td>\n",
       "      <td>264</td>\n",
       "      <td>318</td>\n",
       "      <td>0.457422</td>\n",
       "      <td>0.458003</td>\n",
       "      <td>0.727765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>92.4755</td>\n",
       "      <td>84.2349</td>\n",
       "      <td>73.6215</td>\n",
       "      <td>84.1027</td>\n",
       "      <td>48.617</td>\n",
       "      <td>76.6103</td>\n",
       "      <td>15.2219</td>\n",
       "      <td>0.0370519</td>\n",
       "      <td>0.0738072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>700</td>\n",
       "      <td>894</td>\n",
       "      <td>280</td>\n",
       "      <td>295</td>\n",
       "      <td>0.465552</td>\n",
       "      <td>0.465597</td>\n",
       "      <td>0.732508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>74.1953</td>\n",
       "      <td>65.7518</td>\n",
       "      <td>84.2132</td>\n",
       "      <td>65.1882</td>\n",
       "      <td>32.8804</td>\n",
       "      <td>64.4458</td>\n",
       "      <td>17.2304</td>\n",
       "      <td>0.516725</td>\n",
       "      <td>0.95696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.751759</td>\n",
       "      <td>0.748749</td>\n",
       "      <td>748</td>\n",
       "      <td>919</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>0.534225</td>\n",
       "      <td>0.53424</td>\n",
       "      <td>0.767276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>46.6946</td>\n",
       "      <td>49.373</td>\n",
       "      <td>52.0918</td>\n",
       "      <td>37.7714</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>42.1696</td>\n",
       "      <td>9.87732</td>\n",
       "      <td>0.425461</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727728</td>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.729188</td>\n",
       "      <td>727</td>\n",
       "      <td>902</td>\n",
       "      <td>272</td>\n",
       "      <td>268</td>\n",
       "      <td>0.498814</td>\n",
       "      <td>0.498817</td>\n",
       "      <td>0.749483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>14.4524</td>\n",
       "      <td>14.8459</td>\n",
       "      <td>10.1767</td>\n",
       "      <td>6.22884</td>\n",
       "      <td>11.4074</td>\n",
       "      <td>11.4223</td>\n",
       "      <td>3.14409</td>\n",
       "      <td>0.109201</td>\n",
       "      <td>0.0818152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640324</td>\n",
       "      <td>0.635176</td>\n",
       "      <td>0.63774</td>\n",
       "      <td>632</td>\n",
       "      <td>819</td>\n",
       "      <td>355</td>\n",
       "      <td>363</td>\n",
       "      <td>0.332995</td>\n",
       "      <td>0.333004</td>\n",
       "      <td>0.666395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>307.123</td>\n",
       "      <td>379.482</td>\n",
       "      <td>492.242</td>\n",
       "      <td>429.361</td>\n",
       "      <td>213.657</td>\n",
       "      <td>364.373</td>\n",
       "      <td>96.7421</td>\n",
       "      <td>3.22035</td>\n",
       "      <td>2.00201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744376</td>\n",
       "      <td>0.731658</td>\n",
       "      <td>0.737962</td>\n",
       "      <td>728</td>\n",
       "      <td>924</td>\n",
       "      <td>250</td>\n",
       "      <td>267</td>\n",
       "      <td>0.519388</td>\n",
       "      <td>0.519452</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>31.9763</td>\n",
       "      <td>31.7864</td>\n",
       "      <td>31.8414</td>\n",
       "      <td>31.192</td>\n",
       "      <td>8.30906</td>\n",
       "      <td>27.021</td>\n",
       "      <td>9.35985</td>\n",
       "      <td>0.502497</td>\n",
       "      <td>0.662884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.527638</td>\n",
       "      <td>0.629874</td>\n",
       "      <td>525</td>\n",
       "      <td>1027</td>\n",
       "      <td>147</td>\n",
       "      <td>470</td>\n",
       "      <td>0.412637</td>\n",
       "      <td>0.433646</td>\n",
       "      <td>0.701213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>24.037</td>\n",
       "      <td>24.4264</td>\n",
       "      <td>24.231</td>\n",
       "      <td>24.4041</td>\n",
       "      <td>6.19139</td>\n",
       "      <td>20.658</td>\n",
       "      <td>7.23465</td>\n",
       "      <td>0.252068</td>\n",
       "      <td>0.331617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751989</td>\n",
       "      <td>0.569849</td>\n",
       "      <td>0.64837</td>\n",
       "      <td>567</td>\n",
       "      <td>987</td>\n",
       "      <td>187</td>\n",
       "      <td>428</td>\n",
       "      <td>0.418288</td>\n",
       "      <td>0.429599</td>\n",
       "      <td>0.705282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_N-Gram_TF-IDF</td>\n",
       "      <td>16.2884</td>\n",
       "      <td>16.1502</td>\n",
       "      <td>16.4692</td>\n",
       "      <td>16.4074</td>\n",
       "      <td>4.51221</td>\n",
       "      <td>13.9655</td>\n",
       "      <td>4.72788</td>\n",
       "      <td>0.128438</td>\n",
       "      <td>0.119955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615965</td>\n",
       "      <td>0.573869</td>\n",
       "      <td>0.594173</td>\n",
       "      <td>571</td>\n",
       "      <td>818</td>\n",
       "      <td>356</td>\n",
       "      <td>424</td>\n",
       "      <td>0.27205</td>\n",
       "      <td>0.272599</td>\n",
       "      <td>0.635316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_CharLevel_TF-IDF</td>\n",
       "      <td>76.4607</td>\n",
       "      <td>76.9057</td>\n",
       "      <td>76.4275</td>\n",
       "      <td>76.0929</td>\n",
       "      <td>17.0665</td>\n",
       "      <td>64.5907</td>\n",
       "      <td>23.7635</td>\n",
       "      <td>1.07035</td>\n",
       "      <td>0.870651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729829</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.658577</td>\n",
       "      <td>597</td>\n",
       "      <td>953</td>\n",
       "      <td>221</td>\n",
       "      <td>398</td>\n",
       "      <td>0.417415</td>\n",
       "      <td>0.423331</td>\n",
       "      <td>0.705877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model fit_time_cv1 fit_time_cv2 fit_time_cv3 fit_time_cv4  \\\n",
       "0       NB_Count_Vectors     0.083657     0.087841     0.084295    0.0876408   \n",
       "0    NB_WordLevel_TF-IDF    0.0570619    0.0624511    0.0512819    0.0743842   \n",
       "0       NB_N-Gram_TF-IDF    0.0146081     0.031333    0.0386279    0.0167282   \n",
       "0    NB_CharLevel_TF-IDF     0.849732     0.797022       1.1039      1.13063   \n",
       "0       LR_Count_Vectors      18.4121      19.1657       17.922      19.1001   \n",
       "0    LR_WordLevel_TF-IDF     0.667456     0.669091      0.61599     0.690176   \n",
       "0       LR_N-Gram_TF-IDF     0.495503     0.429021     0.355603     0.409944   \n",
       "0    LR_CharLevel_TF-IDF      4.52604      5.30917      5.46984      5.11398   \n",
       "0      SVM_Count_Vectors       142.24      143.032      143.314      144.206   \n",
       "0   SVM_WordLevel_TF-IDF      171.507      164.749      171.223       171.68   \n",
       "0      SVM_N-Gram_TF-IDF      67.1528      65.9447      66.7256       65.973   \n",
       "0   SVM_CharLevel_TF-IDF      567.083       577.67      578.479      579.833   \n",
       "0      kNN_Count_Vectors     0.071985     0.072355       0.0727    0.0731351   \n",
       "0   kNN_WordLevel_TF-IDF     0.049027    0.0487049     0.051249    0.0619388   \n",
       "0      kNN_N-Gram_TF-IDF     0.013175    0.0136449     0.020715    0.0663297   \n",
       "0   kNN_CharLevel_TF-IDF     0.655702     0.672649      0.65748     0.667741   \n",
       "0      SGD_Count_Vectors     0.458804     0.473416     0.420624     0.444952   \n",
       "0   SGD_WordLevel_TF-IDF       0.3128     0.269268     0.277042     0.274197   \n",
       "0     SGD_N-Gram_Vectors     0.118953     0.126627       0.1075     0.105808   \n",
       "0  SGD_CharLevel_Vectors      1.92091      1.93717       1.7542      1.82993   \n",
       "0       GB_Count_Vectors      62.9065      55.3991      56.9035       52.061   \n",
       "0       GB_N-Gram_TF-IDF      5.80552      7.31059      7.17226      6.93111   \n",
       "0    GB_CharLevel_TF-IDF      220.842      238.157      282.195      257.065   \n",
       "0    GB_WordLevel_TF-IDF      92.4755      84.2349      73.6215      84.1027   \n",
       "0      XGB_Count_Vectors      74.1953      65.7518      84.2132      65.1882   \n",
       "0   XGB_WordLevel_TF-IDF      46.6946       49.373      52.0918      37.7714   \n",
       "0      XGB_N-Gram_TF-IDF      14.4524      14.8459      10.1767      6.22884   \n",
       "0   XGB_CharLevel_TF-IDF      307.123      379.482      492.242      429.361   \n",
       "0       RF_Count_Vectors      31.9763      31.7864      31.8414       31.192   \n",
       "0    RF_WordLevel_TF-IDF       24.037      24.4264       24.231      24.4041   \n",
       "0       RF_N-Gram_TF-IDF      16.2884      16.1502      16.4692      16.4074   \n",
       "0    RF_CharLevel_TF-IDF      76.4607      76.9057      76.4275      76.0929   \n",
       "\n",
       "  fit_time_cv5 fit_time_mean fit_time_std score_time_cv1 score_time_cv2  ...  \\\n",
       "0    0.0806589     0.0848186   0.00268425      0.0557067      0.0472348  ...   \n",
       "0     0.034615     0.0559588    0.0131171      0.0463171      0.0433159  ...   \n",
       "0    0.0169821     0.0236558   0.00956548      0.0413001      0.0396559  ...   \n",
       "0     0.286705      0.833599     0.303969       0.125775      0.0687079  ...   \n",
       "0      9.19119       16.7582      3.81129      0.0704119      0.0472348  ...   \n",
       "0     0.490711      0.626685    0.0722426      0.0417788      0.0403888  ...   \n",
       "0      0.34902      0.407818    0.0535282      0.0507712      0.0475352  ...   \n",
       "0      3.58113       4.80003      0.68807        0.07074      0.0679901  ...   \n",
       "0      113.892       137.337      11.7393        45.6131        45.6117  ...   \n",
       "0      118.665       159.565      20.6152        51.6323        49.8943  ...   \n",
       "0        32.62       59.6832      13.5394        24.1372        25.0592  ...   \n",
       "0      336.997       528.013      95.6156        186.596        184.288  ...   \n",
       "0    0.0939651     0.0768281   0.00857692         3.7683        3.65425  ...   \n",
       "0    0.0523279     0.0526495   0.00483814        4.03613         3.9824  ...   \n",
       "0    0.0168662     0.0261462    0.0202725        1.33747        1.33642  ...   \n",
       "0     0.195899      0.569894     0.187104        48.1581        48.1758  ...   \n",
       "0     0.266678      0.412895    0.0751494      0.0419219      0.0525622  ...   \n",
       "0     0.175028      0.261667    0.0459828        0.03966       0.039535  ...   \n",
       "0    0.0728922      0.106356    0.0183905      0.0420618        0.03441  ...   \n",
       "0      1.36733       1.76191     0.208027      0.0974619       0.103405  ...   \n",
       "0      27.6382       50.9817       12.189      0.0316858      0.0516238  ...   \n",
       "0      5.58752        6.5614     0.719849      0.0432222       0.025759  ...   \n",
       "0      184.308       236.513      33.1146      0.0761411      0.0745621  ...   \n",
       "0       48.617       76.6103      15.2219      0.0370519      0.0738072  ...   \n",
       "0      32.8804       64.4458      17.2304       0.516725        0.95696  ...   \n",
       "0      24.9171       42.1696      9.87732       0.425461       0.305956  ...   \n",
       "0      11.4074       11.4223      3.14409       0.109201      0.0818152  ...   \n",
       "0      213.657       364.373      96.7421        3.22035        2.00201  ...   \n",
       "0      8.30906        27.021      9.35985       0.502497       0.662884  ...   \n",
       "0      6.19139        20.658      7.23465       0.252068       0.331617  ...   \n",
       "0      4.51221       13.9655      4.72788       0.128438       0.119955  ...   \n",
       "0      17.0665       64.5907      23.7635        1.07035       0.870651  ...   \n",
       "\n",
       "  prec_overall recall_overall f1-score_overall tp_overall tn_overall  \\\n",
       "0     0.791759       0.714573         0.751189        711        987   \n",
       "0     0.821277        0.58191         0.681176        579       1048   \n",
       "0     0.730563       0.547739         0.626077        545        973   \n",
       "0     0.834646       0.106533         0.188948        106       1153   \n",
       "0     0.738872       0.750754         0.744766        747        910   \n",
       "0     0.771372       0.779899         0.775612        776        944   \n",
       "0     0.701226       0.632161         0.664905        629        906   \n",
       "0     0.753205       0.708543         0.730192        705        943   \n",
       "0     0.750984       0.766834         0.758826        763        921   \n",
       "0     0.760194       0.786935         0.773333        783        927   \n",
       "0     0.699887       0.621106         0.658147        618        909   \n",
       "0     0.757515       0.759799         0.758655        756        932   \n",
       "0     0.586263       0.720603         0.646528        717        668   \n",
       "0     0.751938      0.0974874         0.172598         97       1142   \n",
       "0     0.584323       0.494472         0.535656        492        824   \n",
       "0     0.739785       0.345729         0.471233        344       1053   \n",
       "0     0.654107       0.872362         0.747631        868        715   \n",
       "0     0.742604       0.756784         0.749627        753        913   \n",
       "0     0.633803       0.678392          0.65534        675        784   \n",
       "0     0.828369       0.586935         0.687059        584       1053   \n",
       "0     0.698055       0.685427         0.691684        682        879   \n",
       "0     0.592755       0.542714         0.566632        540        803   \n",
       "0     0.719447       0.680402          0.69938        677        910   \n",
       "0     0.714286       0.703518         0.708861        700        894   \n",
       "0     0.745763       0.751759         0.748749        748        919   \n",
       "0     0.727728       0.730653         0.729188        727        902   \n",
       "0     0.640324       0.635176          0.63774        632        819   \n",
       "0     0.744376       0.731658         0.737962        728        924   \n",
       "0      0.78125       0.527638         0.629874        525       1027   \n",
       "0     0.751989       0.569849          0.64837        567        987   \n",
       "0     0.615965       0.573869         0.594173        571        818   \n",
       "0     0.729829            0.6         0.658577        597        953   \n",
       "\n",
       "  fp_overall fn_overall cohens_kappa_overall matthews_corrcoef_overall  \\\n",
       "0        187        284             0.559446                  0.561763   \n",
       "0        126        416             0.485369                  0.504886   \n",
       "0        201        450             0.383852                  0.394977   \n",
       "0         21        889            0.0949604                  0.188135   \n",
       "0        264        248             0.525238                  0.525295   \n",
       "0        230        219             0.583496                  0.583527   \n",
       "0        268        366             0.406937                  0.408658   \n",
       "0        231        290             0.514104                  0.514884   \n",
       "0        253        232             0.550446                   0.55055   \n",
       "0        247        212             0.575001                  0.575303   \n",
       "0        265        377             0.398804                  0.401015   \n",
       "0        242        239             0.553539                  0.553541   \n",
       "0        506        278             0.284626                  0.290993   \n",
       "0         32        898            0.0752212                  0.147965   \n",
       "0        350        503             0.198675                   0.20076   \n",
       "0        121        651             0.252933                  0.294636   \n",
       "0        459        127             0.469449                  0.492212   \n",
       "0        261        242              0.53369                  0.533773   \n",
       "0        390        320             0.344348                  0.345069   \n",
       "0        121        411             0.494864                  0.514763   \n",
       "0        295        313             0.434749                   0.43481   \n",
       "0        371        455             0.228168                  0.228875   \n",
       "0        264        318             0.457422                  0.458003   \n",
       "0        280        295             0.465552                  0.465597   \n",
       "0        255        247             0.534225                   0.53424   \n",
       "0        272        268             0.498814                  0.498817   \n",
       "0        355        363             0.332995                  0.333004   \n",
       "0        250        267             0.519388                  0.519452   \n",
       "0        147        470             0.412637                  0.433646   \n",
       "0        187        428             0.418288                  0.429599   \n",
       "0        356        424              0.27205                  0.272599   \n",
       "0        221        398             0.417415                  0.423331   \n",
       "\n",
       "  roc_auc_overall  \n",
       "0        0.777644  \n",
       "0        0.737292  \n",
       "0        0.688265  \n",
       "0        0.544323  \n",
       "0        0.762941  \n",
       "0        0.791994  \n",
       "0        0.701941  \n",
       "0         0.75589  \n",
       "0        0.775666  \n",
       "0        0.788271  \n",
       "0        0.697691  \n",
       "0        0.776833  \n",
       "0        0.644799  \n",
       "0        0.535115  \n",
       "0        0.598173  \n",
       "0        0.621331  \n",
       "0        0.740695  \n",
       "0        0.767234  \n",
       "0        0.673097  \n",
       "0        0.741934  \n",
       "0        0.717075  \n",
       "0         0.61335  \n",
       "0        0.727765  \n",
       "0        0.732508  \n",
       "0        0.767276  \n",
       "0        0.749483  \n",
       "0        0.666395  \n",
       "0        0.759356  \n",
       "0        0.701213  \n",
       "0        0.705282  \n",
       "0        0.635316  \n",
       "0        0.705877  \n",
       "\n",
       "[32 rows x 113 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dl'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "pretrained = fasttext.FastText.load_model('/Users/diego/Documents/NLP/crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88180/88180 [00:04<00:00, 18509.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.54 s, sys: 1.81 s, total: 11.4 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a tokenizer \n",
    "token = Tokenizer(oov_token='<OOV>')\n",
    "token.fit_on_texts(df[TEXT])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=300)\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "words = []\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    words.append(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "if save_model:\n",
    "    filename = NAME_TOKEN_EMBEDDINGS\n",
    "    pickle.dump(token, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_w = {}\n",
    "for i in zip(range(len(class_weights)), class_weights):\n",
    "    class_w[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_NN(model, X, y, X_test, y_test,name=\"NN\", fit_params=None, scoring=None, n_splits=5, save=save_model, batch_size = 32,  use_multiprocessing=True):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param model: (model) neural network model\n",
    "    @param X: (list or matrix or tensor) training X data\n",
    "    @param y: (list) label data \n",
    "    @param X_test: (list or matrix or tensor) testing X data\n",
    "    @param y_test: (list) label test data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param scoring: (dict) dictionary of metrics and names\n",
    "    @param n_splits: (int) number of fold for cross-validation (default 5)\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    # ---- Parameters initialisation\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='auto', patience=3)\n",
    "    seed = 42\n",
    "    k = 1\n",
    "    np.random.seed(seed)\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Creation of list for each metric\n",
    "    if scoring==None:        # create a dictionary if none is passed\n",
    "        dic_scoring = {}\n",
    "    if scoring!=None:        # save the dict \n",
    "        dic_score = scoring.copy()\n",
    "    \n",
    "    dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "    dic_score[\"score_time\"] = None\n",
    "    scorer = {}\n",
    "    for i in dic_score.keys(): \n",
    "        scorer[i] = []\n",
    "    \n",
    "    index = [\"Model\"]\n",
    "    results = [name]\n",
    "    # ---- Loop on k-fold for cross-valisation\n",
    "    for train, test in kfold.split(X, y):   # training NN on each fold \n",
    "        # create model\n",
    "        print(f\"k-fold : {k}\")\n",
    "        fit_start = time.time()\n",
    "        _model = tf.keras.models.clone_model(model)\n",
    "        if len(np.unique(y))==2: # binary\n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        else:  # multiclass \n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "        fit_end = time.time() - fit_start\n",
    "\n",
    "        score_start = time.time()\n",
    "        y_pred = (_model.predict(X[test])>0.5).astype(int)\n",
    "        score_end = time.time() - score_start\n",
    "        #if len(set(y))>2:\n",
    "        #    y_pred =np.argmax(y_pred,axis=1)\n",
    "        #print(y_test[0], y_pred[0])\n",
    "        if len(set(y))==2:\n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], y_pred), 3)}% , Recall: {round(100*recall_score(y.iloc[test], y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        else: \n",
    "            print(f\"Precision: {round(100*precision_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y.iloc[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        \n",
    "        \n",
    "        # ---- save each metric\n",
    "        for i in dic_score.keys():    # compute metrics \n",
    "            if i == \"fit_time\":\n",
    "                scorer[i].append(fit_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(fit_end)\n",
    "                continue\n",
    "            if i == \"score_time\":\n",
    "                scorer[i].append(score_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(score_end)\n",
    "                continue\n",
    "            \n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    scorer[i].append(dic_score[i](y.iloc[test], np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "                elif i==\"roc_auc\":\n",
    "                    scorer[i].append(dic_score[i](to_categorical(y.iloc[test]), y_pred, average = 'macro', multi_class=\"ovo\")) # make each function scorer\n",
    "                else:\n",
    "                    scorer[i].append(dic_score[i]( y.iloc[test], np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i]( y.iloc[test], y_pred)) # make each function scorer\n",
    "            #scorer[i].append(dic_score[i]( y.iloc[test], y_pred))\n",
    "            index.append(\"test_\"+i+'_cv'+str(k))\n",
    "            results.append(scorer[i][-1])\n",
    "        K.clear_session()\n",
    "        del _model\n",
    "        k+=1\n",
    "    \n",
    "    # Train test on the overall data\n",
    "    print(\"Overall train-test data\")\n",
    "    fit_start = time.time()\n",
    "    _model =  tf.keras.models.clone_model(model)\n",
    "    if len(np.unique(y))==2: # binary\n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    else:  # multiclass \n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y.iloc[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y.iloc[test]),\n",
    "                         verbose=False)\n",
    "    if save:\n",
    "        check_p = tf.keras.callbacks.ModelCheckpoint(os.path.join(root_dir, dir_name, name+\".h5\"), save_best_only=True)\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es, check_p], validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    else:\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es],  validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    #_acc = _model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    score_start = time.time()\n",
    "    y_pred = (_model.predict(X_test)>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    #if len(set(y))>2:\n",
    "    #    y_pred =np.argmax(y_pred,axis=1)\n",
    "    if len(set(y))==2:\n",
    "        print(f\"Precision: {round(100*precision_score(y_test, y_pred), 3)}% , Recall: {round(100*recall_score(y_test, y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "    else: \n",
    "        print(f\"Precision: {round(100*precision_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "\n",
    "    # Compute mean and std for each metric\n",
    "    for i in scorer: \n",
    "        \n",
    "        results.append(np.mean(scorer[i]))\n",
    "        results.append(np.std(scorer[i]))\n",
    "        if i == \"fit_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        \n",
    "        index.append(\"test_\"+i+\"_mean\")\n",
    "        index.append(\"test_\"+i+\"_std\")\n",
    "        \n",
    "    # add metrics averall dataset on the dictionary \n",
    "    for i in dic_score.keys():    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            scorer[i].append(fit_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            scorer[i].append(score_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(score_end)\n",
    "            continue\n",
    "        \n",
    "        if len(set(y))>2:\n",
    "            if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "            elif i==\"roc_auc\":\n",
    "                scorer[i].append(dic_score[i](to_categorical(y_test), y_pred, average = 'weighted', multi_class=\"ovo\")) # make each function scorer\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "        else:\n",
    "            scorer[i].append(dic_score[i](y.iloc[test], y_pred))                             \n",
    "            #scorer[i].append(dic_score[i](_model, X_test, y_test))\n",
    "        index.append(i+'_overall')\n",
    "        results.append(scorer[i][-1])\n",
    "    \n",
    "            \n",
    "    return pd.DataFrame(results, index=index).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='snn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Shallow Neural Network</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a shallow neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) shallow neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 16)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      \n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "        \n",
    "      #keras.layers.Dense(6, activation=\"relu\"),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = score_metrics\n",
    "\n",
    "# Creation of list for each metric\n",
    "if scoring==None:        # create a dictionary if none is passed\n",
    "    dic_scoring = {}\n",
    "if scoring!=None:        # save the dict \n",
    "    dic_score = scoring.copy()\n",
    "\n",
    "dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "dic_score[\"score_time\"] = None\n",
    "scorer = {}\n",
    "for i in dic_score.keys(): \n",
    "    scorer[i] = []\n",
    "\n",
    "index = [\"Model\"]\n",
    "results = ['Shallow_NN_WE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dic_score.keys():\n",
    "    scorer[i].append(dic_score[i](_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score['acc'](y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430    fanboy ism h k stuff scar h acr exactly aesthe...\n",
       "5089    curse aye awww good old bargaining alarm clock...\n",
       "111     usually psychic quality knowing come specific ...\n",
       "7536    thank advice appreciate just fantastic advice ...\n",
       "1113    trying understand difference sx sx just differ...\n",
       "                              ...                        \n",
       "2926    true comfortable contradiction hi following re...\n",
       "3821    pretty christian ish t believe jesus christ go...\n",
       "5805    bat sherlock holmes quite alright agnostic spi...\n",
       "3140    try lang studying japanese sent gt using tapat...\n",
       "2770    ugh honestly inclined think behavior abusive t...\n",
       "Name: text_clean_joined, Length: 2169, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_score[i](_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 71.454% , Recall: 67.391%, Time \t 237.6945 ms\n",
      "k-fold : 2\n",
      "Precision: 72.438% , Recall: 68.677%, Time \t 365.2575 ms\n",
      "k-fold : 3\n",
      "Precision: 70.662% , Recall: 66.164%, Time \t 396.282 ms\n",
      "k-fold : 4\n",
      "Precision: 73.179% , Recall: 69.012%, Time \t 263.4461 ms\n",
      "k-fold : 5\n",
      "Precision: 71.357% , Recall: 71.357%, Time \t 327.027 ms\n",
      "Overall train-test data\n",
      "Precision: 70.657% , Recall: 67.035%, Time \t 338.0158 ms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1301, 2169]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-40eebde9b6a9>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mscorer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;31m#scorer[i].append(dic_score[i](_model, X_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_overall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/venv-MBTI/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1301, 2169]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if shallow_network:\n",
    "    df_results = df_results.append(cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Shallow_NN_WE\", scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dnn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Deep Neural Net</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_WE\",scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits , save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var1(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, y_train, test_seq_x, y_test,\n",
    "                                                     name=\"Deep_NN_var1_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"span5 alert alert-info\">\n",
    "    <H5>Transformers</H5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](img/transformers_model_architecture.png)\n",
    "\n",
    "The Transformer – Model Architecture - [Source](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=vidya></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
